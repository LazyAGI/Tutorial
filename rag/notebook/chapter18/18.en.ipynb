{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6e3e74",
   "metadata": {},
   "source": [
    "# Chapter 18 High-level RAG: Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214942dd",
   "metadata": {},
   "source": [
    ">In the previous tutorial, we learned how to build a RAG system, as well as improve the performance, speed optimization, function expansion, etc. of the RAG system. In this tutorial, we will further introduce the recently popular Agentic RAG on this basis. It is a variant of RAG, but more intelligent. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f9aeb",
   "metadata": {},
   "source": [
    "If **RAG ​** is compared to a candidate who goes to the exam with **books**, then Agentic RAG is a candidate who goes to the exam with **teacher and books** at the same time!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5100f51",
   "metadata": {},
   "source": [
    "![image.png](18_images/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffd936",
   "metadata": {},
   "source": [
    "Agentic RAG is a RAG that integrates AI Agent. This article will first introduce Agentic RAG based on concepts such as RAG and AI Agent; then introduce the basic principles and components of Agentic RAG in detail; then introduce why Agentic RAG is used and compare it with traditional RAG; and finally introduce how to build an Agentic RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d914cc",
   "metadata": {},
   "source": [
    "## Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b88d1",
   "metadata": {},
   "source": [
    "What is Agentic RAG? Let us break down this complex concept into RAG and AI Agent (Agentic introduces AI Agent) and introduce them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904404a4",
   "metadata": {},
   "source": [
    "### 1. Review of RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67c658",
   "metadata": {},
   "source": [
    "First let's review the basic concepts of RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cef04",
   "metadata": {},
   "source": [
    "#### Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea232ab",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) technology is a technology that uses plug-in knowledge sources to supplement the context of large language models to strengthen the input, thereby improving the quality of content generated by large language models and reducing hallucinations (hallucinations are unreal content generated by LLM confidently fabricating information at will). For example, RAG is a candidate who takes a book with him to take an exam. The test questions are the input, the book is the plug-in knowledge base, the candidates are the big model, and the content of the candidates' answers is the content generated by the big model. Generally speaking, if you can take a closed-book exam with a textbook, the score on the answer sheet will be very high. This is an image explanation of how RAG can improve the quality of content generated by large models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e111",
   "metadata": {},
   "source": [
    "#### Basic components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01b6f9",
   "metadata": {},
   "source": [
    "RAG mainly consists of two components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb678c",
   "metadata": {},
   "source": [
    "* Retrieval Component: The retrieval component is used to match information in the knowledge base based on input. For example, it is to search for answers in textbooks with test questions.\n",
    "* Generative Component: The Generative Component is used to send the input and retrieved information to the large model to generate high-quality responses. For example, the candidate answers the test questions by combining the questions and the content found in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff547654",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffadc67",
   "metadata": {},
   "source": [
    "The name RAG (Retrieval-Augmented Generation) has revealed the workflow of this technology. Let us combine the diagram and disassemble the name to see:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bfa44",
   "metadata": {},
   "source": [
    "* First we enter a query:\n",
    "    * Retrieval: retrieval, the query is first used to retrieve in a knowledge base (the details of embedding, vectorization and other aspects in RAG are simplified here. For details, please see the previous tutorial [Basic 2: Getting to know RAG](https://github.com/LazyAGI/Tutorial/tree/main/rag/notebook/chapter2/2.ipynb). The documents and query in the knowledge base will be vectorized for similarity calculation. Vector Search in the figure below Corresponding to the search of the knowledge base);\n",
    "    * Augmented: Enhanced, splicing the retrieved content (context) with the query we input to achieve the effect of enhancing the query;\n",
    "    * Generation: Generation. Send the enhanced query in the previous step to the LLM large model to generate the reply content.\n",
    "* Return the generated content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21c998fd",
   "metadata": {},
   "source": [
    "![image.png](18_images/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86317271",
   "metadata": {},
   "source": [
    "### 2. Introduction to AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d62e5",
   "metadata": {},
   "source": [
    "#### Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ac108",
   "metadata": {},
   "source": [
    "First of all, what is Agent? A common translation in Chinese is: agent. When we want to do something, there are generally two ways. We can achieve it step by step by ourselves; the other is to find someone, who is called an agent. We give the agent full authority without caring about what he does, as long as he can help us achieve our goal. In the former we need to worry about every detail, while in the latter we can sit back and enjoy the results. Therefore, one of the characteristics of Agent is that we do not need to care about the details of completing a certain task, but we only need to trust him to give the task to him and let him help us achieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e535f",
   "metadata": {},
   "source": [
    "Returning to the AI ​​Agent, an AI Agent is generally considered an LLM with specific roles and tasks, and it can access memory and external tools. But I think the AI ​​agent is more like a person, an agent we invite. I prefer to compare it to a person with a high degree of professional ability - an expert. LLM is its brain. With its smart brain, it can automatically plan steps and take repeated actions (such as calling tools) based on feedback to solve the task at hand. We don't need to worry about the whole process, we just need to let him do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4520f",
   "metadata": {},
   "source": [
    "Imagine that you are a king. When you want to expand your territory, you do not need to do it yourself. You only need to find an agent - your general (that is, an expert in leading troops in combat). Delegate power to the general and let the general do it. He will plan the battle plan (planning), deploy troops (call tools), and charge into battle (take action). You just have to wait for the good news of his triumph. This general is like our AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d2994",
   "metadata": {},
   "source": [
    "#### Basic components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528155d3",
   "metadata": {},
   "source": [
    "An AI Agent mainly consists of the following components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aee071",
   "metadata": {},
   "source": [
    "* LLM: This is the brain of the intelligent agent, corresponding to the brain of the general;\n",
    "* Memory: The memory of the intelligent agent corresponds to all the memories of the general from the beginning to the end of a certain territorial expansion mission, even the memory of previous battles;\n",
    "* Planning: The intelligent agent can reflect, self-criticize, automatically route (take action), etc., which corresponds to the king delegating power to the general, allowing him to complete the task according to his own ideas;\n",
    "* Tools: They are tools that the agent can call, corresponding to the troops that the general can call, the weapons that can be used, etc.;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0107298",
   "metadata": {},
   "source": [
    "![image.png](18_images/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df438194",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392b808",
   "metadata": {},
   "source": [
    "AI Agent has many types of workflows. Here are some common workflows: Function Call Agent, ReAct, PlanAndSolve and ReWOO. The workflow of an AI agent is mainly its behavior pattern, which is like a person's behavioral habits of doing things:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383643e8",
   "metadata": {},
   "source": [
    "* Function Call Agent: After the agent receives a task, it will continue to try to call the tool with various parameters and observe the output until the problem is solved or the maximum number of repetitions is reached.\n",
    "* ReAct: After the agent receives the task, it will think first, then try to call the tool and observe the output, and repeat this process until the problem is solved or the maximum number of repetitions is reached.\n",
    "* PlanAndSolve: After receiving the task, the agent will first plan to decompose the task, and then try to solve the current step task. Based on the results of the current step, it will continue to execute the task or re-plan subsequent tasks until the task is solved or the maximum number of repetitions is reached.\n",
    "* ReWOO: After receiving the task, the agent will first plan to decompose the task, then complete all steps, and provide feedback based on the results of all steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9081d",
   "metadata": {},
   "source": [
    "##### Function Call Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09367d",
   "metadata": {},
   "source": [
    "Function Call Agent mainly includes the following processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b772de",
   "metadata": {},
   "source": [
    "1. Action: After the Agent receives a query, it will take direct action, such as calling a tool;\n",
    "2. Observation: Agent observes feedback from actions, such as the output of a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f053abf",
   "metadata": {},
   "source": [
    "The above process will continue in a loop. If the feedback of the observed action is OK, meets the requirements of the query, or reaches the maximum number of iterations, then the Agent will exit and return the result response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e51105f7",
   "metadata": {},
   "source": [
    "![image.png](18_images/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850a028",
   "metadata": {},
   "source": [
    "We can use AI Agent in LazyLLM. First define the tool, and then register the defined tool into LazyLLM. After that, we can define the model and use FunctionCall Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import json\n",
    "import lazyllm\n",
    "from lazyllm.tools import fc_register, FunctionCall, FunctionCallAgent\n",
    "@fc_register(\"tool\")\n",
    "def get_current_weather(location: str, unit: Literal[\"fahrenheit\", \"celsius\"] = \"fahrenheit\"):\n",
    "    ...\n",
    "@fc_register(\"tool\")\n",
    "def get_n_day_weather_forecast(location: str, num_days: int, unit: Literal[\"celsius\", \"fahrenheit\"] = 'fahrenheit'):\n",
    "    ...\n",
    "llm = lazyllm.TrainableModule(\"internlm2-chat-20b\").start()  # or llm = lazyllm.OnlineChatModule()\n",
    "tools = [\"get_current_weather\", \"get_n_day_weather_forecast\"]\n",
    "fc = FunctionCall(llm, tools)\n",
    "query = \"What's the weather like today in celsius in Tokyo and Paris.\"\n",
    "ret = fc(query)\n",
    "print(f\"ret: {ret}\")\n",
    "agent = FunctionCallAgent(llm, tools)\n",
    "ret = agent(query)\n",
    "print(f\"ret: {ret}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13cb0dca",
   "metadata": {},
   "source": [
    "![image.png](18_images/img5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ef9da",
   "metadata": {},
   "source": [
    "##### React"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7911c17",
   "metadata": {},
   "source": [
    "React mainly includes the following processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e041e",
   "metadata": {},
   "source": [
    "1. Thought: After the Agent receives the query, it will first give the next action to be taken;\n",
    "2. Action: Agent will take and execute an action, such as using a tool (or continuing to think);\n",
    "3. Observation: Agent observes feedback from actions, such as the output of a tool;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92f2b4",
   "metadata": {},
   "source": [
    "The above process will continue to repeat until the query request is satisfied or the maximum number of iterations is reached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a37c5f",
   "metadata": {},
   "source": [
    "![image.png](18_images/img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d09d0b",
   "metadata": {},
   "source": [
    "The execution process of ReactAgent is the same as that of FunctionCallAgent. The only difference is that the prompt is different, and ReactAgent must have Thought output at each step, while an ordinary FunctionCallAgent may only output information about tool calls without content. Examples are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm.tools import fc_register, ReactAgent\n",
    "@fc_register(\"tool\")\n",
    "def multiply_tool(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "@fc_register(\"tool\")\n",
    "def add_tool(a: int, b: int):\n",
    "    return a + b\n",
    "tools = [\"multiply_tool\", \"add_tool\"]\n",
    "llm = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"DeepSeek-V3\")\n",
    "agent = ReactAgent(llm, tools)\n",
    "query = \"What is 20+(2*4)? Calculate step by step.\"\n",
    "res = agent(query)\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22986c06",
   "metadata": {},
   "source": [
    "![image.png](18_images/img7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76842b",
   "metadata": {},
   "source": [
    "##### PlanAndSolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600d2d6",
   "metadata": {},
   "source": [
    "PlanAndSolve mainly includes the following processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234514f",
   "metadata": {},
   "source": [
    "1. Plan: After the Agent receives the query, it will decompose the task into smaller subtasks;\n",
    "2. Action: Agent executes the current subtask;\n",
    "3. Observation: Agent observes the results of the current action and returns if the problem is solved. If it only solves the current subtask, it continues to execute the plan. If the current subtask is not solved, it re-plans the subsequent steps;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2a2869c",
   "metadata": {},
   "source": [
    "![image.png](18_images/img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab41e7",
   "metadata": {},
   "source": [
    "> Note: ② Action x1 in the above figure means that each action only executes one sub-task (not all sub-tasks will be executed, which is different from ② Action xN in the corresponding process of ReWOO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d4bdf",
   "metadata": {},
   "source": [
    "PlanAndSolveAgent consists of two components: first, breaking the entire task into smaller subtasks, and second, executing these subtasks according to the plan. The final result is output as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98230719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm.tools import fc_register, PlanAndSolveAgent\n",
    "@fc_register(\"tool\")\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "@fc_register(\"tool\")\n",
    "def add(a: int, b: int):\n",
    "    return a + b\n",
    "llm = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"DeepSeek-V3\")\n",
    "tools = [\"multiply\", \"add\"]\n",
    "agent = PlanAndSolveAgent(llm, tools=tools)\n",
    "query = \"What is 20+(2*4)? Calculate step by step.\"\n",
    "ret = agent(query)\n",
    "print(ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9167e2d",
   "metadata": {},
   "source": [
    "![image.png](18_images/img9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2898bb",
   "metadata": {},
   "source": [
    "##### ReWOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babb321",
   "metadata": {},
   "source": [
    "ReWOO (Reasoning WithOut Observation) mainly includes the following processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a02f9",
   "metadata": {},
   "source": [
    "1. Plan: After the Agent receives the query, it will generate a plan, which contains smaller subtasks decomposed by the task. The execution results between the subtasks are represented by placeholders;\n",
    "2. Action: Agent executes each sub-task in turn (calling tools) and fills the results into the placeholders of the schedule;\n",
    "3. Solve: Agent observes the feedback of all actions and returns the result response to the user;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1d04efa",
   "metadata": {},
   "source": [
    "![image.png](18_images/img10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814024d",
   "metadata": {},
   "source": [
    "ReWOOAgent consists of three parts: Planner, Worker and Solver. Among them, Planner uses predictive reasoning capabilities to create solution blueprints for complex tasks; Worker interacts with the environment through tool calls and fills actual evidence or observations into instructions; Solver processes all plans and evidence to formulate solutions to the original task or problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63419d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import fc_register, ReWOOAgent, deploy\n",
    "import wikipedia\n",
    "@fc_register(\"tool\")\n",
    "def WikipediaWorker(input: str):\n",
    "    try:\n",
    "        evidence = wikipedia.page(input).content\n",
    "        evidence = evidence.split(\"\\n\\n\")[0]\n",
    "    except wikipedia.PageError:\n",
    "        evidence = f\"Could not find [{input}]. Similar: {wikipedia.search(input)}\"\n",
    "    except wikipedia.DisambiguationError:\n",
    "        evidence = f\"Could not find [{input}]. Similar: {wikipedia.search(input)}\"\n",
    "    return evidence\n",
    "@fc_register(\"tool\")\n",
    "def LLMWorker(input: str):\n",
    "    llm = lazyllm.OnlineChatModule(stream=False)\n",
    "    query = f\"Respond in short directly with no extra words.\\n\\n{input}\"\n",
    "    response = llm(query, llm_chat_history=[])\n",
    "    return response\n",
    "tools = [\"WikipediaWorker\", \"LLMWorker\"]\n",
    "llm = lazyllm.TrainableModule(\"Qwen2-72B-Instruct-AWQ\").deploy_method(deploy.vllm).start()\n",
    "agent = ReWOOAgent(llm, tools=tools)\n",
    "query = \"What is the name of the cognac house that makes the main ingredient in The Hennchata?\"\n",
    "ret = agent(query)\n",
    "print(ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3736efb1",
   "metadata": {},
   "source": [
    "![image.png](18_images/img11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f43eb2",
   "metadata": {},
   "source": [
    "Let us briefly summarize it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b581de",
   "metadata": {},
   "source": [
    "|  | Function Call Agent | ReAct | PlanAndSolve | ReWOO |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Workflow | Loop within the maximum number of loops:<br><br> - Call the tool with trial parameters; <br> - Observe the tool output, and end the loop when the task is completed. | Maximum number of loops within the loop: <br><br> - Thinking; <br> - Test parameters to call the tool; <br> - Observe the tool output, and end the loop when the task is completed. | Maximum number of loops within the loop:<br><br> - (Re) plan and decompose the task;<br> - Call the tool to solve the current subtask;<br> - Observe the tool output, determine whether the subtask is completed, and end the cycle when the entire task is completed | - Plan and decompose the task;<br> - Call the tool to solve all subtasks step by step<br> - Comprehensive results of all steps for feedback |\n",
    "| Work characteristics | Simple and direct, the thinking process is invisible | Introducing thinking links, making thinking visible | Emphasis on task decomposition and dynamic adjustment of tasks | Emphasis on overall planning and comprehensive feedback |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ce264",
   "metadata": {},
   "source": [
    "#### Simplify Agent workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b92f5a",
   "metadata": {},
   "source": [
    "In Agent development, problems such as reinventing the wheel, inconsistent tool interfaces, and complex context management make the development process lengthy and inefficient. In order to solve these difficulties, we can improve development efficiency and lower the threshold through the \"**MCP Protocol**+**​LazyLLM\"** framework, allowing developers to focus on core business and innovative design, thus promoting the faster implementation of large model applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a5f9a",
   "metadata": {},
   "source": [
    "##### **Basic concepts of MCP protocol**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b080850",
   "metadata": {},
   "source": [
    "**MCP (Model Context Protocol) is an **open standard protocol** launched by Anthropic in November 2024, aiming to allow large language models to \"seamlessly connect\" external tools and data sources. To put it simply, MCP is a \"standardized weapon\" that is created to solve the pain points at the beginning. A more vivid metaphor is: **MCP is equivalent to the USB-C interface for AI applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75249856",
   "metadata": {},
   "source": [
    "Just as USB-C unifies the charging and data interfaces of electronic devices of different brands, MCP standardizes the way in which AI interacts with the external world, allowing models to efficiently call databases, tools, network searches and other resources in a standardized form, thereby achieving efficient linkage between models and external systems. In other words, in the past, every time a new tool was connected, there was a big problem of \"interface non-uniformity\". With MCP, it is like using peripherals with unified interfaces. **Plug it in and use it**. In this way, there is no need for secondary development**, and a variety of databases, Web APIs, file systems, GitHub... massive and powerful functions can all be easily accessed through this protocol."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3ea739",
   "metadata": {},
   "source": [
    "![image.png](18_images/img12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c40a5",
   "metadata": {},
   "source": [
    "In the past, if you wanted the AI ​​Agent to check the weather, read PDFs, and execute Python code, you might need to write a bunch of integrated code for each function, including the description of the tool, input parameters, etc., and package it into a \"Tool\" and give it to the model; with MCP, you only need to connect an MCP server that meets the requirements, and the model will automatically know what tools are available and how to call them, and the input and output formats are also unified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56641946",
   "metadata": {},
   "source": [
    "![image.png](18_images/img13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f5088",
   "metadata": {},
   "source": [
    "The whole process is like plugging in a **docking station** to a laptop. The protocol will help you with the tedious docking details such as HDMI, SD card, network cable and other interfaces. From now on, developers don't need to worry about those conversion processes. Therefore, the emergence of MCP has greatly improved the efficiency of AI Agent application development. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd1877",
   "metadata": {},
   "source": [
    "##### Technical architecture of MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a393f",
   "metadata": {},
   "source": [
    "From a technical architecture perspective, MCP follows a typical client-server model, which decouples the internal logic and external extension functions of AI applications into three core modules:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "619a0b84",
   "metadata": {},
   "source": [
    "![image.png](18_images/img14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4963f",
   "metadata": {},
   "source": [
    "**1. Host**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3012209",
   "metadata": {},
   "source": [
    "Refers to the host environment that runs the AI ​​application itself (similar to IDE plug-ins that support AI dialogue such as Cursor, desktop applications such as Claude Desktop, and the agent applications we created). Host is responsible for **providing the AI ​​interactive environment** and **starting the MCP Client** internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb253ed",
   "metadata": {},
   "source": [
    "**2. Client**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59155766",
   "metadata": {},
   "source": [
    "The client running inside the Host establishes a connection with the MCP Server and acts as a bridge between the AI ​​application and the external world. The MCP client maintains a 1:1 connection with the server. When the AI ​​model needs to call tools or obtain data, the client communicates with the server according to the protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682fed2",
   "metadata": {},
   "source": [
    "**3. Server**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd576b71",
   "metadata": {},
   "source": [
    "The MCP server provides specific functions and data, which is equivalent to the **peripherals** that the AI ​​brain can call remotely. Several types of content are usually exposed on a server for AI to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a3507",
   "metadata": {},
   "source": [
    "* Tools: Functions that allow large model calls. For example, code execution, web browsing, sending emails, etc., these capabilities can be packaged by the server as callable tools and provided to AI.\n",
    "* Resources: Data or content provided to the large model. For example, database records, file contents, screenshots of web browsing, etc. The server can send these external data to the AI ​​application through the protocol to serve as the context of LLM.\n",
    "* Prompts: preset reusable prompt word templates or interactive workflows. Server can store some commonly used prompt words and provide them to AI on demand to avoid writing complex prompts from scratch every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dd6cb",
   "metadata": {},
   "source": [
    "More details of MCP technical architecture can be found at: https://modelcontextprotocol.io/docs/concepts/architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8ed9c",
   "metadata": {},
   "source": [
    "Through the above architecture, problems that were solved patchworkly in the past now have clear protocol specifications to follow. So, what is the relationship between these terms, MCP, Agent, LLM, Tool Call...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554d922",
   "metadata": {},
   "source": [
    "* **LLM** is the **\"brain\"** of the Agent, which can output corresponding text content based on input information (such as system prompt words, user instructions, historical dialogue information, available tool set information, etc.), which may be staged tool call information, or may be the final output content after the task is completed.\n",
    "* **Tool Call** is a **tool calling capability**​ acquired by LLM after extensive training. This capability allows LLM to synthesize historical information and available tool information, dynamically make decisions and output formatted tool calling instructions (deciding which tool to use and what parameters to pass in when calling the tool). This instruction guides the Agent to correctly complete the tool call, thereby achieving specific actions (such as operating files, executing code), and obtaining necessary information (such as returning web crawler results).\n",
    "* **MCP Server** is a **tool provider** that follows the MCP protocol. It provides the Agent with a powerful tool set for LLM to identify and execute Tool Calls. At the same time, it receives the Tool Call instructions given by the Agent and safely interacts with external resources to implement specific actions or return specific information.\n",
    "* **Agent** serves as the **only entrance** for interaction between the agent application and the user. After receiving the task instructions, it will call LLM and various tools in an orderly manner to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f565da",
   "metadata": {},
   "source": [
    "##### Practice: Using MCP with LazyLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7da851",
   "metadata": {},
   "source": [
    "For MCP, LazyLLM provides two access methods: **direct access** and **deployment and remote access**​."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04e0f8",
   "metadata": {},
   "source": [
    "* ** Direct access: ** Directly give the startup configuration of the specified MCP Server to lazyllm.tools.MCPClient, start the Server in Stdio mode, and obtain the tool set that the Agent can call.\n",
    "* ** Deployment and remote access: ** For some scenarios where resource usage is high, or the started MCP Server is expected to be reusable, LazyLLM supports one-click deployment of MCP Server. With just one line of commands, the MCP Server can be started separately, and then the MCP Server can be remotely accessed in SSE mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c9e2",
   "metadata": {},
   "source": [
    "Specifically, the steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2c9b5",
   "metadata": {},
   "source": [
    "1. Configure all dependencies required by LazyLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c5b38",
   "metadata": {},
   "source": [
    "First refer to the Getting started section of https://docs.lazyllm.ai/zh-cn/latest/ to install LazyLLM and complete the environment configuration. At the same time, since the use of MCP Server relies on Node.js and npm, you can refer to https://nodejs.org/en/download to complete the installation and configuration of the latest version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf63d5",
   "metadata": {},
   "source": [
    "2. Utilize existing MCP services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3daa7",
   "metadata": {},
   "source": [
    "If you need to access existing MCP services (such as the geographical location service of Amap), you can directly connect through LazyLLM's MCPClient tool without deploying the server yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c42128",
   "metadata": {},
   "source": [
    "SSE URL access (taking AutoNavi MCP as an example): No need to start the local server, configure the client directly through the SSE long connection URL provided by the service provider. You need to replace \"xxx\" with your own key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77792d33",
   "metadata": {},
   "source": [
    "(Create key: https://lbs.amap.com/api/mcp-server/create-project-and-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm.tools.agent import ReactAgent\n",
    "from lazyllm.tools import MCPClient\n",
    "mcp_configs = {\n",
    "    \"amap_mcp\": {\n",
    "        \"url\": \"http://mcp.amap.com/sse?key=xxx\"\n",
    "    }\n",
    "}\n",
    "client = MCPClient(command_or_url=mcp_configs[\"amap_mcp\"][\"url\"])\n",
    "llm = lazyllm.OnlineChatModule(source='qwen', model='qwen-max-latest', stream=False)\n",
    "agent = ReactAgent(llm=llm.share(), tools=client.get_tools(), max_retries=15)\n",
    "print(agent(\"Check the weather in Beijing\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "839cc0f3",
   "metadata": {},
   "source": [
    "![image.png](18_images/img15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c2375",
   "metadata": {},
   "source": [
    "3. Use **direct access** to call MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cbe96",
   "metadata": {},
   "source": [
    "- Configuration acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa708af0",
   "metadata": {},
   "source": [
    "We select a file management MCP Server and obtain the startup configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d65949",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "{  \n",
    "    \"mcpServers\": {    \n",
    "        \"filesystem\": {     \n",
    "            \"command\": \"npx\",      \n",
    "            \"args\": [        \n",
    "                \"-y\",        \n",
    "                \"@modelcontextprotocol/server-filesystem\",        \n",
    "                \"/Users/username/Desktop\"      \n",
    "            ]    \n",
    "        }  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8dd7b",
   "metadata": {},
   "source": [
    "Note that if you are using a Windows system, you need to use \"cmd\" for command, and \"/c\" needs to be added to the beginning of the startup parameters. There will be some changes to the startup configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec214f9",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "{  \n",
    "    \"mcpServers\": {    \n",
    "        \"filesystem\": {     \n",
    "            \"command\": \"cmd\",      \n",
    "            \"args\": [\n",
    "                \"/c\", \n",
    "                \"npx\",         \n",
    "                \"-y\",        \n",
    "                \"@modelcontextprotocol/server-filesystem\",        \n",
    "                \"/Users/username/Desktop\"      \n",
    "            ]    \n",
    "        }  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ca6ae",
   "metadata": {},
   "source": [
    "- MCP access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824d8b4",
   "metadata": {},
   "source": [
    "You can then use LazyLLM's MCPClient tool to access the MCP Server (the path example here is /xxx/xxx/xxx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm.tools import MCPClient\n",
    "config = {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/xxx/xxx/xxx\"]}\n",
    "client = MCPClient(command_or_url=config[\"command\"], args=config[\"args\"], env=config.get(\"env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd555f",
   "metadata": {},
   "source": [
    "- Toolset acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34473641",
   "metadata": {},
   "source": [
    "```bash\n",
    ">>> tools = client.get_tools()\n",
    "Secure MCP Filesystem Server running on stdio\n",
    "Allowed directories: [ '/Users/username/Desktop' ]\n",
    ">>> tools\n",
    "[<function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269cad11c0>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91e520>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91d800>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91d8a0>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91e5c0>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91e0c0>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91d940>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91e480>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91db20>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91da80>, <function generate_lazyllm_tool.<locals>.dynamic_lazyllm_func at 0x7f269c91dda0>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6fda",
   "metadata": {},
   "source": [
    "**Code explanation**: Call client.get_tools() to obtain all the tools in the currently connected MCP Server (in an asynchronous environment, the following code can be changed to tools = await client.aget_tools()). At the same time, LazyLLM supports developers to obtain a specific tool set by passing in a list of tool names to the method, such as client.get_tools([\"tool_name1\", \"tool_name2\"])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d1dfd",
   "metadata": {},
   "source": [
    "- Tool call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bb6f6",
   "metadata": {},
   "source": [
    "```json\n",
    ">>> for t in tools:\n",
    "...     print(f\"\\nTool name:\\n{t.__name__}\\nTool desc:\\n{t.__doc__}\\nTool params:\\n{t.__annotations__}\\n\")\n",
    "... \n",
    "Tool name:\n",
    "read_file\n",
    "Tool desc:\n",
    "Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Only works within allowed directories.\n",
    "Args:    \n",
    "    path (str): type: string.\n",
    "Tool params:\n",
    "{'path': <class 'str'>}\n",
    "Tool name:\n",
    "write_file\n",
    "Tool desc:\n",
    "Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.\n",
    "Args:    \n",
    "    path (str): type: string.    \n",
    "    content (str): type: string.\n",
    "Tool params:\n",
    "{'path': <class 'str'>, 'content': <class 'str'>}\n",
    "......\n",
    "Tool name:\n",
    "list_allowed_directories\n",
    "Tool desc:\n",
    "Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.\n",
    "Args:    \n",
    "    No parameters.\n",
    "Tool params:\n",
    "{}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96f8dd",
   "metadata": {},
   "source": [
    "**Code explanation**: Traverse the tools obtained from MCP Server, where each member is a function. Each functional function has a function name (__name__), function description (__doc__, including function description and parameter description), and parameter declaration (__annotations__). When calling the corresponding function, you only need to pass in the correct parameters. Here are two examples of function calls:\n",
    "\n",
    "- Call the file reading tool read_file and pass in the required input parameter path to obtain the return information after reading the file;\n",
    "\n",
    "- Call the tool list_allowed_directories to obtain authorized paths. This tool does not require any input parameters. Pass in nothing to get the tool return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d28fd",
   "metadata": {},
   "source": [
    "```json\n",
    ">>> t1 = tools[0]\n",
    ">>> t1.__name__\n",
    "'read_file'\n",
    ">>> t1(path=\"xxx/xxx/xxx/test.md\")\n",
    "Secure MCP Filesystem Server running on stdio\n",
    "Allowed directories: [ 'xxx/xxx/xxx' ]\n",
    "'Tool call result:\\nReceived text message:\\nThis is a test file for LazyLLM and MCP.\\n\\nEnd\\n'\n",
    ">>> t2 = tools[-1]\n",
    ">>> t2.__name__\n",
    "'list_allowed_directories'\n",
    ">>> t2()\n",
    "Secure MCP Filesystem Server running on stdio\n",
    "Allowed directories: [ 'xxx/xxx/xxx' ]\n",
    "'Tool call result:\\nReceived text message:\\nAllowed directories:\\n/xxx/xxx/xxx'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7232c",
   "metadata": {},
   "source": [
    "4. Use LazyLLM to deploy MCP Server and access it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854d76a",
   "metadata": {},
   "source": [
    "LazyLLM supports one-click deployment of MCP Server. With just one line of command, the MCP Server can be started separately. The main program can use SSE mode to access the MCP Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3bbbc",
   "metadata": {},
   "source": [
    "* Deploy MCP Server with one click\n",
    "  \n",
    "Select the browser tool playwright (https://github.com/microsoft/playwright-mcp) to obtain configuration information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea821407",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "{  \n",
    "    \"mcpServers\": {    \n",
    "        \"playwright\": {      \n",
    "            \"command\": \"npx\",      \n",
    "            \"args\": [        \n",
    "                \"@playwright/mcp@latest\"     \n",
    "            ]    \n",
    "        }  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2989034",
   "metadata": {},
   "source": [
    "In the command line, you only need to use the \"lazyllm deploy mcp\\_server xxxxxx\" command and configure the host and port to complete the deployment of MCP Server. Since the Linux environment does not have a GUI, here is a demonstration of the startup command in the Windows environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cad581",
   "metadata": {},
   "source": [
    "```bash\n",
    "lazyllm deploy mcp_server --sse-port 11238 cmd -- /c npx @playwright/mcp@latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662ab44",
   "metadata": {},
   "source": [
    "After startup it looks like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d49fef1a",
   "metadata": {},
   "source": [
    "![image.png](18_images/img16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2679d0",
   "metadata": {},
   "source": [
    "* Access the deployed MCP Server\n",
    "  \n",
    "We can pass in the URL in other programs and access the MCP Server through SSE. Note that the URL here needs to be added with '/sse', otherwise it will not work properly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5cf69",
   "metadata": {},
   "source": [
    "```bash\n",
    ">>> config = {\"url\": \"http://127.0.0.1:11238/sse\"}\n",
    ">>> client = MCPClient(command_or_url=config[\"url\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30bcac",
   "metadata": {},
   "source": [
    "After accessing the MCP Server using the above method, the specific tool acquisition and tool calling methods are consistent with direct access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49c5af",
   "metadata": {},
   "source": [
    "5.LazyLLM calls the MCP tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00909982",
   "metadata": {},
   "source": [
    "Step 1: Get a list of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95975de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = client.get_tools() # Synchronous acquisition\n",
    "# Or tools = await client.aget_tools() # Asynchronous environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69f8aa",
   "metadata": {},
   "source": [
    "Step 2: View tool details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tools:\n",
    "    print(f\"Tool name: {t.__name__}\")\n",
    "    print(f\"Tool desc: {t.__doc__}\")\n",
    "    print(f\"Tool params: {t.__annotations__}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ed77b",
   "metadata": {},
   "source": [
    "Step 3: Call the MCP tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f3358",
   "metadata": {},
   "source": [
    "Take the file reading tool as an example, assuming tools[0] is read\\_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef771ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tools[0]\n",
    "result = t1(path=\"xxx/xxx/xxx/test.md\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "538bc734",
   "metadata": {},
   "source": [
    "![image.png](18_images/img17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc54c6",
   "metadata": {},
   "source": [
    "6.LazyLLM+MCP agent Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ae927",
   "metadata": {},
   "source": [
    "Next, we use filesystem+playwright, combined with LazyLLM’s Agent module, to create an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2206feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "import lazyllm.tools.agent \n",
    "from lazyllm.tools import ReactAgent\n",
    "import MCPClient\n",
    "if __name__ == \"__main__\":    \n",
    "    mcp_configs = {        \n",
    "        \"file_system\": {            \n",
    "            \"command\": \"cmd\",            \n",
    "            \"args\": [                \n",
    "                \"/c\",                \n",
    "                \"npx\",                \n",
    "                \"-y\",                \n",
    "                \"@modelcontextprotocol/server-filesystem\",                \n",
    "                \"./\"            \n",
    "            ]        \n",
    "        },        \n",
    "        \"play_wright\": {            \n",
    "            \"url\": \"http://127.0.0.1:11244/sse\"        \n",
    "        }    \n",
    "    }    \n",
    "client1 = MCPClient(command_or_url=mcp_configs[\"file_system\"][\"command\"], args=mcp_configs[\"file_system\"][\"args\"])    \n",
    "client2 = MCPClient(command_or_url=mcp_configs[\"play_wright\"][\"url\"])    \n",
    "llm = lazyllm.OnlineChatModule(source=\"deepseek\")    \n",
    "agent = ReactAgent(llm=llm.share(), tools=client1.get_tools()+client2.get_tools(), max_retries=15)    \n",
    "print(agent(\"Browse Google News and write a today's news briefing, save it locally in markdown format.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc98d2",
   "metadata": {},
   "source": [
    "Through this practice, we can learn that the emergence of MCP Server directly eliminates the cost of tool development and debugging in the Agent development process, greatly improving research and development efficiency**. LazyLLM provides flexible access methods to MCP, allowing developers to greatly reduce the cost of using MCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb3b01",
   "metadata": {},
   "source": [
    "Summary: In the era of large models, development efficiency is the core competitiveness. You may be able to practice by reinventing the wheel from scratch, but in the process of actual implementation of AI applications, we should reserve precious time and brainpower for the parts that truly create value - such as business logic design, user experience optimization, innovative interaction methods, etc., rather than re-creating basic components such as tools and context splicing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325509f",
   "metadata": {},
   "source": [
    "**MCP** provides a set of efficient and unified **standard protocols**; **LazyLLM** provides a set of **flexible MCP access solutions**, allowing every developer to easily get started and quickly build their own smart Agent applications, so as to stand on the \"shoulders\" of the **community and open source ecosystem** to see further and do more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aaa7a4",
   "metadata": {},
   "source": [
    "##### Treat MCP rationally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365dfa2",
   "metadata": {},
   "source": [
    "Although MCP simplifies the development process, it is important to note its limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d8572",
   "metadata": {},
   "source": [
    "- Dependency risk: Over-reliance on third-party MCP services may cause the business to be subject to external stability and policy changes.\n",
    "- Tool selection: MCP does not solve a dilemma of the current Agent: when there are many tools, how to quickly and accurately select the most appropriate tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424b405",
   "metadata": {},
   "source": [
    "Developers should weigh their choices based on actual needs, give priority to trying MCP in lightweight scenarios, and gradually verify its applicability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d13e931",
   "metadata": {},
   "source": [
    "![image.png](18_images/img18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59246b7",
   "metadata": {},
   "source": [
    "### 3. Introduction to Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5fb9c",
   "metadata": {},
   "source": [
    "#### Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d871e67",
   "metadata": {},
   "source": [
    "Agentic RAG is an extension of RAG that enhances the functionality of RAG by introducing AI agents, allowing the system to perform more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a225a1",
   "metadata": {},
   "source": [
    "For example, if RAG is a candidate who brings **books** to the exam, and AI Agent is an **expert**, then Agentic RAG is a candidate who brings **experts** to the exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ec8e3",
   "metadata": {},
   "source": [
    "To put it simply, in the figure below, a single LLM is like a student taking a closed-book exam; if we bring a book to this student, we can get a RAG; if we replace the book with an expert, we can get an Agentic RAG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f4939e2",
   "metadata": {},
   "source": [
    "![image.png](18_images/img19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d9243",
   "metadata": {},
   "source": [
    "#### Basic architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01a670",
   "metadata": {},
   "source": [
    "Agentic RAG is a RAG that introduces AI agents. In the previous example, we replaced RAG's search component (Retreival Component) with a single AI agent. In addition, we can also replace the search component with multiple AI agents, or even replace the Generative Component with an AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d14d51",
   "metadata": {},
   "source": [
    "##### Single Agent RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150684df",
   "metadata": {},
   "source": [
    "Below is a common Agentic RAG, in which the AI ​​Agent module provides two plug-in knowledge bases, a network search tool, a calculator and a database, so that the agent can decide where to retrieve information based on contextual needs. And if satisfactory information cannot be obtained in a round of retrieval, the agent can also search again (it can automatically change the search keywords, select different tools, etc.)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cced807",
   "metadata": {},
   "source": [
    "![image.png](18_images/img20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cfb70",
   "metadata": {},
   "source": [
    "In Agentic RAG, AI agents can be integrated into the retrieval component to form a Retrieval Agent. The retrieval process becomes intelligent, and the agent can loop through the query and optimize the results dynamically. At the same time, intelligent agents can access multiple tools such as networks and databases to break through the limitations of a single knowledge base and obtain richer and more accurate contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124aa91",
   "metadata": {},
   "source": [
    "The workflow of a single Agent RAG can be broken down into:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff0c45",
   "metadata": {},
   "source": [
    "- User input Query → Agent dynamically plans retrieval strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f574a5c",
   "metadata": {},
   "source": [
    "- Multiple searches (changing keywords/tools) → multi-source data fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60674",
   "metadata": {},
   "source": [
    "- Result enhancement → LLM generated reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a2843",
   "metadata": {},
   "source": [
    "After the introduction of agents, the query process is automated and intelligent, and the system can independently perform multiple rounds of searches, improving the information matching effect without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae36f5",
   "metadata": {},
   "source": [
    "##### Multiple Agent RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe54d0",
   "metadata": {},
   "source": [
    "We can also bring in a team of experts! Yes, it is a multi-Agent intelligence. In the figure below, the Retrieval Agent A expert is responsible for retrieval of two knowledge bases, the Retrieval Agent B expert is responsible for network search, and the Retrieval Agent C expert is responsible for the search of two databases. They are all search experts for various data sources. Finally, there is a Retreival Agent expert as the commander-in-chief, who is good at allocating search tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d050b5a",
   "metadata": {},
   "source": [
    "![image.png](18_images/img21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf550b7",
   "metadata": {},
   "source": [
    "If you want, of course we can also replace the generation module with an AI agent, as shown in the figure below. In this way, we have two experts, one expert is responsible for retrieval, and the other expert is responsible for generating content. As shown in the figure below, the retrieval expert has many ways to make independent decisions to retrieve information. The generation expert can also generate content while searching. If it feels that the generated content is not satisfactory, it will automatically regenerate it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f46c5dd",
   "metadata": {},
   "source": [
    "![image.png](18_images/img22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b836b",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc9ee4",
   "metadata": {},
   "source": [
    "In Agentic RAG, AI agents can be integrated into different components. It is generally common to replace the retrieval component with an AI agent (become: Retrieval Agent). This also means that the retrieval component will become intelligent and can continuously retrieve according to the query to obtain richer and more accurate context. At the same time, because AI agents can access many tools, this greatly enhances their retrieval capabilities. Even if suitable content cannot be retrieved in the knowledge base, AI agents can obtain more content from the network, databases, or other accessible tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ffef2",
   "metadata": {},
   "source": [
    "Let us take a single Agent RAG as an example, as shown in the figure below, to see how retrieval is completed under different agent workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f6889",
   "metadata": {},
   "source": [
    "* First, a query is passed to the agent ①.\n",
    "    * If the agent is a Function Call Agent, then it will continuously call tools based on the query, and observe the queried information, and continue to loop ② until it queries information that satisfies it, or reaches the maximum number of cycles;\n",
    "    * If the agent is React, then it first thinks based on the query, then starts to call the tool, and observes the queried information, and continues to cycle ② until it queries information that satisfies it, or reaches the maximum number of cycles;\n",
    "    * If the agent is PlanAndSolve, then it will first make a plan based on the query and decompose the query task into subtasks, and then it will start to execute the subtasks, such as calling a tool to query the knowledge base. After the knowledge base returns the information, it will observe the results. If the results are not satisfied, it will re-modify the plan. If the results are OK, it will continue to execute the next subtask along the plan, and this will continue to cycle ② until it finally completes all the tasks it has formulated to obtain the query information;\n",
    "    * If the agent is ReWOO, then it will also make a plan based on the query, decompose the query task into subtasks, then it will execute all the subtasks in sequence②, and finally it will synthesize all the execution results to give the results of its query.\n",
    "* After the agent queries the information, it returns to the classic RAG workflow: the queried information (already the result of the query fusion and enhancement by the agent) ③ will be sent to the LLM to complete the content generation task ④."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8269d70",
   "metadata": {},
   "source": [
    "At this point, a single-Agent RAG workflow is completed. From this we can see that in the query stage, since we introduced the agent, the query becomes more intelligent. The agent will continue to query on its own. We don't have to worry about the query process and worry about not finding matching information after searching only once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "259c1376",
   "metadata": {},
   "source": [
    "![image.png](18_images/img23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048247a",
   "metadata": {},
   "source": [
    "##Introduce motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335b828",
   "metadata": {},
   "source": [
    "Agentic RAG simply replaces its components with agents in the workflow of the original RAG. Why is this so? Why create Agentic RAG? Or why introduce agents into RAG? A very simple reason is to make it more powerful! More intelligent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ebafb6",
   "metadata": {},
   "source": [
    "![image.png](18_images/img24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3771dc",
   "metadata": {},
   "source": [
    "1. Classic RAG only performs a single query. If the appropriate document information cannot be recalled in a single time, the effect of the subsequent generation process cannot be guaranteed. However, Agentic RAG can perform multiple queries. If the recall effect is not good, the agent will automatically change the representation method or change the tool for retrieval;\n",
    "2. The data source of classic RAG is very single, often with only one knowledge base. But Agentic RAG can access a large number of knowledge bases, and more than that, it can also access databases and even Internet searches, which means that Agentic RAG's data sources are diverse (multi source);\n",
    "    - Diverse data sources can not only supplement the insufficient information of a single data source, but also provide more information;\n",
    "    - Diverse data sources can also provide mutual corroboration of the queried information to ensure the accuracy of the query results;\n",
    "3. Agentic RAG also has the ability to call multiple tools, which is full of unlimited functions (multi-function), and it can process and process information;\n",
    "4. Agentic RAG More importantly, it can make intelligent decisions (smart decision-making)! It can automatically formulate plans to implement complex query processes. The whole process doesn't require us to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81144424",
   "metadata": {},
   "source": [
    "You can imagine this is the difference between taking a textbook with you and taking an exam with an expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bef9e",
   "metadata": {},
   "source": [
    "## Build implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de822b",
   "metadata": {},
   "source": [
    "Let’s start with a basic RAG, then demonstrate how to register tools in LazyLLM and use React AI agents, and finally combine the two to implement a simple Agentic RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46398483",
   "metadata": {},
   "source": [
    "### 1. Build a basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f3b37",
   "metadata": {},
   "source": [
    "Based on the previous tutorial, we can use LazyLLM to quickly build a RAG application. The logic of the application is as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd9cc6d",
   "metadata": {},
   "source": [
    "![image.png](18_images/img25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4ad27",
   "metadata": {},
   "source": [
    "The specific code is as follows. In this RAG, we set up a retriever Retriever and Reranker to retrieve the knowledge base. [Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter18/basic_rag.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Document, Retriever, Reranker\n",
    "\n",
    "prompt = 'You will play the role of an AI Q&A assistant and complete a dialogue task. In this task, you need to provide your answer based on the given context and question.'\n",
    "\n",
    "documents = Document(dataset_path=\"rag_master\", embed=OnlineEmbeddingModule(), manager=False)\n",
    "documents.create_node_group(name=\"sentences\", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)\n",
    "\n",
    "with pipeline() as ppl:\n",
    "    ppl.retriever = Retriever(documents, group_name=\"sentences\", similarity=\"cosine\", topk=1)\n",
    "    ppl.reranker = Reranker(\"ModuleReranker\", model=OnlineEmbeddingModule(type=\"rerank\"), topk=1, output_format='content', join=True) | bind(query=ppl.input)\n",
    "    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)\n",
    "    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(prompt, extra_keys=[\"context_str\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lazyllm.WebModule(ppl, port=range(23467, 24000)).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601f8a8",
   "metadata": {},
   "source": [
    "Let's run it and see the results:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31c93ec2",
   "metadata": {},
   "source": [
    "![image.png](18_images/img26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30ceec",
   "metadata": {},
   "source": [
    "### 2. AI agent React"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfca7e4",
   "metadata": {},
   "source": [
    "Agentic RAG is a RAG that introduces AI agents. Here let us use LazyLLM to register a fake knowledge base search tool and implement a React:\n",
    "\n",
    "[Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/react.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lazyllm\n",
    "from lazyllm import fc_register, ReactAgent\n",
    "\n",
    "@fc_register(\"tool\")\n",
    "def search_knowledge_base(query: str):\n",
    "    '''\n",
    "    Get info from knowledge base in a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query for search knowledge base.\n",
    "    '''\n",
    "    return \"invisible\"\n",
    "\n",
    "llm = lazyllm.OnlineChatModule(stream=False)\n",
    "\n",
    "tools = [\"search_knowledge_base\"]\n",
    "agent = ReactAgent(llm, tools)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    res = agent(\"What is the way of heaven?\")\n",
    "    print(\"Result: \\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade5750",
   "metadata": {},
   "source": [
    "Let's try to run it:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eb8cf3a",
   "metadata": {},
   "source": [
    "![image.png](18_images/img27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892dfcf2",
   "metadata": {},
   "source": [
    "With React, we can replace its tools with Retriever and Reranker in RAG as a real knowledge base. Make it callable to the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import (pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Reranker,\n",
    "                     Document, Retriever, fc_register, ReactAgent)\n",
    "\n",
    "documents = Document(dataset_path=\"rag_master\", embed=OnlineEmbeddingModule(), manager=False)\n",
    "documents.create_node_group(name=\"sentences\", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)\n",
    "with pipeline() as ppl_rag:\n",
    "    ppl_rag.retriever = Retriever(documents, group_name=\"sentences\", similarity=\"cosine\", topk=3)\n",
    "    ppl_rag.reranker = Reranker(\"ModuleReranker\", model=OnlineEmbeddingModule(type=\"rerank\"), topk=1, output_format='content', join=True) | bind(query=ppl_rag.input)\n",
    "\n",
    "@fc_register(\"tool\")\n",
    "def search_knowledge_base(query: str):\n",
    "    '''\n",
    "    Get info from knowledge base in a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query for search knowledge base.\n",
    "    '''\n",
    "    return ppl_rag(query)\n",
    "\n",
    "tools = [\"search_knowledge_base\"]\n",
    "llm = lazyllm.OnlineChatModule(stream=False)\n",
    "\n",
    "agent = ReactAgent(llm, tools)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    res = agent(\"What is the way of heaven?\")\n",
    "    print(\"Result: \\n\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407ed2b",
   "metadata": {},
   "source": [
    "The running results are as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58a9217b",
   "metadata": {},
   "source": [
    "![image-2.png](18_images/img28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280439ce",
   "metadata": {},
   "source": [
    "### 3. Implement Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c775b57",
   "metadata": {},
   "source": [
    "Let's replace RAG's retrieval component with React with a single knowledge base and implement the following logic (for simplicity here, only one knowledge base is used as a tool)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ca3b3f",
   "metadata": {},
   "source": [
    "![image.png](18_images/img29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c81cd9",
   "metadata": {},
   "source": [
    "The corresponding code is as follows:\n",
    "\n",
    "[Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_react.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import (pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Reranker,\n",
    "                     Document, Retriever, fc_register, ReactAgent)\n",
    "\n",
    "prompt = 'You will play the role of an AI Q&A assistant and complete a dialogue task. In this task, you need to provide your answer based on the given context and question.'\n",
    "\n",
    "documents = Document(dataset_path=\"rag_master\", embed=OnlineEmbeddingModule(), manager=False)\n",
    "documents.create_node_group(name=\"sentences\", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)\n",
    "with pipeline() as ppl_rag:\n",
    "    ppl_rag.retriever = Retriever(documents, group_name=\"sentences\", similarity=\"cosine\", topk=3)\n",
    "    ppl_rag.reranker = Reranker(\"ModuleReranker\", model=OnlineEmbeddingModule(type=\"rerank\"), topk=1, output_format='content', join=True) | bind(query=ppl_rag.input)\n",
    "\n",
    "@fc_register(\"tool\")\n",
    "def search_knowledge_base(query: str):\n",
    "    '''\n",
    "    Get info from knowledge base in a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query for search knowledge base.\n",
    "    '''\n",
    "    return ppl_rag(query)\n",
    "\n",
    "tools = [\"search_knowledge_base\"]\n",
    "\n",
    "with pipeline() as ppl:\n",
    "    ppl.retriever = ReactAgent(lazyllm.OnlineChatModule(stream=False), tools)\n",
    "    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)\n",
    "    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(prompt, extra_keys=[\"context_str\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lazyllm.WebModule(ppl, port=range(23467, 24000)).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaddeef",
   "metadata": {},
   "source": [
    "The effect is as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "364ffbe3",
   "metadata": {},
   "source": [
    "![image.png](18_images/img30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89eb6",
   "metadata": {},
   "source": [
    "At this point we have implemented a simple Agentic RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa8ed1",
   "metadata": {},
   "source": [
    "### 4. More attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df48d2",
   "metadata": {},
   "source": [
    "You can try using a different AI agent workflow to replace React above:\n",
    "\n",
    "- [FunctionCallAgent code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_functioncall.py)\n",
    "\n",
    "- [PlanAndSolveAgent code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_planandsolve.py)\n",
    "\n",
    "- [ReWOOAgent code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_rewoo.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f18a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import FunctionCallAgent, PlanAndSolveAgent, ReWOOAgent\n",
    "\n",
    "# Use FunctionCallAgent:\n",
    "ppl.retriever = FunctionCallAgent(lazyllm.OnlineChatModule(), tools)\n",
    "# Use PlanAndSolveAgent:\n",
    "ppl.retriever = PlanAndSolveAgent(lazyllm.OnlineChatModule(), tools)\n",
    "# Use ReWOOAgent:\n",
    "ppl.retriever = ReWOOAgent(lazyllm.OnlineChatModule(), tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392d35b",
   "metadata": {},
   "source": [
    "Here we try to replace ReactAgent with FunctionCallAgent, PlanAndSolveAgent, and ReWOOAgent respectively to see the effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df3e82",
   "metadata": {},
   "source": [
    "#### FunctionCallAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf3884",
   "metadata": {},
   "source": [
    "Effect of FunctionCallAgent:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dbdeef6",
   "metadata": {},
   "source": [
    "![image.png](18_images/img31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f187b",
   "metadata": {},
   "source": [
    "#### PlanAndSolveAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de4671",
   "metadata": {},
   "source": [
    "Effects of PlanAndSolveAgent:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b718386",
   "metadata": {},
   "source": [
    "![image.png](18_images/img32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a878b5",
   "metadata": {},
   "source": [
    "#### ReWOOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c5fdb",
   "metadata": {},
   "source": [
    "Effects of ReWOOAgent:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8f1915c",
   "metadata": {},
   "source": [
    "![image.png](18_images/img33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fa9a4",
   "metadata": {},
   "source": [
    "You can even introduce multiple AI agents and more RAG components! Give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe1d4a",
   "metadata": {},
   "source": [
    "## Extended Case: Multi-Agent RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac11b7",
   "metadata": {},
   "source": [
    "In order to improve the coverage and response quality of complex problems, a multi-Agent RAG architecture design can also be introduced:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f34af",
   "metadata": {},
   "source": [
    "🔧 Agent division of labor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb29f74",
   "metadata": {},
   "source": [
    "- Search Agent: Determine the search tool (local knowledge base/network search) based on the query content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b7a95",
   "metadata": {},
   "source": [
    "- Agent A (knowledge base expert): Responsible for efficient retrieval of local knowledge base, giving priority to structured and stable information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab3d50",
   "metadata": {},
   "source": [
    "- Agent B (web search expert): performs web search, extracts data content, and writes it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f386c6b",
   "metadata": {},
   "source": [
    "After the retrieval is completed, all results are sent to LLM to generate a response to ensure language quality and context consistency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbb7b058",
   "metadata": {},
   "source": [
    "![image.png](18_images/img34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8554f5",
   "metadata": {},
   "source": [
    "MCP network search tool definition and registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP-Search Web and Save Local\n",
    "mcp_client1 = lazyllm.tools.MCPClient(command_or_url=\"python\", args=[\"-m\", \"mcp_server_fetch\"],)\n",
    "search_agent = CustomReactAgent(llm=lazyllm.OnlineChatModule(source=\"sensenova\", stream=False),\n",
    "    stream=False, custom_prompt=search_prompt, tools=mcp_client1.get_tools())\n",
    "\n",
    "@fc_register(\"tool\")\n",
    "def search_web(query: str):\n",
    "    '''\n",
    "    Perform targeted web content retrieval using a combination of search terms and URL.\n",
    "    This tool processes both natural language requests and specific webpage addresses \n",
    "    to locate relevant online information.\n",
    "    Args:\n",
    "        query (str): Combined input containing search keywords and/or target URL \n",
    "                   (e.g., \"AI news from https://example.com/tech-updates\")\n",
    "    '''\n",
    "    query += search_prompt\n",
    "    res = search_agent(query)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4a692",
   "metadata": {},
   "source": [
    "RAG tool definition and registration + application orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Retriever\n",
    "documents = Document(dataset_path='path/to/kb', manager=False)\n",
    "documents.add_reader('*.json', process_json)\n",
    "with pipeline() as ppl_rag:\n",
    "    ppl_rag.retriever = Retriever(documents, Document.CoarseChunk,\n",
    "        similarity=\"bm25\", topk=1, output_format='content', join='='*20)\n",
    "@fc_register(\"tool\")\n",
    "def search_knowledge_base(query: str):\n",
    "    '''\n",
    "    Get info from knowledge base in a given query.\n",
    "    Args:\n",
    "        query (str): The query for search knowledge base.\n",
    "    '''\n",
    "    res = ppl_rag(query)\n",
    "    return res\n",
    "\n",
    "# Agentic-RAG:\n",
    "tools = ['search_knowledge_base', 'search_web']\n",
    "with pipeline() as ppl:\n",
    "    ppl.retriever = CustomReactAgent(lazyllm.OnlineChatModule(stream=False), tools, agent_prompt, stream=False)\n",
    "    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)\n",
    "    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(gen_prompt, extra_keys=[\"context_str\"]))\n",
    "# Launch: Web-UI\n",
    "lazyllm.WebModule(ppl, port=range(23467, 24000), stream=True).start().wait()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67391422",
   "metadata": {},
   "source": [
    "![image.png](18_images/img35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e46e9",
   "metadata": {},
   "source": [
    "## Multimodal Agentic RAG paper system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6506b8c3",
   "metadata": {},
   "source": [
    "![image.png](18_images/img36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a6f95",
   "metadata": {},
   "source": [
    "**Configure two MCP tools and Agent**\n",
    "\n",
    "[Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/courseware_codes/chapter18/mcp_agent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lazyllm\n",
    "from lazyllm import ReactAgent\n",
    "mcp_client1 = lazyllm.tools.MCPClient(\n",
    "    command_or_url=\"python\",\n",
    "    args=[\"-m\", \"mcp_simple_arxiv\"],\n",
    ")\n",
    "mcp_client2 = lazyllm.tools.MCPClient(\n",
    "    command_or_url=\"python\",\n",
    "    args=[\"-m\", \"mcp_server_calculator\"],\n",
    ")\n",
    "llm = lazyllm.OnlineChatModule(stream=False)\n",
    "paper_agent = ReactAgent(llm, mcp_client1.get_tools(), return_trace=True)\n",
    "calculator_agent = ReactAgent(llm, mcp_client2.get_tools(), return_trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf78efc",
   "metadata": {},
   "source": [
    "Two tools need to be installed in advance in the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af0ef8",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install mcp-simple-arxiv\n",
    "pip install mcp-server-calculator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ceaf47",
   "metadata": {},
   "source": [
    "**Application Orchestration**\n",
    "\n",
    "[Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/courseware_codes/chapter18/paper_assistant_multimodal.py#L25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build rag workflow and statistical analysis workflow\n",
    "rag_ppl = build_paper_rag()\n",
    "sql_ppl = build_statistical_agent()\n",
    "# Build a main workflow with knowledge Q&A and statistical Q&A capabilities\n",
    "def build_paper_assistant():\n",
    "    llm = OnlineChatModule(source='qwen', stream=False)\n",
    "    vqa = lazyllm.OnlineChatModule(source=\"sensenova\",\\\n",
    "        model=\"SenseNova-V6-Turbo\").prompt(lazyllm.ChatPrompter(gen_prompt))\n",
    "    with pipeline() as ppl:\n",
    "        ppl.ifvqa = lazyllm.ifs(\n",
    "            lambda x: x.startswith('<lazyllm-query>'),\n",
    "            lambda x: vqa(x), lambda x:x)\n",
    "        with IntentClassifier(llm) as ppl.ic:\n",
    "            ppl.ic.case[\"Paper Q&A\", rag_ppl]\n",
    "            ppl.ic.case[\"Statistics Q&A\", sql_ppl]\n",
    "            ppl.ic.case[\"calculator\", calculator_agent]\n",
    "            ppl.ic.case[\"Search for the latest papers on the web\", paper_agent]\n",
    "    return ppl\n",
    "if __name__ == \"__main__\":\n",
    "    main_ppl = build_paper_assistant()\n",
    "    lazyllm.WebModule(main_ppl, port=23459, static_paths=\"./images\", encode_files=True).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332dae81",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "<source src=\"./18_videos/Multimodal Agentic_RAG paper system demonstration.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

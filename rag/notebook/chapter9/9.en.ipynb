{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78b87d2",
   "metadata": {},
   "source": [
    "# Chapter 9: Fine-Tuning in Practice ‚Äî Help Large Models and Embedding Models Better Understand Your Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac6006",
   "metadata": {},
   "source": [
    "> In the previous tutorial, we improved the retrieval accuracy of the RAG system by optimizing the retrieval strategy, recall strategy, and query rewriting strategy based on large models. However, the final response results still need to be fused and processed by large models. The strength of the model directly affects the final results. This is like a good dish requires not only high-quality ingredients (an optimized retrieval module provides high-matching text), but also a good chef (a capable LLM that fuses information to generate answers). In this tutorial, we will further improve the accuracy of the RAG system, go into the generation module of the RAG system, discuss the role of large models in it, and introduce how to significantly improve the generation capabilities of large models through fine-tuning of LazyLLM, and even use small models to surpass general large models (i.e., train a good chef), thereby improving the reply quality of the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bae47",
   "metadata": {},
   "source": [
    "## The role of large models in the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f740a",
   "metadata": {},
   "source": [
    "### Review of RAG process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580332b",
   "metadata": {},
   "source": [
    "In the previous tutorial, we have been able to quickly build the RAG system. Let us quickly review the RAG operation process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4201fc",
   "metadata": {},
   "source": [
    "* The retrieval module will query the knowledge base based on the user's Query to recall Top-K related documents through vector similarity;\n",
    "* The generation module splices the search results with the user Query, and inputs LLM to generate the final response;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfc590",
   "metadata": {},
   "source": [
    "![image.png](9_images/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c38e70",
   "metadata": {},
   "source": [
    "From this we can see that the LLM large model mainly plays the role of content generation in the RAG system. Specifically we can summarize it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fee067",
   "metadata": {},
   "source": [
    "* ‚Äã**Semantic Understanding**‚Äã: Analyze the true intention of the query;\n",
    "* ‚Äã**Knowledge Fusion**‚Äã: Coordinate the fusion of original knowledge (knowledge inherent in the LLM large model) and new knowledge (retrieval content);\n",
    "* ‚Äã**Logical reasoning**‚Äã: Reason a reasonable result based on the intent and context of the query;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db766aa",
   "metadata": {},
   "source": [
    "> So there are two main factors that affect the accuracy of the RAG system:\n",
    "> \n",
    "> **Effectiveness of recall** + **Ability to generate model content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b3c01",
   "metadata": {},
   "source": [
    "In the previous tutorials, we have introduced in detail how to improve the recall effect (quick review: [Chapter 7: Search upgrade practice: build a ‚Äúsmarter‚Äù document understanding system by yourself!](../chapter7/7.en.ipynb), [Chapter 8: More than just cosine! The matching strategy determines the quality of your recall](../chapter8/8.en.ipynb)). In this issue, we will focus on enhancing the model capabilities of the generation module to improve the quality of the content generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed811920",
   "metadata": {},
   "source": [
    "### Impact of model capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5e92e",
   "metadata": {},
   "source": [
    "In a typical RAG architecture, the baseline capability of the large language model (LLM) directly affects the reliability of the final output of the system, and its performance bottlenecks are mainly reflected in the following three dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33f8c7",
   "metadata": {},
   "source": [
    "#### **1. Domain knowledge adaptability defects**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d3e4e",
   "metadata": {},
   "source": [
    "General-purpose large models (such as DeepSeek-R1, GPT-4, Claude-3) show strong capabilities in open domain knowledge understanding, but their performance drops significantly when facing vertical fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c0ae3",
   "metadata": {},
   "source": [
    "* ‚Äã**Difficulties in interpreting professional terms**‚Äã: For example, IC can refer to ‚ÄúIntensive Care‚Äù in the medical field, and ‚ÄúIntegrated Circuit‚Äù in the electronics field.\n",
    "\n",
    "* ‚Äã**Lack of long tail knowledge**‚Äã:\n",
    "    * In the medical field, general large models may be mainly exposed to data of common diseases during training, and have less exposure to data of rare diseases. As a result, when the model encounters cases of rare diseases, it may not be able to accurately identify or diagnose, leading to misdiagnosis or missed diagnosis;\n",
    "    * In natural language processing, general models are usually trained based on Mandarin or mainstream languages, and there is less training data for local dialects. As a result, the model may have misunderstandings or incomprehensibility when processing local dialects, affecting the communication effect;\n",
    "    * In cultural knowledge question and answer or recommendation systems, the general model has a good grasp of mainstream cultural knowledge, but lacks understanding of niche cultural knowledge. As a result, when users ask questions about niche culture, the model may not be able to give accurate or relevant answers.\n",
    "\n",
    "* **Limitations in domain reasoning ability**‚Äã:\n",
    "    * In the legal field, general large models are used to assist in case analysis. When dealing with complex legal cases, it may not be possible to understand the deep legal logic and the relationship between provisions. As a result, the analysis suggestions provided by the model may not be accurate or comprehensive enough, affecting legal decision-making;\n",
    "    * In the field of education, general large models are used to assist in solving mathematical problems. When dealing with advanced mathematical problems, the model may not be able to perform in-depth reasoning and calculations. The model cannot solve complex mathematical problems, or the answers given may be wrong;\n",
    "    * In the field of scientific research, general large models are used to assist in the design of experiments. The model may lack an in-depth understanding of a specific scientific field and cannot take into account all experimental variables and potential effects. As a result, the designed experiments may be flawed and fail to achieve the expected research goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50ed3d",
   "metadata": {},
   "source": [
    "#### **2. Weak structured output control**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8e002",
   "metadata": {},
   "source": [
    "When the system requires fixed format output (such as JSON data tables, standardized reports), the model is susceptible to two key issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089bc0e7",
   "metadata": {},
   "source": [
    "* ‚Äã**Format Drift Phenomenon**‚Äã:\n",
    "    * In financial data analysis, the model needs to output a standardized JSON format data table. However, because the model does not grasp the format details accurately enough, the nesting level of some fields in the output JSON data is incorrect. For example, data that should belong to subfields is placed directly under the parent field, causing data parsing to fail;\n",
    "    * In the field of e-commerce, the model is responsible for generating JSON data of product information. However, due to format drift, unnecessary spaces, line breaks, or missing quotation marks appear in the data output by the model, causing errors in the front-end system's parsing and affecting product display;\n",
    "    \n",
    "* ‚Äã**Hallucination Interference**‚Äã:\n",
    "    * In the medical report generation scenario, the model needs to output a report containing the patient's basic information, diagnosis results, treatment plan and other fields in a fixed format. However, the model exerts itself when generating the report, fabricating non-key field contents outside the format, such as adding non-existent examination items or treatment recommendations, resulting in distortion of the report content, which may interfere with the doctor's diagnosis;\n",
    "    * In the generation of legal documents, the model needs to output documents according to a standard template. However, due to the interference of hallucinations, the model added fictitious facts or legal provisions irrelevant to the case in the document, resulting in inaccurate document content and affecting the legal validity;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2d0ca",
   "metadata": {},
   "source": [
    "#### **3. Performance is limited by the deployment environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364967a",
   "metadata": {},
   "source": [
    "The knowledge base required by RAG is often private to users, and users prefer to deploy it locally, which means deploying local large models at the same time, and local deployment of large models requires computing power support. It is difficult for ordinary users to have strong computing power. At this time, with computing power resources, they often can only choose some smaller LLM models, such as 7B size models, and the basic capabilities of these smaller LLMs cannot be compared with LLMs of more than 600B, and the capabilities of the models are also weak. Therefore, privacy protection requirements force enterprises to adopt localized deployment solutions, but this creates computing power constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe2d8b",
   "metadata": {},
   "source": [
    "* **Model scale limit**: A single A100 server (80G video memory) supports the deployment of a maximum 70B parameter model, and the cloud can call 600B DeepSeek-R1 at the same cost;\n",
    "* ‚Äã**Generational differences in capabilities**‚Äã: For example, the accuracy of the LLaMA-7B model is significantly lower than the accuracy of LLaMA-65B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf63dd6",
   "metadata": {},
   "source": [
    "## Fine-tuning to improve large model capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f38ef6",
   "metadata": {},
   "source": [
    "The problems mentioned above can be solved to a certain extent through targeted fine-tuning strategies, which can significantly improve the domain adaptability and output standardization of the generation module in the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c5531",
   "metadata": {},
   "source": [
    "**üßê So what is fine-tuning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695122d",
   "metadata": {},
   "source": [
    "Fine-tuning is a lightweight technology based on pre-trained large models and secondary training with data in specific fields to adapt the model to professional scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8926a",
   "metadata": {},
   "source": [
    "### Fine-tune core methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87aff9",
   "metadata": {},
   "source": [
    "* Supervisory fine-tuning (SFT): Inject domain QA data (such as legal cases/medical reports)\n",
    "* Domain adaptation: LoRA low-rank adaptation technology, freezing original parameters + training adaptation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc9ebe",
   "metadata": {},
   "source": [
    "### Core Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd9e49",
   "metadata": {},
   "source": [
    "* Efficiency: After fine-tuning the 7B model, the performance on specific tasks is comparable to the untuned 70B model\n",
    "* Low cost: Parameter efficient fine-tuning (PEFT) only needs to update 0.1%~20% of the parameters\n",
    "* Controllability: Strengthen structured output and content output constraints (such as JSON format calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0aa3af",
   "metadata": {},
   "source": [
    "### Typical application scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37499700",
   "metadata": {},
   "source": [
    "**‚Äã(1)‚Äã**‚Äã**Enhanced domain knowledge adaptability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0f1cc",
   "metadata": {},
   "source": [
    "**a. ‚Äã**‚Äã‚Äã**Vertical domain knowledge injection**‚Äã: Construct a fine-tuned data set based on domain-specific data (such as medical cases, legal documents, scientific research papers), and use instruction fine-tuning (Instruction Tuning) to enable the model to learn the knowledge expression pattern in the domain. For example, in a medical scenario, use <symptom description, diagnosis suggestion> data to train a model to understand the correlation between medical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a3b0f",
   "metadata": {},
   "source": [
    "**b. ‚Äã**‚Äã‚Äã**Long-tail knowledge compensation**‚Äã: For rare terms or low-frequency scenes (such as dialect vocabulary, niche cultural concepts), construct question and answer pairs containing such knowledge, and enhance the model's ability to capture long-tail features through fine-tuning. For example, in dialect processing tasks, dialect-standard language control samples are injected into the training data to strengthen the model's generalization ability across language variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca64392",
   "metadata": {},
   "source": [
    "**‚Äã(2)‚Äã**‚Äã**Structural output control enhancement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8d92f",
   "metadata": {},
   "source": [
    "**a. ‚Äã**‚Äã‚Äã**Explicit learning of format tags‚Äã**‚Äã: Embed structured tags (such as `{{KEY_START}}diagnosis results{{KEY_END}}`) in the fine-tuning data to force the model to learn the separated expression of format and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8639dae",
   "metadata": {},
   "source": [
    "**b. ‚Äã**‚Äã‚Äã**Anti-interference training**‚Äã: Inject 20%-30% noise samples into the training set (such as randomly deleting quotation marks, disrupting the JSON hierarchy), requiring the model to maintain content accuracy while repairing the format. In this way, the model's robustness to format drift is improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48d894",
   "metadata": {},
   "source": [
    "**c. ‚Äã**‚Äã**Lightweight deployment adaptation environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e554c8",
   "metadata": {},
   "source": [
    "‚Äã**Small model capability enhancement**‚Äã: Through **progressive knowledge transfer**‚Äã, the capabilities learned by the large model in domain tasks are distilled into the small model. For example, use a large model to generate pseudo-labels for domain-related data (you can also directly use annotated data in relevant professional fields), and then fine-tune the small model based on this data, so that the 7B model can approach the performance of the 65B model on specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b19884",
   "metadata": {},
   "source": [
    "Here we give an example of a specific scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476a2d9",
   "metadata": {},
   "source": [
    "In the medical field, text information in medical reports (for example, \"The patient exhibits symptoms of hypertension and the systolic blood pressure is 150mmHg\") needs to be converted into JSON format, which contains key information such as symptoms and blood pressure readings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f82cd5",
   "metadata": {},
   "source": [
    "![image.png](9_images/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e74973",
   "metadata": {},
   "source": [
    "With fine-tuning, the following benefits can be achieved for this type of task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed394ae",
   "metadata": {},
   "source": [
    "**1. Accurate information extraction**\n",
    "\n",
    "- Fine-tuning can help the model more accurately extract structured information from natural language text, such as symptom type, numerical value, unit, etc.\n",
    "\n",
    "**2. Contextual understanding**\n",
    "\n",
    "- Information in natural language often depends on context, and fine-tuning allows the model to better understand these contexts and thus convert information more accurately.\n",
    "\n",
    "**3. Specific format requirements**\n",
    "\n",
    "- JSON structured text often has strict formatting requirements, and fine-tuning can help the model generate output that meets these requirements.\n",
    "\n",
    "**4. Handling of domain-specific terms**\n",
    "\n",
    "- Fine-tuning can help the model better understand and process professional terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b8671",
   "metadata": {},
   "source": [
    "Converting natural language to JSON structured text is a typical scenario where fine-tuning can significantly improve the performance and output quality of LLM. Through fine-tuning, the model can learn how to identify and extract key information from natural language and present this information in a predefined structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be0a79",
   "metadata": {},
   "source": [
    "Let's put the fine-tuned model into RAG, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f788c1",
   "metadata": {},
   "source": [
    "![image.png](9_images/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5164aac",
   "metadata": {},
   "source": [
    "## Principles and methods of LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630be7f6",
   "metadata": {},
   "source": [
    "We mentioned above that fine-tuning can solve various problems, so what specific fine-tuning methods are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc7808",
   "metadata": {},
   "source": [
    "In response to the inherent challenges of traditional full-parameter fine-tuning in terms of resource consumption and catastrophic forgetting (over-adjustment of pre-training weights will destroy the knowledge expression of the model in general fields), LorRA (Low-Rank Adaptation) fine-tuning provides an effective optimization approach and has become a common method in the field of model fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a48d4",
   "metadata": {},
   "source": [
    "### Overview of LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a4aa6",
   "metadata": {},
   "source": [
    "Let's take LoRA fine-tuning as an example to explain the application of fine-tuning in LazyLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb11d45",
   "metadata": {},
   "source": [
    "![image.png](9_images/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0088c5",
   "metadata": {},
   "source": [
    "LoRA fine-tuning is an advanced fine-tuning technology designed for Transformer models. Its core idea is to simulate and implement the fine-tuning process by introducing low-rank matrices $A$ and $B$ on the basis of keeping the pre-trained model parameters $W \\in \\mathbb{R}^{d \\times k}$ fixed.\n",
    "\n",
    "The uniqueness of this method is that it does not directly adjust the parameters $W \\in \\mathbb{R}^{d \\times k}$ in the original model (such as $W_0$, $W_1$, $W_2$, $W_3$), but introduces additional trainable parameters for each layer - the orange matrix $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$, only fine-tuning these low-rank matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547e068",
   "metadata": {},
   "source": [
    "### LoRA fine-tuning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0b159",
   "metadata": {},
   "source": [
    "1. Load the pre-trained model: First, select a pre-trained Transformer model as the basis.\n",
    "2. Introduce low-rank matrices: In each layer of the model, add trainable low-rank matrices $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$.\n",
    "3. Fine-tune low-rank matrices: Input task-specific data into the model, and use the backpropagation algorithm to update parameters for low-rank matrices $A$ and $B$.\n",
    "4. Evaluation and optimization: Evaluate the performance of the fine-tuned model on the validation set, and further optimize the model based on actual needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7428dd",
   "metadata": {},
   "source": [
    "### LoRA mathematical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78830f4",
   "metadata": {},
   "source": [
    "Going a step further, let's look at the mathematical analysis of LoRA. The LoRA (Low-Rank Adaptation) method is based on the low-rank matrix approximation theory and achieves efficient fine-tuning of parameters by freezing the parameters of the pre-trained model and injecting trainable low-rank matrices. At the mathematical level, for the pre-training weight matrix $W ‚àà R^{d√ók}$, LoRA decomposes its update amount into the product of two low-rank matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce69513",
   "metadata": {},
   "source": [
    "$$ŒîW = B ‚ãÖ A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6d05b",
   "metadata": {},
   "source": [
    "Among them, $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$, and $r \\ll \\min(d, k)$. Such decomposition results in updates with lower rank, thus reducing the number of parameters that need to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75545c1",
   "metadata": {},
   "source": [
    "During forward propagation, the output of the model becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31a686",
   "metadata": {},
   "source": [
    "$$\n",
    "h = Wx + \\dfrac{\\alpha}{r}BAx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b88e63",
   "metadata": {},
   "source": [
    "in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354f9c9",
   "metadata": {},
   "source": [
    "- $W$ is the original weight matrix of the pre-trained model\n",
    "- $x$ is the input\n",
    "- $B$ and $A$ are low-rank matrices (rank $r$)\n",
    "- $\\alpha$ is a scaling factor used to flexibly control the adjustment strength of the low-rank matrix to the original weights.\n",
    "- $r$ is rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4d340",
   "metadata": {},
   "source": [
    "‚Äã**1. The influence of rank $r$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8507ac8",
   "metadata": {},
   "source": [
    "* Parameter efficiency and model capacity\n",
    "\n",
    "    * $r$ determines the approximation ability of the low-rank matrix $BA$.\n",
    "\n",
    "        * A larger $r$ allows the matrix to capture more complex updates and improves the model's adaptability to tasks, but will increase the training parameters (the number of parameters is $r \\times (\\text{dim}_A + \\text{dim}_B)$), which may lead to overfitting.\n",
    "        * A smaller $r$ reduces the number of parameters and improves training speed, but may limit the model's expression ability.\n",
    "\n",
    "* Overfitting and generalization\n",
    "\n",
    "    * Small $r$: more suitable for small data sets, reducing the risk of overfitting, but may underfit complex tasks.\n",
    "    * Large $r$: Suitable for large data sets or complex tasks, but more data is needed to avoid overfitting.\n",
    "\n",
    "* Experience Points: Typically $r = 8$ or $16$ performs well on most tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70a11a",
   "metadata": {},
   "source": [
    "‚Äã**2. Effect of scaling factor $\\alpha$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2d315",
   "metadata": {},
   "source": [
    "* Update amplitude control\n",
    "\n",
    "    * $\\alpha$ adjusts the weight of the low-rank update term $BAx$.\n",
    "\n",
    "        * Larger $\\alpha$ enhances the impact of incremental updates, which may speed up convergence, but may also destroy pre-training knowledge;\n",
    "        * Smaller $\\alpha$ makes updates gentler, retaining more of the original model's capabilities, but requires longer training time.\n",
    "\n",
    "* Synergy with Rank\n",
    "\n",
    "    * The $\\frac{\\alpha}{r}$ design in the formula balances the effects of different $r$ values. For example, when $r$ increases, dividing by $r$ can prevent the update range from being too large and maintain training stability.\n",
    "\n",
    "* Experience value: Usually $\\alpha$ is set to 2 times $r$ (such as $\\alpha = 16$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37850c27",
   "metadata": {},
   "source": [
    "‚Äã**3. The influence of $\\frac{\\alpha}{r}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910adaf",
   "metadata": {},
   "source": [
    "* Update intensity and convergence speed\n",
    "\n",
    "    * $\\frac{\\alpha}{r}$ directly controls the overall contribution of the low-rank term:\n",
    "\n",
    "        * High ratio (such as large $\\alpha$ or small $r$): strong update intensity, suitable for quickly adapting to new tasks, but may cause gradient instability.\n",
    "        * Low ratio (such as small $\\alpha$ or large $r$): the update is gentle, suitable for retaining pre-training knowledge, and requires longer time for fine-tuning.\n",
    "\n",
    "* Relationship with learning rate\n",
    "\n",
    "    * $\\frac{\\alpha}{r}$ acts like an adaptive learning rate for low-rank terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41defb65",
   "metadata": {},
   "source": [
    "**4. Impact of freezing pre-training weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d622b2d",
   "metadata": {},
   "source": [
    "* Mitigation of catastrophic amnesia\n",
    "\n",
    "    * The original weight $W$ is frozen, and only $B$ and $A$ are trained to avoid destroying pre-training knowledge and significantly reduce the risk of overfitting.\n",
    "\n",
    "* Computational efficiency\n",
    "\n",
    "    * Reduce training parameters to the size of low-rank matrices (such as from hundreds of millions of parameters to millions), saving video memory and computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b7283",
   "metadata": {},
   "source": [
    "#### Parameter tuning suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1456b5",
   "metadata": {},
   "source": [
    "* Task complexity and data volume\n",
    "\n",
    "    * Simple tasks/small data: Choose small $r$ (e.g. $r=4$) and medium $\\alpha$.\n",
    "    * Complex tasks/big data: Try larger $r$ (e.g. $r=32$) and increase $\\alpha$.\n",
    "\n",
    "* Resource limitations\n",
    "\n",
    "    * When there is insufficient video memory, priority is given to reducing $r$ instead of reducing model size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d251b",
   "metadata": {},
   "source": [
    "#### Parameter summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f194f",
   "metadata": {},
   "source": [
    "* $r$: balances expression ability and parameter efficiency, determining the flexibility of model fine-tuning.\n",
    "* $\\alpha$: adjust the intensity of new knowledge injection, and jointly control training stability with $r$.\n",
    "* $\\frac{\\alpha}{r}$: directly affects the convergence speed and final effect, and needs to be adjusted according to task requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476a003",
   "metadata": {},
   "source": [
    "## Fine-tune large models based on LazyLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68186af",
   "metadata": {},
   "source": [
    "So how should we implement fine-tuning? Here we implement a LoRA fine-tuning based on LazyLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7d203",
   "metadata": {},
   "source": [
    "We choose a small model and use the training data in the CMRC2018 data set to fine-tune this small model so that it can have better Chinese reading comprehension information extraction capabilities. The models used in RAG before will answer questions based on the knowledge base information we retrieved. The answers are relatively divergent and the degree of freedom is relatively high. Here we hope that the models in RAG have the following characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dac6b9",
   "metadata": {},
   "source": [
    "1. Extract necessary information to answer based on user questions and recalled text paragraphs, without extending the information (corresponding to the above mentioned: weak structured output control and poor adaptability of domain knowledge);\n",
    "2. The model should be small, about 7B, for easy deployment, and a model that is not too large should not be used (corresponding to the contradiction between lightweight deployment requirements and reduced model accuracy mentioned above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc702d6b",
   "metadata": {},
   "source": [
    "Our overall fine-tuning steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d0029",
   "metadata": {},
   "source": [
    "![image.png](9_images/img5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3fa0c",
   "metadata": {},
   "source": [
    "* Question: The answers to the original RAG medium and large models are divergent and need to be streamlined + accurately follow the original text output;\n",
    "* Goal: Strengthen Chinese reading comprehension information extraction capabilities on the 7B small model\n",
    "*Model selection: InternLM2-Chat-7B.\n",
    "* Data preparation: Use CMRC2018 training data, focusing on Chinese question and answer and information extraction.\n",
    "* Fine-tuning method:\n",
    "    * LoRA strategy: freeze the original weights and only train low-rank matrices (BA) to reduce video memory usage.\n",
    "    * Control output: Strengthen the ability to follow the original content and streamline the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c953df",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006fa55",
   "metadata": {},
   "source": [
    "#### Dataset Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad00ba",
   "metadata": {},
   "source": [
    "We choose the CMRC2018 dataset, which consists of nearly 15,000 real-world questions annotated by human experts on **Wikipedia** paragraphs. In addition, the corresponding task of this data set is: \"Span-Extraction Reading Comprehension\". Based on a given document and a question, the model needs to extract the answer to the question from the document, where the answer is a continuous fragment of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57471bc",
   "metadata": {},
   "source": [
    "#### Dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa7f6d",
   "metadata": {},
   "source": [
    "The CMRC2018 data set mainly consists of three parts: test, validation and train. Here we use the train part as our training set, the test part as our evaluation set, and we do not use the validation part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06926f73",
   "metadata": {},
   "source": [
    "| **Dataset** | **Number of essays** | **Number of questions** | **Purpose** |\n",
    "| ------------------ | ------------------ | ------------------ | ---------------- |\n",
    "| test | 256 | 1002 | evaluation set |\n",
    "| train | 2403 | 10142 | training set |\n",
    "| validation | 848 | 3219 | not used |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58f695",
   "metadata": {},
   "source": [
    ">Note:\n",
    ">\n",
    ">* The knowledge base in the previous tutorial was built based on the passages in the test data set in **CMRC2018‚Äã**;\n",
    ">* Here we use the train data set in **CMRC2018‚Äã** for training;\n",
    ">* During the evaluation process, in order to avoid interference from the retrieval module in the RAG system, we use the control variable method, assuming that 100% recall is no problem, that is: we directly use the passage fragments and questions in the test data set in **CMRC2018‚Äã** to splice them together as the input of the large model LLM. The variables are two 7B models before and after fine-tuning to compare and see whether the fine-tuned model has improved its ability to extract Chinese reading comprehension information;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492e3c7",
   "metadata": {},
   "source": [
    "#### Data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea5b3d",
   "metadata": {},
   "source": [
    "The structures of test and train in the data set are consistent. We extract one of the data and see as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ebd50",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"id\": \"TRIAL_154_QUERY_0\",\n",
    "\"context\": \"Eugene Kangaroo (\\\"Macropus Eugenii\\\") is a small member of the Kangaroo family, and is usually the subject of research on kangaroos and marsupials. Eugene's kangaroos are distributed in the southern islands and west coast of Australia. Because they breed in large numbers on Kangaroo Island every season, destroying the living environment on Echidna Island, they are considered a pest. The Eugene's kangaroo was first discovered in Western Australia by survivors of a shipwreck in 1628. It is the earliest recorded discovery of kangaroos by Europeans, and may be the earliest discovery. There are three subspecies of Australian mammals: the Eugene kangaroo is very small, weighing only about 8 kilograms, and is suitable for breeding. There is a substance in the milk of the Eugene kangaroo, which may be a miracle drug and an improvement on penicillin. AGG01 is a protein that has been proven to be 100 times more effective than penicillin and can kill 99% of bacteria and fungi, such as Salmonella, Proteus vulgaris and Staphylococcus aureus.\n",
    "\"question\": \"Where are Eugene's kangaroos distributed?\",\n",
    "        \"answers\": {\n",
    "            \"text\": [\n",
    "\"Eugene's kangaroo is distributed in the southern islands and west coast of Australia\"\n",
    "            ],\n",
    "            \"answer_start\": [\n",
    "                52\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8d7e3",
   "metadata": {},
   "source": [
    "In the above data:\n",
    "\n",
    "* `context`: is a text segment;\n",
    "* `question`: is a question directed at the text segment;\n",
    "* `answers`: Answers to corresponding questions are given, including:\n",
    "* `text`: The specific content of the answer, derived from the text segment;\n",
    "* `answer_start`: The starting position of the answer in the text paragraph;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa1caa",
   "metadata": {},
   "source": [
    "#### Training set processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b2b6c",
   "metadata": {},
   "source": [
    "* We extract the article field `context` and the question field `question` from the original train data set, and splice them into the `instruction` field for fine-tuning. The splicing template is: \"Please use the original text of the following paragraph to answer the question\\\\n\\\\n### Known paragraph: {context}\\\\n\\\\n### Question: {question}\\\\n\".\n",
    "* Use the `answers` and its `text` fields in the original data as the fine-tuned `output` field\n",
    "* Since fine-tuning also requires an `input` field, which we do not need for this task, it is set to empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28345b8",
   "metadata": {},
   "source": [
    "[Code Github linküîó](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter9/run_cmrc.py#L51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53900029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for constructing QA prompts\n",
    "template = \"Please use the original text of the following paragraph to answer the question\\n\\n### Known paragraph: {context}\\n\\n### Question: {question}\\n\"\n",
    "\n",
    "def build_train_data(data):\n",
    "    \"\"\"Format training data using predefined template\"\"\"\n",
    "    extracted_data = []\n",
    "    for item in data:\n",
    "        extracted_item = {\n",
    "            \"instruction\": template.format(context=item[\"context\"], question=item[\"question\"]),\n",
    "            \"input\": \"\",\n",
    "            \"output\": item[\"answers\"][\"text\"][0]\n",
    "        }\n",
    "        extracted_data.append(extracted_item)\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735c4eb",
   "metadata": {},
   "source": [
    "We take a piece of processed data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd71d64",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "\"instruction\": \"Please use the original text of the following paragraph to answer the question\\n\\n### Known text: Huangdu (scientific name:) is a perennial twining vine with spherical or conical tubers in the leaf axils, with spherical or oval bulbils of varying sizes and yellowish-brown outer skin. It has checkered small transverse veins, with 7-9 obvious veins at the base; the base of the petiole is twisted and slightly wider, as long as the leaves or slightly shorter. It blooms in summer and autumn, and is dioecious. The fruiting period is from September to October in Oceania, Korea, and Africa. Asia, India, Japan, Taiwan, Myanmar and China's Jiangsu, Guangdong, Guangxi, Anhui, Jiangxi, Sichuan, Gansu, Yunnan, Hunan, Tibet, Henan, Fujian, Zhejiang, Guizhou, Hubei, Shaanxi and other places, growing at an altitude of 300 meters to 2,000 meters In rice-rich areas, it mostly grows along river valleys, valley ditches or on the edges of mixed woods. It has not yet been artificially introduced and cultivated in the Americas. It is an exotic species to the Americas and has the opportunity to multiply in farmland and climb tall trees to compete for sunlight. Potato. Huangyao (original Materia Medica), Shanzigu (plant names and facts), Lingyuzi Dioscorea (Russian, Latin, and Chinese seed plant names), Lingyuzi (Fora of Guangzhou, Hainan Flora), Huangyaozi (name of medicinal materials in Jiangsu, Anhui, Zhejiang, Yunnan and other provinces), Shanzigu (Chuxiong, Yunnan)\\n\\n### Question: What is the color of the skin of Huangdu? \\n\",\n",
    "        \"input\": \"\",\n",
    "\"output\": \"Tawny skin\"\n",
    "    },\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90da13e",
   "metadata": {},
   "source": [
    "#### Evaluation set processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2734c10",
   "metadata": {},
   "source": [
    "The fields in our evaluation set are basically retained. Only the answers field is processed and the content is extracted:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8422228",
   "metadata": {},
   "source": [
    "[Code Github linküîó](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter9/run_cmrc.py#L39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_data(data):\n",
    "    \"\"\"Extract necessary fields for evaluation dataset\"\"\"\n",
    "    extracted_data = []\n",
    "    for item in data:\n",
    "        extracted_item = {\n",
    "            \"context\": item[\"context\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answers\": item[\"answers\"][\"text\"][0]\n",
    "        }\n",
    "        extracted_data.append(extracted_item)\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43f181",
   "metadata": {},
   "source": [
    "We take a piece of processed data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1e1df",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "\"context\": \"Furong Cave is located on the banks of the Furong River in Jiangkou Town, Wulong County, Chongqing, 20 kilometers away from Wulong County. Furong Cave was discovered in 1993 and opened to tourists in 1994. It was listed as a national 4A tourist attraction in China in 2002, and in June 2007, it became a component of the Southern China Karst-Wulong Karst. It is the first karst cave in China to be listed as a World Natural Heritage. It is 2,846 meters long and famous for its numerous shafts and complete types of cave deposits. There is a rare group of karst shafts in the world within an area of about 20 square kilometers. There are at least 50 shafts over 100 meters scattered, among which Qikeng Cave has a depth of 920 meters, the longest in Asia. There are more than 70 kinds of sediments in Furong Cave, including almost all the karst cave sedimentary types named by scientists, among which the pool sediment is the essence. There is a \"gypsum\" at the east end of Furong Cave. \"Huazhi Cave\", the antler-shaped curled stone branches in the cave are 57 centimeters long, the longest in the world. The gypsum Huazhi Cave is currently permanently sealed. A Jiangkou Power Station Dam has been built on the Furong River adjacent to the entrance of Furong Cave. The impact of the reservoir's impoundment on groundwater circulation and the evolution of the karst landscape is currently difficult to estimate.\",\n",
    "\"question\": \"Where is Furong Cave located?\",\n",
    "\"answers\": \"The banks of Furong River in Jiangkou Town, Wulong County, Chongqing\"\n",
    "    },\n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae170d",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86cd90",
   "metadata": {},
   "source": [
    "After the data processing is complete, we can start fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486be773",
   "metadata": {},
   "source": [
    ">It is worth noting: LazyLLM supports fine-tuning, deployment, and inference in one package!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabd754",
   "metadata": {},
   "source": [
    "The fine-tuning related configuration code is mainly as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a392c",
   "metadata": {},
   "source": [
    "[Code GitHub linküîó](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter9/run_cmrc.py#L155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc45260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import finetune, deploy, launchers\n",
    "\n",
    "model = lazyllm.TrainableModule(model_path)\\\n",
    "    .mode('finetune')\\\n",
    "    .trainset(train_data_path)\\\n",
    "    .finetune_method((finetune.llamafactory, {\n",
    "        'learning_rate': 1e-4,\n",
    "        'cutoff_len': 5120,\n",
    "        'max_samples': 20000,\n",
    "        'val_size': 0.01,\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'num_train_epochs': 2.0,\n",
    "        'launcher': launchers.sco(ngpus=8)\n",
    "    }))\\\n",
    "    .prompt(dict(system='You are a helpful assistant.', drop_builtin_system=True))\\\n",
    "    .deploy_method(deploy.Vllm)\n",
    "model.evalset(eval_data)\n",
    "model.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8ec17",
   "metadata": {},
   "source": [
    "In the above code, LazyLLM's `TrainableModule` is used to implement: fine-tuning->deployment->inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8a5b2",
   "metadata": {},
   "source": [
    "* Model configuration:\n",
    "\n",
    "    * `model_path` specifies the model we want to fine-tune. Here we use Internlm2-Chat-7B and directly specify its path;\n",
    "\n",
    "* Fine-tuned configuration:\n",
    "    * `.mode` sets the startup fine-tuning mode `finetune`;\n",
    "    * `.trainset` sets the data set path for training. What is used here is the training set we processed earlier;\n",
    "    * `.finetune_method` sets which fine-tuning framework to use and its parameters. A tuple is passed in here (only two elements can be set):\n",
    "        * The first element specifies the fine-tuning framework used is Llama-Factory: `finetune.llamafactory`\n",
    "        * The second element is a dictionary containing the parameter configuration of the fine-tuning framework;\n",
    "\n",
    "* Inference configuration:\n",
    "    * `.prompt` sets the Prompt used during inference. Note that in order to be consistent with the system field in the fine-tuned Prompt, `drop_builtin_system` is turned on to replace the original system-prompt with \\`You are a helpful assistant.\\`\n",
    "    * `.deploy_method` sets the inference framework for deployment, and the vLLM inference framework is specified here;\n",
    "\n",
    "* Evaluation configuration:\n",
    "    * Here we use `.evalset` to configure the evaluation set we processed before;\n",
    "\n",
    "* Start task:\n",
    "    * `.update` triggers the start of the task: the model is fine-tuned first. After the fine-tuning is completed, the model will be deployed. After deployment, it will automatically use the evaluation set to go through inference to obtain the results;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a46a3",
   "metadata": {},
   "source": [
    "Some key parameters in fine-tuning are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817a261",
   "metadata": {},
   "source": [
    "| **Parameters** | **Function** | **Recommended settings** | **Tuning suggestions** |\n",
    "| --------------------------------- | ------------------ | -------------------- | -------------------------------- |\n",
    "| learning\\_rate | Control parameter update range | 1e-4~5e-5 | Take the smaller value for large models |\n",
    "| cutoff\\_len | Maximum context length | 5120 | Adjusted according to GPU memory |\n",
    "| max\\_samples | Maximum training sample size | 20000 | Note that too small will result in too little training data, |\n",
    "| per\\_device\\_train\\_batch\\_size | Single card batch size | 2 | Decrease when there is insufficient video memory |\n",
    "| num\\_train\\_epochs | Training rounds | 2.0 | Set according to task loss reduction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e0abd",
   "metadata": {},
   "source": [
    "In addition, we can also configure some LoRA-related parameters (LazyLLM has set a set of experience parameters by default, so it is not reflected in the above code. Here we show it as follows. You can try various parameters to make elixirs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992121f",
   "metadata": {},
   "source": [
    "| **Parameters** | **Function** | **Recommended settings** | **Description** |\n",
    "| ---------------- | ------------------------- | -------------------- | ------------------------ |\n",
    "| lora\\_alpha | LoRA scaling factor | 16 | - |\n",
    "| lora\\_dropout | LoRA dropout rate | 0.0 | - |\n",
    "| lora\\_rank | LoRA rank | 8 | - |\n",
    "| lora\\_target | LoRA target module of the total model | all | All linear modules in the model. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa7c61",
   "metadata": {},
   "source": [
    "### Effect evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b3593",
   "metadata": {},
   "source": [
    "After fine-tuning in the previous step and obtaining the inference results of the evaluation set, we need to compare the results with the correct answers of the evaluation set to confirm the effect of our fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ab5db",
   "metadata": {},
   "source": [
    "#### Evaluation purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d7c24",
   "metadata": {},
   "source": [
    "* ‚Äã**Verify the fine-tuning effect**‚Äã: Compare the model output with the standard answer, and quantify the model optimization results (whether it surpasses the general large model)\n",
    "* **Task Adaptability Test**‚Äã: Common indicators (faithfulness, Answer Relevance) are not applicable to the \"Chapter Fragment Extraction\" task and require customized evaluation\n",
    "* ‚Äã**Optimization Direction Guidance**‚Äã: Locate model shortcomings through indicator differences (such as complete consistency, semantic accuracy, original text dependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730b10c",
   "metadata": {},
   "source": [
    "#### Evaluation indicator design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9c854",
   "metadata": {},
   "source": [
    "![image.png](9_images/img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfedff",
   "metadata": {},
   "source": [
    "Here, based on the characteristics of our tasks, three-dimensional evaluation indicators are designed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2b3ba",
   "metadata": {},
   "source": [
    "* Exact Match\n",
    "* Semantic similarity (Cosine Score)\n",
    "* Origin Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1965b",
   "metadata": {},
   "source": [
    "Let us now introduce the design details of these three indicators in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20457fc8",
   "metadata": {},
   "source": [
    "##### **1. Exact matching rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1a4e8",
   "metadata": {},
   "source": [
    "We define exact matching as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44a3d2",
   "metadata": {},
   "source": [
    "$$\n",
    "EM = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(y_i = \\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fee20",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7df5a",
   "metadata": {},
   "source": [
    "* $N$: total number of test samples;\n",
    "* $y_i$: the standard answer of the $i$th sample;\n",
    "* $\\hat{y}_i$: model prediction results;\n",
    "* $\\mathbb{I}$: indicator function (takes 1 when there is a complete match, otherwise takes 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41675983",
   "metadata": {},
   "source": [
    "The characteristic of this indicator is that the prediction result and the standard answer need to be **completely consistent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecf417",
   "metadata": {},
   "source": [
    "Without considering the calculation of the average, the code to implement only one of them is simple, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2511da",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_score = 1 if output == true_v else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8887ba",
   "metadata": {},
   "source": [
    "**Code function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6c7d7",
   "metadata": {},
   "source": [
    "This code is used to determine whether the model's prediction result (`output`) for a single sample is completely consistent with the standard answer (`true_v`) of the sample, and give an exact matching score (`exact_score`) accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31dacb",
   "metadata": {},
   "source": [
    "**Code explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb040e06",
   "metadata": {},
   "source": [
    "* `output`: The prediction result of the model for a certain sample.\n",
    "* `true_v`: The standard answer for this sample.\n",
    "* `exact_score`: Exact match score, value is 1 or 0. If the prediction result is completely consistent with the standard answer, the score is 1; otherwise, the score is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13816a84",
   "metadata": {},
   "source": [
    "##### **2. Semantic similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856d363",
   "metadata": {},
   "source": [
    "We define semantic similarity as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a6d61",
   "metadata": {},
   "source": [
    "$$\n",
    "CS = \\dfrac{1}{N} \\sum_{i=1}^N \\max\\left(0, \\min\\left(1, \\dfrac{emb(y_i) \\cdot emb(\\hat{y}_i)}{\\|emb(y_i)\\| \\cdot \\|emb(\\hat{y}_i)\\|}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd04bfe",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b54b4c",
   "metadata": {},
   "source": [
    "* $N$: total number of test samples;\n",
    "* $y_i$: the standard answer of the $i$th sample;\n",
    "* $\\hat{y}_i$: model prediction results;\n",
    "* $emb()$: is a vector encoding based on the BGE model (bge-large-zh-v1.5), which can encode natural language into a vector, that is:\n",
    "\n",
    "  $$\n",
    "  emb(text) = BGE\\_Encoder(text)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526d6cb",
   "metadata": {},
   "source": [
    ">It is worth noting that this evaluation index truncates the original value of [-1,0). As long as the semantics of negative correlation are 0 points, the score can only be positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99574c",
   "metadata": {},
   "source": [
    "The corresponding code is implemented as follows. There is no averaging here, just one of the items. It is also assumed that the text has been vectorized by the BGE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce10a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine(x, y):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    product = np.dot(x, y)\n",
    "    norm = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "    raw_cosine = product / norm if norm != 0 else 0.0\n",
    "    return max(0.0, min(raw_cosine, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1139b",
   "metadata": {},
   "source": [
    "**Code function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd45f20",
   "metadata": {},
   "source": [
    "This code implements a function that calculates the cosine similarity between two vectors, and is particularly useful for evaluating the semantic similarity between model predictions and standard answers. This is the core calculation part in the above exact matching definition (CS) and is used for similarity calculation of a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8cd81",
   "metadata": {},
   "source": [
    "**Code explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38525b0",
   "metadata": {},
   "source": [
    "* $x$ and $y$: represent two vectors respectively. In practical applications, these two vectors are usually the result of vectorizing text by the BGE model, namely $emb(y_i)$ and $emb(\\hat{y}_i)$.\n",
    "\n",
    "* $\\text{np.dot}(x, y)$: Calculate the dot product of two vectors.\n",
    "\n",
    "* $\\text{np.linalg.norm}(x)$ and $\\text{np.linalg.norm}(y)$: Calculate the L2 norm (i.e. Euclidean norm) of vectors $x$ and $y$ respectively.\n",
    "\n",
    "* $\\frac{\\text{product}}{\\text{norm}}$: Calculate the original cosine similarity value.\n",
    "\n",
    "* $\\mathrm{raw\\_cosine}$: Stores the original cosine similarity value, but returns 0.0 directly if the denominator is 0 (that is, at least one of the two vectors is a zero vector).\n",
    "\n",
    "* $\\max(0.0, \\min(\\mathrm{raw\\_cosine}, 1.0))$: Truncate the original cosine similarity value to ensure that the result is within the range of $[0, 1]$, and the semantics of negative correlation are scored 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad030b",
   "metadata": {},
   "source": [
    "##### **3. Inclusion of the original text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6407127",
   "metadata": {},
   "source": [
    "We define the original text inclusion degree as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80534f49",
   "metadata": {},
   "source": [
    "$$\n",
    "OS = \\dfrac{1}{N} \\sum_{i=1}^N \\mathbb{I}\\left(\\forall w \\in \\hat{y},\\ w \\in Context \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337ff15",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6765a",
   "metadata": {},
   "source": [
    "* $N$: total number of test samples;\n",
    "* $\\hat{y}_i$: model prediction results;\n",
    "* $w$: represents each word in the prediction result;\n",
    "* $Context$: article content;\n",
    "* $\\mathbb{I}$: indicator function (takes 1 when all words appear in the original text, otherwise takes 0 if there are more words than the original text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b27cf",
   "metadata": {},
   "source": [
    "We implement the corresponding code as follows. This does not include averaging, only one of them is involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b25148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_words_from_content(infer, content):\n",
    "    \"\"\"Check if all words in inference output exist in original context\"\"\"\n",
    "    return 1 if all(w in content for w in infer.split()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ad01a",
   "metadata": {},
   "source": [
    "**Code function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53da8cc",
   "metadata": {},
   "source": [
    "This code implements a function `check_words_from_content`, which is used to check whether all words in the model prediction results appear in the original content. This is the core calculation part in the above definition of original inclusion (OS), which is used to determine the inclusion of a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97589c20",
   "metadata": {},
   "source": [
    "**Code explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb98df",
   "metadata": {},
   "source": [
    "* `infer`: The prediction result of the model for a certain sample, that is, $\\hat{y}$.\n",
    "* `content`: The original content of the sample, that is, $Context$.\n",
    "* `infer.split()`: Split the prediction results into a list of single words.\n",
    "* `all(w in content for w in infer.split())`: Use Python's `all` function and a generator expression to check whether each word or vocabulary in `infer` is present in content. Returns `True` if all words or terms exist; otherwise returns `False`.\n",
    "* `return 1 if ... else 0`: Returns 1 or 0 depending on the check result, as the implementation of the indicator function $\\mathbb{I}$. Returns 1 if all words or terms are present in the original text; otherwise returns 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4bd02",
   "metadata": {},
   "source": [
    "##### **4. Comparative analysis of indicators**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f64aa9",
   "metadata": {},
   "source": [
    "We will conduct a comparative analysis of the above three evaluation indicators:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abaf1d",
   "metadata": {},
   "source": [
    "| **Indicator dimensions** | **Value range (single item)** | **Ideal value (single item)** | **Numerical characteristics (single item** | **Advantages** | **Limitations** | **Task evaluation dimension description** |\n",
    "| -------------------- | ---------------------------------------- | -------------------------------------- | ---------------------------------------- | ------------------ | ------------------ | -------------------------------------------------------------------------- |\n",
    "| Exact match rate | {0, 1} | 1.0 | Binary judgment | The result is clear and unambiguous | Zero tolerance for differences in expression | How much reasoning can be 100% faithful to the answer |\n",
    "| Semantic similarity | [0, 1] | 1.0 | Continuous value | Capture semantic similarity | Depend on encoding model quality | Due to the selection of fragment range or the change of expression, similarity is used to evaluate different expressions of the same content; |\n",
    "| Original text inclusion degree | {0, 1} | 1.0 | Binary judgment | Ensure answers are faithful to the original text | Ignore reasonable synonymous substitutions | Task requirements, answers must be in the original text, this indicator can reflect whether the answers are all derived from the original text; |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4cb3",
   "metadata": {},
   "source": [
    "#### Comprehensive evaluation script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e4c68",
   "metadata": {},
   "source": [
    "Let us bring all the above evaluation indicators together to achieve the following complete evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39480979",
   "metadata": {},
   "source": [
    "[Code GitHub linküîó](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter9/run_cmrc.py#L84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_score(eval_set, infer_set):\n",
    "    \"\"\"Calculate three evaluation metrics: exact match, cosine similarity, and word containment\"\"\"\n",
    "    assert len(eval_set) == len(infer_set)\n",
    "\n",
    "    # Initialize embedding model\n",
    "    m = lazyllm.TrainableModule('bge-large-zh-v1.5')\n",
    "    m.start()\n",
    "\n",
    "    accu_exact_score = 0\n",
    "    accu_cosin_score = 0\n",
    "    accu_origi_score = 0\n",
    "    res = []\n",
    "    for index, eval_item in enumerate(eval_set):\n",
    "        output = infer_set[index].strip()\n",
    "        true_v = eval_item['answers']\n",
    "        # Exact match scoring:\n",
    "        exact_score = 1 if output == true_v else 0\n",
    "        accu_exact_score += exact_score\n",
    "        # Cosine similarity scoring:\n",
    "        outputs = json.loads(m([output, true_v]))\n",
    "        cosine_score = cosine(outputs[0], outputs[1])\n",
    "        accu_cosin_score += cosine_score\n",
    "        # Word containment scoring:\n",
    "        origin_score = check_words_from_content(output, eval_item['context'])\n",
    "        accu_origi_score += origin_score\n",
    "        res.append({'context':eval_item['context'],\n",
    "                    'true': true_v,\n",
    "                    'infer':output,\n",
    "                    'exact_score': exact_score,\n",
    "                    'cosine_score': cosine_score,\n",
    "                    'origin_score': origin_score})\n",
    "    save_res(res, 'eval/infer_true_cp.json')\n",
    "    total_score = len(eval_set)\n",
    "    return (f'Exact Score : {accu_exact_score}/{total_score}, {round(accu_exact_score/total_score,4)*100}%\\n'\n",
    "            f'Cosine Score: {accu_cosin_score}/{total_score}, {round(accu_cosin_score/total_score,4)*100}%\\n'\n",
    "            f'Origin Score: {accu_origi_score}/{total_score}, {round(accu_origi_score/total_score,4)*100}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ce185",
   "metadata": {},
   "source": [
    "In the above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f68d22",
   "metadata": {},
   "source": [
    "* First pass in the test set and the results of inference, and ensure that the two sets are the same size;\n",
    "* Then in order to implement text vectorization here, we use LazyLLM's `TrainableModule` to load a `bge-large-zh-v1.5` model, and use `.start` to deploy it;\n",
    "* Then we go through all the data, calculate each score under the three indicators, and accumulate them;\n",
    "* Finally we save all results, calculate the final evaluation result and return it as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea15fe8",
   "metadata": {},
   "source": [
    "#### Comparison of evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0387df",
   "metadata": {},
   "source": [
    "Here we compare the model Internlm2-Chat-7B before fine-tuning and the model after fine-tuning. We also compare the online model DeepSeek-V3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d58a6f",
   "metadata": {},
   "source": [
    "In the results in the table below, the scores are in brackets, the total score is 1002 points, and the percentage is the score as a percentage of the total score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493e95f",
   "metadata": {},
   "source": [
    "| **Model** | **Exact match rate** | **Semantic similarity** | **Original text inclusion** |\n",
    "| ------------------------- | ----------------------- | ------------------------- | ----------------------- |\n",
    "| Internlm2-Chat-7B      | 2.10%(21)             | 74.51%(746.6)           | 5.19%(52)             |\n",
    "| DeepSeek-V3             | 5.29%(53)             | 74.85%(750.0)           | 15.17%(152)           |\n",
    "| After Internlm2-Chat-7B training | **39.72%**(398) | **86.19%**(863.6) | **94.91%**(951) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79743b36",
   "metadata": {},
   "source": [
    "From the above evaluation results, we can see that the model after fine-tuning has significantly better indicators than before fine-tuning, even for large online models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf75e0",
   "metadata": {},
   "source": [
    "1. **Exact Match Rate Leap**\n",
    "    1. After fine-tuning, the improvement was ‚Äã**37.62 percentage points**‚Äã (2.10% ‚Üí 39.72%), **nearly 8 times higher than the online model**\n",
    "    2. Explain that the model learns to follow a specific answer format\n",
    "\n",
    "2. **Semantic relevance optimization**\n",
    "    1. The similarity increased by 11.68 percentage points (74.51% ‚Üí 86.19%)\n",
    "    2. Compared with online models (large models with more than 600 B): **+11.34 percentage points advantage**\n",
    "   \n",
    "3. **Qualitative change in original text dependence**\n",
    "    1. The inclusion rate jumped from 5.19% to 94.91%, an **increase of 18.3 times**\n",
    "    2. Show that the model has been mastered:\n",
    "        * ‚úÖ Key information positioning capabilities\n",
    "        * ‚úÖ Original text extraction strategy\n",
    "        * ‚úÖ Knowledge boundary control (avoiding hallucinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1cfe2",
   "metadata": {},
   "source": [
    "Based on the experimental data we can conclude:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6051654",
   "metadata": {},
   "source": [
    ">In the RAG system, under the premise that the recall module is accurate and the recall is correct, the degree of adaptation of the model in the generation module to the task greatly affects the final effect. Fine-tuning is an effective means to improve the model's ability to adapt to downstream tasks, and even the ability improved through fine-tuning can surpass the general large model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb883a69",
   "metadata": {},
   "source": [
    "#### General evaluation comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cb95c",
   "metadata": {},
   "source": [
    "Here we evaluate the model based on the evaluation indicators of the two general generation modules introduced in [Retrieve with Higher Accuracy](../chapter6/6.en.ipynb), and compare them with the evaluation indicators we designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd3776",
   "metadata": {},
   "source": [
    "![image.png](9_images/img7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d693236",
   "metadata": {},
   "source": [
    "From the data in the table above, we can see that the general evaluation index shows that after fine-tuning, the model's answer correlation has dropped significantly. Under this evaluation index, it means that fine-tuning has failed, but is this really the case? Let‚Äôs extract the data from the evaluation process and take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70f425",
   "metadata": {},
   "source": [
    "After fine-tuning InternLM2-Chat-7B, **answer correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"context\": \"Based on the games developed on \\\"Kart Runner\\\" and \\\"Bubble Hall\\\", it is developed and published by South Korea's Nexon. Mainland China is operated by Shanda Games. This is the first time Nexon has granted Shanda Network its game operating rights again after 6 years. Taiwan is operated by Game Orange. Players use water guns, small guns, hammers or water bombs to soak enemies (players or NPCs), which is a bubble seal, and the bubble is broken into a kick. If the bubble is not there at the time If kicked out within a certain period of time, the number of lives will be reduced, and the number of lives will be exhausted. The reborn person will be invincible for a certain period of time, and the player with the most points will win. The rules vary depending on the mode. In 2V2 and 4V4 random matching, players can climb up the ranking list (in order: rough stone, bronze medal, silver medal, gold medal, platinum, diamond, and master). , you can choose classic, hot-blooded, sniper and other modes to play. If you are in the game, you will not be able to match within 4 minutes (each time you are in the game + 4 minutes). The opening time is from time to time during the summer or winter vacation. The 8-player classic mode is randomly matched and the points are scored. The more points you get during the event, the rewards will be obtained at the end. \",\n",
    "    \"exact_score\": 1,\n",
    "    \"cosine_score\": 0.9999,\n",
    "    \"origin_score\": 1,\n",
    "\"question\": \"What does it count as if the lives are exhausted?\",\n",
    "\"true_answer\": \"Kick blast\",\n",
    "\"answer\": \"Kick blast\",\n",
    "    \"infer_questions\": [\n",
    "        {\n",
    "\"question\": \"\\nWhat does the word kick mean?\",\n",
    "            \"score\": 0.3781\n",
    "        },\n",
    "        {\n",
    "\"question\": \"\\nWhat does kicking mean? In what context is this word used?\",\n",
    "            \"score\": 0.3825\n",
    "        },\n",
    "        {\n",
    "\"question\": \"\\nWhat does kicking mean? In what context is this word usually used?\",\n",
    "            \"score\": 0.3829\n",
    "        }\n",
    "    ],\n",
    "    \"final_score\": 0.3812\n",
    "},\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed0de5",
   "metadata": {},
   "source": [
    "DeepSeek-V3, **Answer Relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"context\": \"Based on the games developed on \\\"Kart Runner\\\" and \\\"Bubble Hall\\\", it is developed and published by South Korea's Nexon. Mainland China is operated by Shanda Games. This is the first time Nexon has granted Shanda Network its game operating rights again after 6 years. Taiwan is operated by Game Orange. Players use water guns, small guns, hammers or water bombs to soak enemies (players or NPCs), which is a bubble seal, and the bubble is broken into a kick. If the bubble is not there at the time If kicked out within a certain period of time, the number of lives will be reduced, and the number of lives will be exhausted. The reborn person will be invincible for a certain period of time, and the player with the most points will win. The rules vary depending on the mode. In 2V2 and 4V4 random matching, players can climb up the ranking list (in order: rough stone, bronze medal, silver medal, gold medal, platinum, diamond, and master). , you can choose classic, hot-blooded, sniper and other modes to play. If you are in the game, you will not be able to match within 4 minutes (each time you are in the game + 4 minutes). The opening time is from time to time during the summer or winter vacation. The 8-player classic mode is randomly matched and the points are scored. The more points you get during the event, the rewards will be obtained at the end. \",\n",
    "    \"exact_score\": 0,\n",
    "    \"cosine_score\": 0.6646,\n",
    "    \"origin_score\": 1,\n",
    "\"question\": \"What does it count as if the lives are exhausted?\",\n",
    "\"true_answer\": \"Kick blast\",\n",
    "\"answer\": \"When the number of lives is exhausted, it will be counted as a kick.\",\n",
    "    \"infer_questions\": [\n",
    "        {\n",
    "            \"question\": \"\\nIn game terminology, what does \\\"when the number of lives is exhausted is counted as a kick\\\" mean?\",\n",
    "            \"score\": 0.7191\n",
    "        },\n",
    "        {\n",
    "\"question\": \"\\nUnder what circumstances will it be considered a kick? What will happen when the life count is exhausted?\",\n",
    "            \"score\": 0.6896\n",
    "        },\n",
    "        {\n",
    "\"question\": \"\\nIn the game rules, what happens after the number of lives is exhausted?\",\n",
    "            \"score\": 0.7918\n",
    "        }\n",
    "    ],\n",
    "    \"final_score\": 0.7335\n",
    "},\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355af5c1",
   "metadata": {},
   "source": [
    "In the above comparison, `true_answer` is the annotated answer, `answer` is the answer of model inference, `question` is the question, and **answer correlation** needs to generate possible questions (`question` in `infer_questions`) based on the answer `answer` of model inference. Here are three, and then let the possible questions and the real questions be vectorized to find the cosine similarity. Under the short `answer`, it is difficult for the model to infer questions related to the real question `question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efd9ed",
   "metadata": {},
   "source": [
    "* The `answer` of InternLM2-Chat-7B after fine-tuning is: \"Kicked\"\n",
    "* The `answer` of DeepSeek-V3 is: \"When the number of lives is exhausted, it will be counted as a kick.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f23003",
   "metadata": {},
   "source": [
    "It can be seen that because DeepSeek-V3 provides more information, the questions inferred by the evaluation model are more accurate, so the score is higher. But actually this is not what we expected, we wanted it to be short and precise, just like the standard answer \"kick it to the punch\"! Correspondingly, the semantic similarity `cosine_score` and `exact_score` we use here reflect the expectations very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30954a9",
   "metadata": {},
   "source": [
    "Faithfulness makes little difference here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2207734",
   "metadata": {},
   "source": [
    "After fine-tuning InternLM2-Chat-7B, faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"context\": \"China (Nanjing) Chess Super Competition (Pearl Spring Super Tournament), formerly known as the China (Nanjing) Chess Grandmaster Invitational Tournament, the first competition was held at the Mingfa Pearl Spring Hotel in Pukou District, Nanjing from December 11 to 22, 2008. This competition was hosted by the Nanjing Municipal People's Government and the Chess and Card Sports Management Center of the State Sports General Administration, and was hosted by the Pukou District People's Government, Organized by the Nanjing Municipal Sports Bureau and co-organized by Kangyuan Pharmaceutical Co., Ltd., Yangtze Evening News, and Mondale International Entrepreneurship University, it is designated as a level 21 event by FIDE and is the highest level chess competition held in Asia so far. The competition will consist of ten rounds, with the first five rounds from December 11th to 15th, and a break on the 16th and 17th. The last 5 rounds were played on the 21st. Each side had 90 minutes and 30 seconds added to each move. The total prize money was 250,000 euros, of which the winner was 80,000 euros, and the second to sixth place were 55,000 euros, 30,000 euros, 25,000 euros, and 20,000 euros. As a result, Topalov won the championship, Aronyan won the runner-up, and Bu Xiangzhi won the third place. Name. It was admitted as a Grand Slam event on February 1, 2009 and was renamed the China (Nanjing) Chess Super Competition. The second competition was held from September 27 to October 9, 2009. The \\\"Kangyuan Pharmaceutical Cup\\\" 2010 China (Nanjing) Chess Super Competition was held from October 19 to 30, 2010.\",\n",
    "    \"exact_score\": 1,\n",
    "    \"cosine_score\": 0.9999,\n",
    "    \"origin_score\": 1,\n",
    "    \"question\": \"Where will the first competition be held?\",\n",
    "    \"true_answer\": \"Mingfa Pearl Spring Hotel, Pukou District, Nanjing\",\n",
    "    \"answer\": \"Mingfa Pearl Spring Hotel, Pukou District, Nanjing\",\n",
    "    \"statements\": \"\\nThe first competition was held in Pukou District, Nanjing City.|||The specific location of the competition is Mingfa Pearl Spring Hotel.\",\n",
    "    \"scores\": [\n",
    "        {\n",
    "            \"statement\": \"The first competition was held in Pukou District, Nanjing.\",\n",
    "            \"score\": 1\n",
    "        },\n",
    "        {\n",
    "            \"statement\": \"The specific venue of the competition is Mingfa Pearl Spring Hotel.\",\n",
    "            \"score\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"final_score\": 1.0\n",
    "},\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20faac0",
   "metadata": {},
   "source": [
    "DeepSeek-V3, faithfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"context\": \"China (Nanjing) Chess Super Competition (Pearl Spring Super Tournament), formerly known as the China (Nanjing) Chess Grandmaster Invitational Tournament, the first competition was held at the Mingfa Pearl Spring Hotel in Pukou District, Nanjing from December 11 to 22, 2008. This competition was hosted by the Nanjing Municipal People's Government and the Chess and Card Sports Management Center of the State Sports General Administration, and was hosted by the Pukou District People's Government, Organized by the Nanjing Municipal Sports Bureau and co-organized by Kangyuan Pharmaceutical Co., Ltd., Yangtze Evening News, and Mondale International Entrepreneurship University, it is designated as a level 21 event by FIDE and is the highest level chess competition held in Asia so far. The competition will consist of ten rounds, with the first five rounds from December 11th to 15th, and a break on the 16th and 17th. The last 5 rounds were played on the 21st. Each side had 90 minutes and 30 seconds added to each move. The total prize money was 250,000 euros, of which the winner was 80,000 euros, and the second to sixth place were 55,000 euros, 30,000 euros, 25,000 euros, and 20,000 euros. As a result, Topalov won the championship, Aronyan won the runner-up, and Bu Xiangzhi won the third place. Name. It was admitted as a Grand Slam event on February 1, 2009 and was renamed the China (Nanjing) Chess Super Competition. The second competition was held from September 27 to October 9, 2009. The \\\"Kangyuan Pharmaceutical Cup\\\" 2010 China (Nanjing) Chess Super Competition was held from October 19 to 30, 2010.\",\n",
    "    \"exact_score\": 0,\n",
    "    \"cosine_score\": 0.5574,\n",
    "    \"origin_score\": 1,\n",
    "    \"question\": \"Where will the first competition be held?\",\n",
    "    \"true_answer\": \"Mingfa Pearl Spring Hotel, Pukou District, Nanjing\",\n",
    "    \"answer\": \"The first competition was held at the Mingfa Pearl Spring Hotel in Pukou District, Nanjing from December 11 to 22, 2008.\",\n",
    "    \"statements\": \"\\nThe first competition was held from December 11 to 22, 2008.|||The competition location is Pukou District, Nanjing City.|||The specific venue is Mingfa Pearl Spring Hotel.\",\n",
    "    \"scores\": [\n",
    "        {\n",
    "            \"statement\": \"The first competition was held from December 11 to 22, 2008.\",\n",
    "            \"score\": 1\n",
    "        },\n",
    "        {\n",
    "            \"statement\": \"The competition location is Pukou District, Nanjing City.\",\n",
    "            \"score\": 1\n",
    "        },\n",
    "        {\n",
    "            \"statement\": \"The specific venue is Mingfa Pearl Spring Hotel.\",\n",
    "            \"score\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"final_score\": 1.0\n",
    "},\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6847558",
   "metadata": {},
   "source": [
    "In the above comparison, we can see that although DeepSeek-V3 provides more information, it is the same as the fine-tuned model. It basically uses the content in the original text to answer the questions as required. Therefore, although it has more statements (3), the final score is the same as the fine-tuned model. This explains why the final faithfulness scores are indistinguishable. It is worth noting that `cosine_score` semantic similarity and `exact_score` exact matching are used here to distinguish the difference between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc33e2",
   "metadata": {},
   "source": [
    "From the comparison results of the two indicators above, we can see that if there are specific task requirements for the generation module in the RAG system, the commonly used evaluation indicators cannot be measured well. At this time, the evaluation indicators need to be designed according to the final effect requirements of the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8cec2",
   "metadata": {},
   "source": [
    "### Use fine-tuned large models in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6e29c",
   "metadata": {},
   "source": [
    "LazyLLM supports one-stop fine-tuning, deployment, and inference, but what should you do if you have fine-tuned a large model and want to use it directly? It's very simple: the base\\_model remains unchanged, and you can use target\\_path to specify the fine-tuned model path, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49748860",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'internlm2-chat-7b'\n",
    "sft_model = '/path/to/sft/internlm2-chat-7b'\n",
    "\n",
    "llm = lazyllm.TrainableModule(base_model, sft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ceba17",
   "metadata": {},
   "source": [
    "Taking the basic RAG as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lazyllm\n",
    "\n",
    "prompt = ('You will act as an AI question-answering assistant and complete a dialogue task.'\n",
    "          'In this task, you need to provide your answers based on the given context and questions.')\n",
    "\n",
    "base_model = 'internlm2-chat-7b'\n",
    "sft_model = '/path/to/sft/internlm2-chat-7b'\n",
    "llm = lazyllm.TrainableModule(base_model, sft_model)\n",
    "\n",
    "documents = lazyllm.Document(dataset_path=os.path.join(os.getcwd(), \"KB\"), embed=embed = lazyllm.TrainableModule('bge-large-zh-v1.5'), manager=False)\n",
    "documents.create_node_group(name='split_sent', transform=lambda s: s.split('\\n'))\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    ppl.retriever = lazyllm.Retriever(\n",
    "        doc=documents, group_name=\"split_sent\", similarity=\"cosine\", topk=1, output_format='content', join='')\n",
    "    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | lazyllm.bind(query=ppl.input)\n",
    "    ppl.llm = llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "ppl.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42ef97",
   "metadata": {},
   "source": [
    "### Experimental process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40b82",
   "metadata": {},
   "source": [
    "The experimental processes of the three models are shown in the following videos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc43af",
   "metadata": {},
   "source": [
    "1. Deployment, reasoning and evaluation of Internlm2-Chat-7B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adec027",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/1origin.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95b142",
   "metadata": {},
   "source": [
    "2. Fine-tuning, deployment, reasoning and evaluation of Internlm2-Chat-7B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45804220",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/2sft.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5bfcd1",
   "metadata": {},
   "source": [
    "3. Reasoning and evaluation of DeepSeek-V3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246c105",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/3deepseek.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b2baa0",
   "metadata": {},
   "source": [
    "## Fine-tune the Embedding model based on LazyLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eeaeef",
   "metadata": {},
   "source": [
    "In the RAG (Retrieval-Augmented Generation) system, the Embedding model plays a key role:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620f60f",
   "metadata": {},
   "source": [
    "![image.png](9_images/img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7bba8",
   "metadata": {},
   "source": [
    "* ‚Äã**Semantic encoding**‚Äã: Convert text data into high-dimensional vector representation, retaining semantic information\n",
    "* **‚ÄãSimilarity calculation:‚Äã** Efficient correlation retrieval through vector cosine similarity\n",
    "* ‚Äã**Knowledge Base Index**‚Äã: Pre-coded document library to create a vector index for fast retrieval\n",
    "* ‚Äã**Query Understanding**‚Äã: Convert user query into vector and match the most relevant knowledge fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1561a0",
   "metadata": {},
   "source": [
    "In this tutorial, we will use BAAI's bge-large-zh-v1.5 as the basic model to improve vertical field effects through financial field data fine-tuning (SFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a02845",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0fae3",
   "metadata": {},
   "source": [
    "In embedding learning, our goal is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76374d02",
   "metadata": {},
   "source": [
    "* Semantically similar samples (positive pairs) are closer in vector space.\n",
    "* Semantically irrelevant or opposite samples (negative pairs) are farther away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f19b64",
   "metadata": {},
   "source": [
    "The role of negative samples is to provide a comparison reference to let the model know \"which ones should not be close.\" For example, when our query is \"What is ChatGPT?\", the doc of our positive sample is \"ChatGPT is a language model developed by OpenAI, based on the Transformer architecture...\", and the doc of our negative sample can be set to \"Midjourney is an AI image generation model...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709e1d8",
   "metadata": {},
   "source": [
    "Here we use the financial question and answer data set: [virattt/financial-qa-10K](https://huggingface.co/datasets/virattt/financial-qa-10K) for demonstration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d21866",
   "metadata": {},
   "source": [
    "![image.png](9_images/img9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fb7c3",
   "metadata": {},
   "source": [
    "Data processing flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0e38d",
   "metadata": {},
   "source": [
    "1. Load the original data set\n",
    "2. Generate negative samples (10 negative examples for each sample)\n",
    "3. Create training set/evaluation set split (9:1 ratio)\n",
    "4. Build knowledge base files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc486606",
   "metadata": {},
   "source": [
    "The main code implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_corpus(instruction: str, neg_num: int = 10, test_size: float = 0.1, seed: int = 1314) -> tuple:\n",
    "    \"\"\"Process dataset and create training/evaluation files.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): Instruction template for prompts\n",
    "        neg_num (int): Number of negative samples per instance\n",
    "        test_size (float): Proportion of data for test split\n",
    "        seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        tuple: Paths to training data, evaluation data, and knowledge base directory\n",
    "    \"\"\"\n",
    "    # Load and preprocess dataset\n",
    "    ds = load_dataset(\"virattt/financial-qa-10K\", split=\"train\")\n",
    "    ds = ds.select_columns(column_names=[\"question\", \"context\"])\n",
    "    ds = ds.rename_columns({\"question\": \"query\", \"context\": \"pos\"})\n",
    "\n",
    "    # Generate negative samples\n",
    "    np.random.seed(seed)\n",
    "    new_col = []\n",
    "    for i in range(len(ds)):\n",
    "        ids = np.random.randint(0, len(ds), size=neg_num)\n",
    "        while i in ids:  # Ensure no self-match in negatives\n",
    "            ids = np.random.randint(0, len(ds), size=neg_num)\n",
    "        neg = [ds[int(i)][\"pos\"] for i in ids]\n",
    "        new_col.append(neg)\n",
    "\n",
    "    # Create dataset splits\n",
    "    ds = ds.add_column(\"neg\", new_col)\n",
    "\n",
    "    def str_to_lst(data):\n",
    "        data[\"pos\"] = [data[\"pos\"]]\n",
    "        return data\n",
    "    ds = ds.map(str_to_lst)  # Convert pos to list format\n",
    "    ds = ds.add_column(\"prompt\", [instruction] * len(ds))\n",
    "    split = ds.train_test_split(test_size=test_size, shuffle=True, seed=seed)\n",
    "\n",
    "    # Save training data\n",
    "    train_data_path = build_data_path('dataset', 'train.json')\n",
    "    split[\"train\"].to_json(train_data_path)\n",
    "\n",
    "    # Process and save evaluation data\n",
    "    test = split[\"test\"].select_columns([\"query\", \"pos\"]).rename_column(\"pos\", \"corpus\")\n",
    "    eval_data_path = build_data_path('dataset', 'eval.json')\n",
    "    test.to_json(eval_data_path)\n",
    "\n",
    "    # Create knowledge base\n",
    "    kb_data_path = build_data_path('KB', 'knowledge_base.txt')\n",
    "    corpus = \"\\n\".join([''.join(item) for item in test['corpus']])\n",
    "    with open(kb_data_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(corpus)\n",
    "\n",
    "    return train_data_path, eval_data_path, os.path.dirname(kb_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b462f",
   "metadata": {},
   "source": [
    "After processing, a piece of data from the training set is as follows (json file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46441c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"query\":\"What was the total stockholder's equity (deficit) for Peloton Interactive, Inc. as of June 30, 2021?\",\"pos\":[\"As of June 30, 2021, Peloton Interactive, Inc.'s consolidated statements reflected a total stockholder's equity (deficit) of $1,754.1 million.\"],\"neg\":[\"In June 2023, the company entered into an ASR agreement to repurchase $500 million of its common stock with a completion date no later than August 2023, and in 2024, the company expects to repurchase $2.0 billion of its common stock.\",...,\"\\u2022Overhead costs as a percentage of net sales increased 40 basis points due to wage inflation and other cost increases, partially offset by the positive scale impacts of the net sales increase and productivity savings.\"],\"prompt\":\"Represent this sentence for searching relevant passages: \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb53f7",
   "metadata": {},
   "source": [
    "The following fields need to be included:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a89b7d",
   "metadata": {},
   "source": [
    "* `query`: (str) User question\n",
    "* `pos`: (List[str]) Correct answer paragraph\n",
    "* `neg`: (List[str]) Randomly sampled negative samples\n",
    "* `prompt`: (str) command template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd1b56",
   "metadata": {},
   "source": [
    "A piece of data from the evaluation set is as follows (json file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"query\":\"How have certain vendors been impacted in the supply chain financing market?\",\"corpus\":[\"Certain vendors have been impacted by volatility in the supply chain financing market.\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103efa1b",
   "metadata": {},
   "source": [
    "The following fields need to be included:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e4822",
   "metadata": {},
   "source": [
    "* `query`: user questions\n",
    "* `corpus`: the correct text fragment corresponding to the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204d30a",
   "metadata": {},
   "source": [
    "The knowledge base part is as follows (txt file):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f40524",
   "metadata": {},
   "source": [
    "```bash\n",
    "Certain vendors have been impacted by volatility in the supply chain financing market.\n",
    "Recruitment As the demand for global technical talent continues to be competitive, we have grown our technical workforce and have been successful in attracting top talent to NVIDIA. We have attracted strong talent globally with our differentiated hiring strategies for university, professional, executive and diverse recruits. The COVID-19 pandemic created expanded hiring opportunities in new geographies and provided increased  flexibility for employees to work from locations of their choice. Our workforce is about 80% technical and about 50% hold advanced degrees.\n",
    "In 2023, Moody‚Äôs Investors Service upgraded AbbVie‚Äôs senior unsecured long-term credit rating to A3 with a stable outlook from Baa1 with a positive outlook.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f0a75",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/sft_embed01.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b6bc2",
   "metadata": {},
   "source": [
    "### Fine-tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f82ee",
   "metadata": {},
   "source": [
    "Distributed fine-tuning through the LazyLLM framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = lazyllm.TrainableModule(embed_path)\\\n",
    "    .mode('finetune').trainset(train_data_path)\\\n",
    "    .finetune_method((\n",
    "        lazyllm.finetune.flagembedding,\n",
    "        {\n",
    "            'launcher': lazyllm.launchers.remote(nnode=1, nproc=1, ngpus=4),\n",
    "            'per_device_train_batch_size': 16,\n",
    "            'num_train_epochs': 2,\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "docs = Document(kb_path, embed=embed, manager=False)\n",
    "docs.create_node_group(name='split_sent', transform=lambda s: s.split('\\n'))\n",
    "retriever = lazyllm.Retriever(doc=docs, group_name=\"split_sent\", similarity=\"cosine\", topk=1)\n",
    "retriever.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f13aec",
   "metadata": {},
   "source": [
    "This code is consistent with the previous configuration using LazyLLM's `TrainableModule` to fine-tune LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c23b7",
   "metadata": {},
   "source": [
    "* `embed_path`: used to specify the fine-tuned model;\n",
    "* `train_data_path`: Data set path used for training;\n",
    "* `lazyllm.finetune.flagembedding`: specifies the fine-tuning framework;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f415a",
   "metadata": {},
   "source": [
    "Key parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb680ad1",
   "metadata": {},
   "source": [
    "* `ngpus=4`: Use 4 GPUs for parallel training\n",
    "* `per_device_batch_size=16`: batch size per GPU\n",
    "* `num_train_epochs=2`: train for 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f00b4",
   "metadata": {},
   "source": [
    "It is worth noting that in the code here, we not only gave embed the fine-tuning configuration parameters, but also put it into the Document later. The Document registered a strategy to split the knowledge base document according to newlines. Finally, we also configured Retriever to act on the document and its corresponding segmentation method, and used cosine similarity as a measurement tool, while allowing only the most relevant text segment (topk=1) to be returned. Because LazyLLM supports one-click fine-tuning, deployment and inference, after executing `update()`, LazyLLM will first fine-tune the embed model, and then deploy the fine-tuned model to provide vectorization for Document and Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8abcd",
   "metadata": {},
   "source": [
    "### Effect evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9bb5c",
   "metadata": {},
   "source": [
    "Here we use the contextual recall rate and Context Relevance introduced in the previous tutorial to evaluate our fine-tuned model. As a comparison, here we use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659149d",
   "metadata": {},
   "source": [
    "bge-large-zh-v1.5 is used as the base model, and the changes in the two indicators before and after fine-tuning are compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ed8c6",
   "metadata": {},
   "source": [
    "The call of the evaluation indicators is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm.tools.eval import NonLLMContextRecall, ContextRelevance\n",
    "\n",
    "def evaluate_results(data: list) -> tuple:\n",
    "    \"\"\"Evaluate retrieval results using multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of retrieval results to evaluate\n",
    "\n",
    "    Returns:\n",
    "        tuple: Evaluation scores (context recall, context relevance)\n",
    "    \"\"\"\n",
    "    recall_eval = NonLLMContextRecall(binary=False)\n",
    "    relevance_eval = ContextRelevance()\n",
    "    return recall_eval(data), relevance_eval(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb1d47",
   "metadata": {},
   "source": [
    "The logic of fine-tuning, deployment and inference is mainly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "train_data_path, eval_data_path, kb_path = build_dataset_corpus(\n",
    "    instruction=args.instruction,\n",
    "    neg_num=args.neg_num,\n",
    "    test_size=args.test_size,\n",
    "    seed=args.seed\n",
    ")\n",
    "# Deploy retrieval service\n",
    "retriever = deploy_serve(\n",
    "    kb_path=kb_path,\n",
    "    embed_path=args.embed_path,\n",
    "    train_data_path=train_data_path,\n",
    "    train_flag=args.train_flag,\n",
    "    per_device_batch_size=args.per_device_batch_size,\n",
    "    num_epochs=args.num_epochs,\n",
    "    ngpus=args.ngpus\n",
    ")\n",
    "# Run SFT or Evaluation\n",
    "results = []\n",
    "query_corpus = load_json(eval_data_path)\n",
    "for item in tqdm(query_corpus, desc=\"Processing queries\"):\n",
    "    query = item['query']\n",
    "    inputs = f\"{args.instruction}{query}\" if args.use_instruction or args.train_flag else query\n",
    "    retrieved = retriever(inputs)\n",
    "    results.append({\n",
    "        'question': query,\n",
    "        'context_retrieved': [text.get_text() for text in retrieved],\n",
    "        'context_reference': item['corpus']\n",
    "    })\n",
    "\n",
    "# Save and report results\n",
    "save_json(results, args.output_path)\n",
    "recall_score, relevance_score = evaluate_results(results)\n",
    "print(f\"Evaluation Complete!\\nContext Recall: {recall_score}\\nContext Relevance: {relevance_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a9f81",
   "metadata": {},
   "source": [
    "Based on the above logic, we obtain the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24604ed",
   "metadata": {},
   "source": [
    "| | **Before fine-tuning**‚Äã**bge-large-zh-v1.5** | **After fine-tuning**‚Äã**bge-large-zh-v1.5** |\n",
    "| -------------- | ----------------------------------------------- | ----------------------------------------------- |\n",
    "| Context Recall | 78.28 | 88.57 |\n",
    "| Context Relevance | 75.71 | 86.57 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371a8ae",
   "metadata": {},
   "source": [
    "It can be seen that after fine-tuning, both indicators have been significantly improved. It shows that fine-tuning is effective! The overall evaluation process is: load the evaluation set ‚Üí use the retrieval service (service deployed after fine-tuning) ‚Üí perform batch inference ‚Üí calculate dual indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88364f05",
   "metadata": {},
   "source": [
    "**„Äêbge-large-zh-v1.5 before fine-tuning„Äë**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429527c",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/sft_embed02.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6eb3a",
   "metadata": {},
   "source": [
    "**„ÄêAfter fine-tuning bge-large-zh-v1.5„Äë**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943bb31",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./9_videos/sft_embed03.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af31ba0",
   "metadata": {},
   "source": [
    "### Use the fine-tuned Embedding model in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db9485",
   "metadata": {},
   "source": [
    "Similar to using a fine-tuned LLM, here we can also use a fine-tuned Embedding model, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lazyllm\n",
    "\n",
    "prompt = ('You will act as an AI question-answering assistant and complete a dialogue task.'\n",
    "          'In this task, you need to provide your answers based on the given context and questions.')\n",
    "\n",
    "embed = lazyllm.TrainableModule('bge-large-zh-v1.5', 'path/to/sft/bge')\n",
    "\n",
    "documents = lazyllm.Document(dataset_path=os.path.join(os.getcwd(), \"KB\"), embed=embed, manager=False)\n",
    "documents.create_node_group(name='split_sent', transform=lambda s: s.split('\\n'))\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    ppl.retriever = lazyllm.Retriever(\n",
    "        doc=documents, group_name=\"split_sent\", similarity=\"cosine\", topk=1, output_format='content', join='')\n",
    "    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | lazyllm.bind(query=ppl.input)\n",
    "    ppl.llm = lazyllm.OnlineChatModule(source=\"sensenova\")\\\n",
    "        .prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "ppl.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47992316",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d3339",
   "metadata": {},
   "source": [
    "### More fine-tuning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c805",
   "metadata": {},
   "source": [
    "From the perspective of the updated parameter range in the model, in addition to the LoRA fine-tuning introduced above, common fine-tuning includes full-parameter fine-tuning, frozen fine-tuning, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a908cfc",
   "metadata": {},
   "source": [
    "#### 1. Fine-tuning of all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a080eb",
   "metadata": {},
   "source": [
    "![image.png](9_images/img10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa81e2c",
   "metadata": {},
   "source": [
    "Full Parameter Fine-tuning is the most direct fine-tuning method. Its main idea is to fine-tune all parameters of the entire model for specific tasks on the basis of a pre-trained model (also a fine-tuned model). As shown in the figure above, the orange parameters of the four layers of the model: W1, W2, W3, and W4 are all involved in fine-tuning. The specific steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cc194",
   "metadata": {},
   "source": [
    "1. Load the pre-trained model: Use the pre-trained large language model as the basic model.\n",
    "2. Prepare task data: Collect and organize relevant data based on specific tasks.\n",
    "3. Fine-tune the model: Input the task data into the model and update all parameters of the model through the back propagation algorithm.\n",
    "4. Evaluation and optimization: Evaluate the performance of the fine-tuned model on the validation set and optimize according to needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c008e",
   "metadata": {},
   "source": [
    "The advantage of full-parameter fine-tuning is that it can fully tap the potential of the model on specific tasks, but the disadvantage is that it consumes a lot of computing resources and is prone to over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d718fde",
   "metadata": {},
   "source": [
    "#### 2. Freeze fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a29ff",
   "metadata": {},
   "source": [
    "![image.png](9_images/img11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf47314",
   "metadata": {},
   "source": [
    "Freeze Parameter Fine-tuning is a fine-tuning method that saves computing resources. During the frozen fine-tuning process, the underlying parameters of the pre-trained model (also fine-tuned models) remain unchanged, and only some layer parameters are fine-tuned** (frozen fine-tuning can freeze any layer, and it is common to fine-tune the top layer of the model). As shown in the picture above, freeze the first three layers of the model, blue W0, W1, and W2, and only fine-tune the last (top) layer, orange W3. The specific steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b8151",
   "metadata": {},
   "source": [
    "1. Load the pre-trained model: Use the pre-trained large language model.\n",
    "2. Freeze the underlying parameters: The underlying parameters of the model are fixed and will not participate in training.\n",
    "3. Fine-tune top-level parameters: Enter task data into the model and only update top-level parameters.\n",
    "4. Evaluation and optimization: Evaluate the model performance on the validation set and optimize according to needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf061ff4",
   "metadata": {},
   "source": [
    "The advantage of frozen fine-tuning is that it consumes less computing resources, but compared to full-parameter fine-tuning, model performance may be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf270b",
   "metadata": {},
   "source": [
    "We summarize these fine-tuning techniques as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e577a1",
   "metadata": {},
   "source": [
    "| **Fine-tuning method** | **Computing resource consumption** | **Parameter update range** | **Advantages** | **Disadvantages** |\n",
    "| --------------------------------------------------- | ------------------------ | ------------------------ | ---------------------------- | ------------------------ |\n",
    "| **Full Parameter Fine-tuning** | High | All parameters | Fully tap the potential of the model, strong adaptability | High resource consumption, easy to overfit |\n",
    "| **Freeze Fine-tuning (Freeze)** | Low | Only some layer parameters | Save computing resources, fast training | Performance may not be as good as full parameter fine-tuning |\n",
    "| **LoRA fine-tuning** | Medium | Low-rank matrix | Save resources and maintain the advantages of pre-training | There are certain restrictions on the model structure |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaa7a1",
   "metadata": {},
   "source": [
    "### Fine-tune data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e532ee",
   "metadata": {},
   "source": [
    "As mentioned above, we have processed the data set fields into: Instruction, input and output formats, which are the data formats of Alpaca instruction fine-tuning. Here we not only introduce the instruction fine-tuning format in detail, but also introduce another commonly used format: the OpenAI instruction fine-tuning data format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4739746",
   "metadata": {},
   "source": [
    "##### 1. Alpaca command fine-tuning data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef1668",
   "metadata": {},
   "source": [
    "Alpaca format is a data format used for command fine-tuning. It contains information such as commands, input, output, system prompt words, and historical conversations. The format is suitable for both single-turn and multi-turn conversation scenarios, allowing the model to generate more accurate answers based on historical information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697211f",
   "metadata": {},
   "source": [
    "**Basic data format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb12df3",
   "metadata": {},
   "source": [
    "The basic structure of this data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a734cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "\"instruction\": \"Human instructions (required)\",\n",
    "\"input\": \"Human input (optional)\",\n",
    "\"output\": \"Model's answer (required)\",\n",
    "\"system\": \"System prompt word (optional)\",\n",
    "    \"history\": [\n",
    "[\"Instructions for the first round (optional)\", \"Answers for the first round (optional)\"],\n",
    "[\"Second round of instructions (optional)\", \"Second round of answers (optional)\"]\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39d59a",
   "metadata": {},
   "source": [
    "**1. ‚Äúinstruction‚Äù (required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084004f",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field contains the instructions or questions given by the user, which is the core content that the model needs to understand and respond to.\n",
    "* ‚Äã**Example**‚Äã: For a single-turn conversation, it might be \"Tell me what the weather is like today.\"; for a multi-turn conversation, it might be \"I want to learn about artificial intelligence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9e68",
   "metadata": {},
   "source": [
    "**2. ‚Äúinput‚Äù (optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad0e30",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field is used to provide additional input information, which may be background information, context or specific data related to the instruction.\n",
    "* **Example**‚Äã: In a single-round dialogue, if the **instruction** is \"Translate the following sentence\", the **input** may be \"Hello, how are you?\"; in a multi-round dialogue, supplementary information from the previous round of dialogue may not be filled in or provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511b99c",
   "metadata": {},
   "source": [
    "**3. ‚Äúoutput‚Äù (required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aff171",
   "metadata": {},
   "source": [
    "* **Description**: This field contains the answers or outputs generated by the model in response to the instructions and inputs.\n",
    "* **Example**‚Äã: For a single-turn conversation, it might be \"The weather is sunny today and the temperature is 20 degrees Celsius.\"; for a multi-turn conversation, it might be \"Artificial intelligence is a technology that simulates intelligent human behavior.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285caae",
   "metadata": {},
   "source": [
    "**4. ‚Äúsystem‚Äù (optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f4302",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field is used to provide system-level prompt words or instructions that can guide the model's behavior or answering style.\n",
    "* ‚Äã**Example**‚Äã: This might be ‚ÄúPlease answer in a formal tone.‚Äù or ‚ÄúTry to use simple vocabulary when answering.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce0250",
   "metadata": {},
   "source": [
    "**5. ‚Äúhistory‚Äù (optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056046f",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field is an array used to store historical information of multiple rounds of dialogue. Each element is an array containing two strings, representing the user instructions and model responses in one round of dialogue.\n",
    "* ‚Äã**Example**‚Äã: In a multi-turn conversation, the following historical information may be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [\"What is artificial intelligence?\", \"Artificial intelligence is technology that simulates human intelligent behavior.\"],\n",
    "    [\"What are the applications of artificial intelligence?\", \"Artificial intelligence is used in fields such as autonomous driving, speech recognition, and image processing.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecf914",
   "metadata": {},
   "source": [
    "**Single-turn dialogue example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bd773",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"instruction\": \"Please tell me what the weather is like today.\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"It's sunny today and the temperature is 20 degrees Celsius.\",\n",
    "    \"system\": \"\",\n",
    "    \"history\": []\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578efd00",
   "metadata": {},
   "source": [
    "**Multi-turn dialogue example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa932d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"instruction\": \"I want to know information about artificial intelligence.\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Artificial intelligence is technology that simulates human intelligent behavior.\",\n",
    "    \"system\": \"\",\n",
    "    \"history\": [\n",
    "      [\"What is artificial intelligence?\", \"Artificial intelligence is technology that simulates human intelligent behavior.\"],\n",
    "      [\"What are the applications of artificial intelligence?\", \"Artificial intelligence is used in fields such as autonomous driving, speech recognition, and image processing.\"]\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab66ee",
   "metadata": {},
   "source": [
    "#### **2. OpenAI command fine-tuning data format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed589bcf",
   "metadata": {},
   "source": [
    "The OpenAI format is a data format for command fine-tuning that organizes conversations by role and content. The role can be system, user or assistant, and the content is the speech of the corresponding role. The format works equally well for single-turn and multi-turn dialogue scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01544d",
   "metadata": {},
   "source": [
    "**Basic data format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ff99b",
   "metadata": {},
   "source": [
    "The basic structure of this data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3212772",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"System prompt word (optional)\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Human instructions\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Model answer\"\n",
    "      }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f67a4c",
   "metadata": {},
   "source": [
    "**1. ‚Äúmessages‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7e48d",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field is an array that contains all the turns of the entire conversation. Each turn consists of \"role\" and \"content\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d46641",
   "metadata": {},
   "source": [
    "**2. ‚Äúrole‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94582c6f",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field indicates the role of the message, which can be \"system\", \"user\" or \"assistant\".\n",
    "* ‚Äã\"system\"‚Äã: System prompt word, used to provide background information or guide the behavior of the model.\n",
    "* ‚Äã‚Äúuser‚Äù‚Äã: User role, indicating the user‚Äôs instructions or questions.\n",
    "* ‚Äã‚Äúassistant‚Äù‚Äã: Assistant role, representing the model‚Äôs answer or output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64a971",
   "metadata": {},
   "source": [
    "**3. ‚Äúcontent‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008412a6",
   "metadata": {},
   "source": [
    "* ‚Äã**Description**‚Äã: This field contains the specific content of the corresponding role.\n",
    "* **Example**‚Äã: For the \"system\" role, it might be \"Please answer in a friendly tone.\"; For the \"user\" role, it might be \"I would like to learn about artificial intelligence.\"; For the \"assistant\" role, it might be \"Artificial intelligence is technology that simulates intelligent human behavior.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19443e5c",
   "metadata": {},
   "source": [
    "**Single-turn dialogue example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please tell me what the weather is like today.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"It's sunny today and the temperature is 20 degrees Celsius.\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef1f20",
   "metadata": {},
   "source": [
    "**Multi-turn dialogue example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb33445",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to learn about artificial intelligence.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Artificial intelligence is technology that simulates human intelligent behavior.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What are the applications of artificial intelligence?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Artificial intelligence is used in fields such as autonomous driving, speech recognition, and image processing.\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

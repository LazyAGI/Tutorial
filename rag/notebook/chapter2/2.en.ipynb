{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d05e74",
   "metadata": {},
   "source": [
    "# Chapter 2: Build a Minimal RAG System in 10 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df30c",
   "metadata": {},
   "source": [
    "> In chapter 1 we introduced large-model and RAG theories. Now it's time to put that theory into practice and use LazyLLM to build the most basic RAG pipeline.\n",
    ">\n",
    "> This tutorial walks you through the steps required to assemble a minimal RAG system with LazyLLM. We'll start with the environment setup, revisit the RAG workflow, explain the document loader, retriever, and generator components, and finally wire them together to observe the output.\n",
    ">\n",
    "> Here we goÔºÅüèÉüèÉüèÉ\n",
    "\n",
    "## Environment Preparation\n",
    "\n",
    ">**Before building a RAG system with LazyLLM, finish both ‚ÄúDevelopment Environment Setup‚Äù and ‚ÄúEnvironment Variable Configuration.‚Äù**\n",
    "\n",
    "### 1. Development Environment Setup\n",
    "\n",
    "You can pick any of the following approaches to prepare the LazyLLM dev environment.\n",
    "\n",
    "#### Manual configuration‚úèÔ∏è\n",
    "\n",
    "LazyLLM is implemented in Python, so make sure `Python`, `Pip`, and `Git` are available. Installing them on macOS takes a few extra steps‚Äîsee the appendix for detailed instructions.\n",
    "\n",
    "Create a virtual environment named `lazyllm-venv` and activate it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c24329",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "python -m venv lazyllm-venv\n",
    "source lazyllm-venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a520ef3",
   "metadata": {},
   "source": [
    "If everything works you should see `(lazyllm-venv)` at the beginning of your prompt. Keep all subsequent commands inside this virtual environment.\n",
    "\n",
    "Download the `LazyLLM` code from GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea12d47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "git clone https://github.com/LazyAGI/LazyLLM.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166429ec",
   "metadata": {},
   "source": [
    "Then switch into the cloned repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2703df0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "cd LazyLLM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad32332",
   "metadata": {},
   "source": [
    "Install the base dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2d1df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dc4cf",
   "metadata": {},
   "source": [
    "Add `LazyLLM` to Python's module search path:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b158b80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "export PYTHONPATH=$PWD:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b489ab7",
   "metadata": {},
   "source": [
    "Now you can import it from any directory.\n",
    "\n",
    "#### Pull the imageüìë\n",
    "\n",
    "We also publish a Docker image with the latest LazyLLM release so you can get started immediately:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cceecb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "docker pull lazyllm/lazyllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd8398",
   "metadata": {},
   "source": [
    "You can also browse [https://hub.docker.com/r/lazyllm/lazyllm/tags](https://hub.docker.com/r/lazyllm/lazyllm/tags) and pull the tag you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cce992",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip3 install lazyllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03247558",
   "metadata": {},
   "source": [
    "Install the minimal dependency bundle that unlocks every LazyLLM feature. It supports fine-tuning and inference for hosted models as well as offline fine-tuning (powered by `LLaMA-Factory`) and offline inference (`vLLM` for LLMs, `LMDeploy` for multimodal models, and `Infinity` for embedding models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fe60b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip3 install lazyllm\n",
    "lazyllm install standard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d763c",
   "metadata": {},
   "source": [
    "Install the full dependency bundle to enable advanced features such as automatic framework selection (`AutoFinetune`, `AutoDeploy`, etc.), more offline inference engines (e.g., `LightLLM`), and additional offline training pipelines (e.g., `AlpacaloraFinetune`, `CollieFinetune`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae274c6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip3 install lazyllm\n",
    "lazyllm install full\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3da8b0",
   "metadata": {},
   "source": [
    "### 2. Configure API keys\n",
    "\n",
    "Calling LLMs can happen online or locally. For online calls you must supply the provider's API key. Register an account with that platform if you do not already have one. LazyLLM automatically reads platform API keys from environment variables: set the key once, then simply specify the platform and model name when you invoke the module.\n",
    "\n",
    "LazyLLM currently supports the following providers:\n",
    "\n",
    "| **Platform** | **How to get the API key** | **Environment variables** |\n",
    "| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
    "| [SenseNova](https://platform.sensenova.cn/) | [Get access keys (ak and sk)](https://console.sensecore.cn/help/docs/model-as-a-service/nova/) (Method 1), [Get access key (API key only)](https://console.sensecore.cn/aistudio/management/api-key) (Method 2) | `LAZYLLM_SENSENOVA_API_KEY`, `LAZYLLM_SENSENOVA_SECRET_KEY` (Method 1) / `LAZYLLM_SENSENOVA_API_KEY` (Method 2) |\n",
    "| [OpenAI](https://openai.com/index/openai-api/) | [Create an API key](https://platform.openai.com/api-keys) | `LAZYLLM_OPENAI_API_KEY` |\n",
    "| [Zhipu](https://open.bigmodel.cn/) | [Create an API key](https://open.bigmodel.cn/usercenter/apikeys) | `LAZYLLM_GLM_API_KEY` |\n",
    "| [Kimi](https://platform.moonshot.cn/) | [Create an API key](https://platform.moonshot.cn/console/api-keys) | `LAZYLLM_KIMI_API_KEY` |\n",
    "| [Tongyi Qianwen](https://help.aliyun.com/zh/dashscope/developer-reference/use-qwen-by-api) | [Create an API key](https://help.aliyun.com/zh/dashscope/developer-reference/acquisition-and-configuration-of-api-key) | `LAZYLLM_QWEN_API_KEY` |\n",
    "| [Doubao](https://www.volcengine.com/product/doubao) | [Create an API key](https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey) | `LAZYLLM_DOUBAO_API_KEY` |\n",
    "| [DeepSeek](https://www.deepseek.com/) | [Create an API key](https://platform.deepseek.com/api_keys/apiKey) | `LAZYLLM_DEEPSEEK_API_KEY` |\n",
    "\n",
    "To expose your API key as an environment variable:\n",
    "\n",
    "1.  Obtain the API key for your target platform (SenseNova requires two keys for Method 1).\n",
    "2.  Run the following command to set the environment variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec056b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "export LAZYLLM_<name_of_provider_upper_case>_API_KEY=<your_api_key>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a67077",
   "metadata": {},
   "source": [
    "For example, if you use SenseNova and obtained the key through Method 1, set the variables below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3782e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "export LAZYLLM_SENSENOVA_API_KEY=\"Your Access Key ID\"\n",
    "export LAZYLLM_SENSENOVA_SECRET_KEY=\"Your Access Key Secret\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66372650",
   "metadata": {},
   "source": [
    "If you used Method 2, set only this variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2e459",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "export LAZYLLM_SENSENOVA_API_KEY=\"Your API Key\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdd2f307",
   "metadata": {},
   "source": [
    "Once the environment variables are in place, instantiate `OnelineChatModule`, specify the model source, and LazyLLM will read the corresponding API key automatically.\n",
    "\n",
    "This makes it easy to manage API keys for multiple providers and keeps the online invocation flow simple.\n",
    "\n",
    "## RAG in Practice\n",
    "\n",
    "### 1. Review of the Core Principles ‚úàÔ∏è\n",
    "\n",
    "After the LazyLLM environment is configured, let's recap the basics of RAG (Retrieval-Augmented Generation). When the model needs to produce an answer, it first retrieves relevant information from a large collection of documents. The retrieved facts are fed into the generation step so the final response is grounded and accurate. The following diagram shows the workflow: the system receives a user query, the retriever fetches similar content from external documents, the query plus retrieved context are sent to the LLM, and the LLM returns the answer.\n",
    "\n",
    "![image.png](2_images/img2.png)\n",
    "\n",
    "The offline workflow boils down to three steps:\n",
    "\n",
    "1.  **Document ingestion and parsing (Reader)**\n",
    "\n",
    "Load documents of various formats into the system. You can use open-source tools such as MinerU to improve parsing accuracy.\n",
    "\n",
    "2.  **Chunking and vectorization (Transform and Vectorize)**\n",
    "\n",
    "Clean, deduplicate, and split the raw data, then convert it into embeddings.\n",
    "\n",
    "3.  **Indexing and storage (Indexing and Store)**\n",
    "\n",
    "Store and index the processed text in a vector database or another high-performance retrieval backend.\n",
    "\n",
    "![image-2.png](2_images/img1.png)\n",
    ">**Summary**\n",
    ">\n",
    ">The end-to-end RAG pipeline can be summarized in three phases:\n",
    ">\n",
    ">**1. Retrieval**\n",
    ">\n",
    ">Given a user query, look up related information in the **knowledge base**.\n",
    ">\n",
    ">**2. Augmentation**\n",
    ">\n",
    ">Attach the retrieved text as extra context and send it to the LLM together with the user query.\n",
    ">\n",
    ">**3. Generation**\n",
    ">\n",
    ">The **LLM** combines the retrieved knowledge with its own pretraining to produce the final answer.\n",
    ">\n",
    ">We'll now dig into LazyLLM's **document management, retriever, and generator components** in that order.\n",
    ">\n",
    ">**Here we goÔºÅüèÑ**\n",
    ">\n",
    "\n",
    "### 2. Document management üìö\n",
    "\n",
    "The core of RAG is document retrieval from a document collection. This collection may include many different formats: rich-text files such as DOCX, PDF, and PPT; plain-text formats such as Markdown; or content retrieved from an API, such as results returned by a search engine. Because these documents come in diverse formats, we need specialized parsers to extract useful information such as text, images, tables, audio, or video.\n",
    "\n",
    "In LazyLLM, these parsers are abstracted through the `Document` class. The built-in `Document` component can extract content from common rich-text formats including DOCX, PDF, PPT, and Excel. You can also implement a custom Reader to handle additional formats, which we will cover in later tutorials.\n",
    "\n",
    "The main parameters of Document are as follows:\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* **`dataset_path`** (`str`) ‚Äì Path to the dataset directory. The directory must contain the documents to be managed by the module (single-file inputs are not supported yet).\n",
    "* **`embed`** (`Optional[Union[Callable, Dict[str, Callable]]]`, default: `None`) ‚Äì Embedding generator. Provide a dict if you want to create multiple embeddings per document; the dict keys are embedding names and the values are the embedding callables.\n",
    "* **`manager`** (`bool`, default: `False`) ‚Äì Whether to spin up a UI for the document module. Defaults to `False`.\n",
    "* **`launcher`** (`optional`, default: `None`) ‚Äì Object or function that starts the server module. If omitted, LazyLLM uses the default async launcher in `lazyllm.launchers` (`sync=False`).\n",
    "* **`store_conf`** (`optional`, default: `None`) ‚Äì Configures the storage backend and index backend.\n",
    "* **`doc_fields`** (`optional`, default: `None`) ‚Äì Describes which fields should be stored and retrieved plus their types (currently used only by the Milvus backend).\n",
    "\n",
    "We'll focus on the simplest usage here. Advanced parameters such as `embed` and `manager` will be covered later. For now you only need to provide the dataset directory path:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbe1fd",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG document loading\n",
    "from lazyllm import Document\n",
    "\n",
    "# Using an absolute path\n",
    "doc = Document(\"path/to/content/docs/\")\n",
    "print(f\"Actual path passed in: {}\")\n",
    "\n",
    "# Using a relative path\n",
    "doc = Document(\"/content/docs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e20b33",
   "metadata": {},
   "source": [
    "Make sure you pass an absolute path or a path relative to the current directory. Otherwise set the `LAZYLLM_DATA_PATH` environment variable to your document root and then provide a relative path, as shown below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed3efdcf",
   "metadata": {},
   "source": [
    "![image.png](2_images/img4.png)\n",
    "\n",
    "![image-2.png](2_images/img3.png)\n",
    "\n",
    "In the snippet above we inspect `doc._manager._dataset_path` to confirm the path that was ultimately passed into the document loader‚Äîboth approaches produce identical results.\n",
    "\n",
    "### 3. Retriever component üïµ\n",
    "\n",
    "The documents in your corpus rarely align perfectly with every query, so you need a retriever to filter the relevant context. LazyLLM provides the `Retriever` component, which creates a retrieval module that can search across one or more `Document` instances using the similarity metric you specify. Key parameters include:\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* **`doc`** (`object`) ‚Äì The document module instance (or list of instances) to search.\n",
    "* **`group_name`** (`str`) ‚Äì The node group to query.\n",
    "\n",
    "`group_name` has three built-in chunking strategies, all powered by `SentenceSplitter` but with different chunk sizes:\n",
    "\n",
    "‚ó¶ CoarseChunk: chunk size 1024 with 100-token overlap<br>\n",
    "‚ó¶ MediumChunk: chunk size 256 with 25-token overlap<br>\n",
    "‚ó¶ FineChunk: chunk size 128 with 12-token overlap<br>\n",
    "\n",
    "* **`similarity`** (`Optional[str]`, default: `None`) ‚Äì Similarity metric used during retrieval. Defaults to `\"dummy\"`. Options: `\"bm25\"`, `\"bm25_chinese\"`, `\"cosine\"`.\n",
    "* **`similarity_cut_off`** (`Union[float, Dict[str, float]]`, default: `float('-inf')`) ‚Äì Drop nodes whose similarity falls below this threshold. Provide a dict to set individual thresholds for each embedding.\n",
    "* **`index`** (`str`, default: `'default'`) ‚Äì Index type used for retrieval. Currently only `'default'` is supported.\n",
    "* **`topk`** (`int`, default: `6`) ‚Äì Number of nodes to return.\n",
    "* **`embed_keys`** (`Optional[List[str]]`, default: `None`) ‚Äì Embedding keys to use when retrieving. If omitted, all embeddings are considered.\n",
    "\n",
    "The following line configures a retriever that works on the `doc` corpus, searches the Coarse chunk node group with `bm25_chinese`, and returns the top 3 most similar nodes. We'll focus on how to call the retriever here; algorithms and tuning tips will be covered later (see Practice 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513d214",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import Retriever\n",
    "\n",
    "# Load documents using an absolute path\n",
    "doc = Document(\"/path/to/content/docs/\")\n",
    "\n",
    "# Create a Retriever. Here we use the built-in chunking strategy \"CoarseChunk\"\n",
    "# and the BM25-based similarity function for Chinese text.\n",
    "retriever = Retriever(doc, group_name=Document.CoarseChunk, similarity=\"bm25_chinese\", topk=3)\n",
    "\n",
    "# Run the retriever with a query\n",
    "retriever_result = retriever(\"your query\")\n",
    "\n",
    "# Print the content of the first retrieved chunk\n",
    "print(retriever_result[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a2745",
   "metadata": {},
   "source": [
    "Let's run the code and inspect the output:\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./2_videos/ÊúÄÁÆÄÂçïÁöÑrag2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "<!-- <div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video \n",
    "    controls \n",
    "    style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\"\n",
    "    preload=\"metadata\"  \n",
    "    playsinline        \n",
    "  >\n",
    "    <source src=\"./2_videos/ÊúÄÁÆÄÂçïÁöÑrag2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div> -->\n",
    "\n",
    "`retriever_result` is a list in which each element represents a node (we'll explain nodes in Advanced 1). Call `get_content()` on a node to print its text.\n",
    "\n",
    "### 4. Generator component üôã\n",
    "\n",
    "Once we have the retrieved context and the user query, we feed both into the generator‚Äîthe LLM‚Äîto produce the final answer. We'll use an online model as an example and show how LazyLLM calls it.\n",
    "\n",
    "`OnlineChatModule` unifies access to hosted LLMs. Regardless of whether you're targeting the OpenAI, SenseNova, or any other API, LazyLLM wraps the parameters for you: simply specify the provider via `source` and the model name via `model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3a9519",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1be7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt = \"You are a small cat. After every response, you must add 'ÂñµÂñµÂñµ'.\"\n",
    "llm = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"SenseChat-5-1202\").prompt(llm_prompt)\n",
    "\n",
    "print(llm(\"Êó©‰∏äÂ•ΩÔºÅ\"))\n",
    "# >>> Good morning! The sun is up~ (stretching) Want to sit by the window and enjoy the sunlight together?\n",
    "# >>> I'll share half of my dried fish with you~ (nuzzles) ÂñµÂñµÂñµ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aa607",
   "metadata": {},
   "source": [
    "Those three lines are all you need to call an LLM. First define a prompt so the model knows how to respond‚Äîevery subsequent reply will follow that instruction.\n",
    "\n",
    "Next, configure the model by instantiating `OnlineChatModule`, passing the provider via `source` and the `model` name, and registering the prompt through `.prompt`.\n",
    "\n",
    "Finally, invoke the configured model by sending your question to `llm` just like any regular chat interface.\n",
    "\n",
    "You can also replace line 4 above with the following snippet to spawn a simple chat UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca28e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyllm.WebModule(llm, port=23466, history=[llm]).start().wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae2f537",
   "metadata": {},
   "source": [
    "Here we start a lightweight web client with `lazyllm.WebModule`. Paste the host and port into your browser to interact with the chat interface.\n",
    "\n",
    "![image.png](2_images/img6.png)\n",
    "\n",
    "## Build the RAG Knowledge Base\n",
    "\n",
    "With the foundational components in place, we can now assemble a complete RAG workflow. Before that, we need a corpus. We'll construct a knowledge base from the original [cmrc2018](https://huggingface.co/datasets/hfl/cmrc2018) dataset and reuse it throughout the remaining chapters whenever we discuss improvements or optimizations.\n",
    "\n",
    "### 1. Dataset overview\n",
    "\n",
    "CMRC 2018 (Chinese Machine Reading Comprehension 2018) [1] is a Chinese span-extraction reading comprehension dataset. Human annotators created nearly 20,000 real questions based on Wikipedia passages to increase linguistic diversity in Chinese MRC tasks.\n",
    "\n",
    "![image-2.png](2_images/img5.png)\n",
    "\n",
    "Each record contains four fields: `id`, `context`, `question`, and `answers`. `id` is the identifier, `context` is a descriptive passage (history, news, fiction, etc.), and `answers` lists the human-written spans. `answer_start` marks the token offset inside `context`, and `text` stores the actual answer. In the example below two experts annotated the same question independently to ensure accuracy.\n",
    "\n",
    "Download the dataset locally with the `datasets` library (install it first via `pip install datasets` if needed):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40ed70",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4aa9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('cmrc2018')\n",
    "# Alternatively, specify a custom download path:\n",
    "# dataset = load_dataset('cmrc2018', cache_dir='path/to/datasets')\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbbf5f34",
   "metadata": {},
   "source": [
    "The dataset is downloaded to `.cache/huggingface/datasets` by default. Pass `cache_dir` if you prefer a custom path. After the download completes you'll see `train`, `validation`, and `test` splits, as shown below:\n",
    "\n",
    "![image.png](2_images/img7.png)\n",
    "\n",
    "### 2. Build the knowledge base\n",
    "\n",
    "We'll use only the `test` split for the RAG corpus; the other splits will be used later when we fine-tune models (we'll explain why in those chapters). Each sample contains `id`, `context`, `question`, and `answers`. We'll feed the `context` values into the knowledge base so we can evaluate the RAG system by querying with the paired `question` and comparing the generated answer with the original `answers`. Sound confusing? Don't worry‚Äîfuture chapters will break it down. For now just remember that **the knowledge base is built from the `context` field of the `test` split.** The code is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888a637",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd62fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_KB(dataset):\n",
    "    \"\"\"Create a knowledge base from the 'context' field in the test set.\n",
    "    Every 10 entries are saved into one .txt file. The remaining entries\n",
    "    (if fewer than 10) are saved into a final file.\n",
    "    \"\"\"\n",
    "    Context = []\n",
    "    for i in dataset:\n",
    "        Context.append(i['context'])\n",
    "    Context = list(set(Context))  # Deduplicate and obtain 256 unique items\n",
    "\n",
    "    # Compute the number of files needed\n",
    "    chunk_size = 10\n",
    "    total_files = (len(Context) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "\n",
    "    # Create the folder 'data_kb' to store the text files\n",
    "    os.makedirs(\"data_kb\", exist_ok=True)\n",
    "\n",
    "    # Write every group of 10 entries into separate files\n",
    "    for i in range(total_files):\n",
    "        chunk = Context[i * chunk_size : (i + 1) * chunk_size]\n",
    "        file_name = f\"./data_kb/part_{i+1}.txt\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(chunk))  # Write separated by newline\n",
    "\n",
    "        # print(f\"File {file_name} written successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba419a3",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "* Lines 2‚Äì3 iterate over `data`, grab each entry's `context`, and append it to the `Context` list. By the end `Context` holds every passage.\n",
    "* Line 6 deduplicates the contexts by wrapping the list in `set()` and then converting it back to a list.\n",
    "* Lines 9‚Äì10 set `chunk_size = 10`, meaning each text file stores up to 10 contexts. `total_files` uses ceiling division `(len(Context) + chunk_size - 1) // chunk_size` so the remainder still gets written to a file.\n",
    "* Lines 15‚Äì20 split the contexts into groups of 10 and write each group to a standalone `.txt` file:\n",
    "  * `for i in range(total_files)`: iterate over the number of files we need.\n",
    "  * `chunk = Context[i * chunk_size : (i + 1) * chunk_size]`: grab the current batch of 10 contexts.\n",
    "  * `file_name = f\"./data_kb/part_{i+1}.txt\"`: generate unique file names such as `part_1.txt`, `part_2.txt`, etc.\n",
    "  * `with open(file_name, \"w\", encoding=\"utf-8\") as f:`: open each file for writing with UTF-8 encoding.\n",
    "  * `f.write(\"\n",
    "\".join(chunk))`: join the contexts with newlines and flush them to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654bd86",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c040f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the knowledge base using create_KB()\n",
    "create_KB(dataset['test'])\n",
    "\n",
    "# Display the contents of one of the generated text files\n",
    "with open('data_kb/part_1.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f8393cf",
   "metadata": {},
   "source": [
    "After the script finishes you'll see a `data_kb` folder in the current directory containing multiple `.txt` files:\n",
    "\n",
    "![image.png](2_images/img9.png)\n",
    "\n",
    "The files look like this:\n",
    "\n",
    "![image-2.png](2_images/img8.png)\n",
    "\n",
    ">**Keep track of the `data_kb` path. Unless otherwise noted, all upcoming RAG examples will use this directory as the knowledge base.**\n",
    "\n",
    "### 3. Environment check\n",
    "\n",
    "Verify whether the SQLite library on your machine supports multithreading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a093732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm.common.queue import sqlite3_check_threadsafety\n",
    "print(sqlite3_check_threadsafety())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15707d0b",
   "metadata": {},
   "source": [
    "If the result is **False**, reinstall **SQLite** with multithreading support. On macOS you can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4866b7f",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew update\n",
    "brew install sqlite\n",
    "which sqlite3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9082b",
   "metadata": {},
   "source": [
    "If the output path is **not** the Homebrew-installed SQLite, export the variables below and reinstall Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d61db2",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew uninstall python\n",
    "export PATH=\"/opt/homebrew/opt/sqlite/bin:$PATH\"\n",
    "export LDFLAGS=\"-L/opt/homebrew/opt/sqlite/lib\"\n",
    "export CPPFLAGS=\"-I/opt/homebrew/opt/sqlite/include‚Äù\n",
    "brew install python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ed2ef",
   "metadata": {},
   "source": [
    "## Build the basic RAG\n",
    "\n",
    "Now that we understand the three core components and prepared the knowledge base, let's implement the simplest RAG pipeline.\n",
    "\n",
    "Remember the three building blocks:\n",
    "\n",
    "* **Document** ‚Äì Loads and manages the corpus; just point it to your documents.\n",
    "* **Retriever** ‚Äì Searches the document store. Configure which corpus to search, the strategy, and the number of hits to return.\n",
    "* **LLM** ‚Äì Generates the final answer from the query plus retrieved documents. LazyLLM offers `TrainableModule` for local models and `OnlineChatModule` for hosted models so you can switch between them without changing code.\n",
    "\n",
    "Chaining these components together yields the simplest RAG system:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526d8ac",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter2/%E5%AE%9E%E6%88%981%EF%BC%9A%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84RAG.py#L89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "# Load documents\n",
    "documents = lazyllm.Document(dataset_path=\"/content/docs\")\n",
    "\n",
    "# Define the retriever\n",
    "retriever = lazyllm.Retriever(\n",
    "    doc=documents,\n",
    "    group_name=\"CoarseChunk\",\n",
    "    similarity=\"bm25_chinese\",\n",
    "    topk=3\n",
    ")\n",
    "\n",
    "# Define the generation module\n",
    "llm = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"SenseChat-5\")\n",
    "\n",
    "# Prompt design\n",
    "prompt = (\n",
    "    \"You will act as an AI question-answering assistant and complete a dialogue task. \"\n",
    "    \"In this task, you should provide your answers based on the given context and question.\"\n",
    ")\n",
    "llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "# Inference\n",
    "query = \"Introduce Yushan arrow bamboo for me.\"\n",
    "\n",
    "# Retrieve document nodes\n",
    "doc_node_list = retriever(query=query)\n",
    "\n",
    "# Build input for the model by combining query and retrieved content\n",
    "res = llm({\n",
    "    \"query\": query,\n",
    "    \"context_str\": \"\".join([node.get_content() for node in doc_node_list])\n",
    "})\n",
    "\n",
    "print(f'With RAG Answer: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395db36b",
   "metadata": {},
   "source": [
    "Here's how that code works:\n",
    "\n",
    "1.  Lines 4, 7, and 10 instantiate the document loader (`document`), retriever (`retriever`), and generator (`llm`) respectively.\n",
    "2.  Line 13 defines the LLM prompt via `llm.prompt`. The built-in `ChatPrompter` formats the RAG instructions so the model knows which parts are references and which part is the user query. It also converts the content into the wire format expected by online APIs.\n",
    "3.  Line 17 captures the user query, line 19 calls the retriever, and the results are stored in the list `doc_mode_list`.\n",
    "4.  Line 21 calls the LLM with a dict containing the question (`query`) and the concatenated retrieval context (`content_str`), which we build by calling `.get_content()` on each node and joining the strings.\n",
    "\n",
    "For comparison, here's what happens when you call the LLM without any RAG context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generation module (without RAG)\n",
    "llm_without_rag = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"SenseChat-5\")\n",
    "\n",
    "# Query\n",
    "query = \"Introduce Yushan arrow bamboo for me.\"\n",
    "\n",
    "# Run the model directly without retrieval\n",
    "res = llm_without_rag(query)\n",
    "\n",
    "print(f'Without RAG Answer: {res}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "493fce07",
   "metadata": {},
   "source": [
    "Let's review the runtime output:\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./2_videos/ÊúÄÁÆÄÂçïÁöÑrag2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "References:\n",
    "\n",
    "[1] A Span-Extraction Dataset for Chinese Machine Reading Comprehension\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: Install `Python`, `Pip`, and `Git` on macOS\n",
    "\n",
    "1.  Install Xcode first.\n",
    "\n",
    "![image.png](2_images/img10.png)\n",
    "\n",
    "Accept the license agreement and install the Xcode command-line tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1603880",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "sudo xcode-select -s /Applications/Xcode.app/Contents/Developer  # Ensure the path is correct\n",
    "sudo xcodebuild -license accept                                  # Accept the license agreement\n",
    "xcode-select --install                                           # Install Xcode Command Line Tools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739dd67a",
   "metadata": {},
   "source": [
    "2.  Install Homebrew, then use it to install Python and pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8ead9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zshrc\n",
    "source ~/.zshrc\n",
    "brew install pyenv\n",
    "pyenv install 3.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f9e9f",
   "metadata": {},
   "source": [
    "3.  Install Python 3.10. Do not install Python 3.13 because the `spacy` dependency does not support it yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865f74d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.zshrc\n",
    "echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.zshrc\n",
    "echo 'eval \"$(pyenv init --path)\"' >> ~/.zshrc\n",
    "echo 'eval \"$(pyenv init -)\"' >> ~/.zshrc\n",
    "pyenv global 3.10.0\n",
    "python3 -m venv lazyllm\n",
    "source lazyllm/bin/activate\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986b3fb9",
   "metadata": {},
   "source": [
    "# Chapter 12: Practice: Accelerate your RAG with caching, asynchronous and vector engines\n",
    "\n",
    ">Advanced 2 introduces several methods to improve the efficiency of the RAG system, including persistent storage, more efficient vector indexing, engineering optimization in actual projects, and model inference optimization. As a corresponding practical tutorial, this section will first introduce you to how to use LazyLLM to implement persistent storage of knowledge base, and then the course will introduce LazyLLM's custom index component. Here we will teach you step by step how to use LazyLLM to create and use a custom index for retrieval. At the same time, the course will introduce the basic use of the high-performance vector database Milvus, and how to quickly access Milvus in LazyLLM to achieve high-speed vector search. Further, the course will introduce the use of the vLLM framework in LazyLLM to accelerate model inference and the use of quantized models to reduce hardware requirements. (For practical experience related to model distillation, you can jump to Elective 4 to learn.)\n",
    "\n",
    "## Environment preparation and basic component definition\n",
    "\n",
    "If Python is installed on your computer, please install lazyllm and necessary dependency packages through the following command. For more detailed preparations about the LazyLLM environment, please refer to the corresponding content in Lecture 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce567e6d",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "pip install lazyllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f4131",
   "metadata": {},
   "source": [
    "After successfully installing lazyllm, we define the following components: large model llm, vector model embedding_model, and rerank_model. These components will be frequently used in the following practical process. After this part is predefined, it will not be defined again later.\n",
    "\n",
    "For developers with tight GPU resources, it is recommended that you use the online model throughout the process to quickly get started with development and lower the threshold for use. The online model is created as follows ([Code Github link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/online_models.py#L31)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a81e6",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from lazyllm import OnlineChatModule, OnlineEmbeddingModule\n",
    "\n",
    "DOUBAO_API_KEY = \"\"\n",
    "DEEPSEEK_API_KEY = \"\"\n",
    "QWEN_API_KEY = \"\"\n",
    "\n",
    "#Online large model, DeepSeek-V3 is used here\n",
    "llm = OnlineChatModule(\n",
    "    source=\"deepseek\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    ")\n",
    "\n",
    "#Vector model, here select the bean bag vector model\n",
    "embedding_model = OnlineEmbeddingModule(\n",
    "    source=\"doubao\",\n",
    "    embed_model_name=\"doubao-embedding-large-text-240915\",\n",
    "    api_key=DOUBAO_API_KEY,\n",
    ")\n",
    "\n",
    "#Rearrangement model. The online rearrangement model only supports Qianwen and Zhipu. The Qianwen rearrangement model is used here.\n",
    "rerank_model = OnlineEmbeddingModule(\n",
    "    source=\"qwen\",\n",
    "    api_key=QWEN_API_KEY,\n",
    "    type=\"rerank\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593a0c2",
   "metadata": {},
   "source": [
    "For some developers who want to use remote connection to connect vector models, but there is no support within LazyLLM, they can use the inheritance method introduced previously, implement the two methods and then create a custom OnlineEmbeddingModule ([Code Github link](https://git hub.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/online_models.py#L8)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "from lazyllm.module import OnlineEmbeddingModuleBase\n",
    "\n",
    "\n",
    "class CustomOnlineEmbeddingModule(OnlineEmbeddingModuleBase):\n",
    "    \"\"\"CustomOnlineEmbeddingModule\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_url,\n",
    "            embed_model_name,\n",
    "            api_key,\n",
    "            model_series):\n",
    "        super().__init__(\n",
    "            embed_url=embed_url, embed_model_name=embed_model_name, api_key=api_key, model_series=model_series\n",
    "        )\n",
    "    \n",
    "#Implement request data encapsulation method\n",
    "    def _encapsulated_data(self, text: str, **kwargs) -> Dict[str, str]:\n",
    "        json_data = {\"inputs\": text, \"model\": self._embed_model_name}\n",
    "        if len(kwargs) > 0:\n",
    "            json_data.update(kwargs)\n",
    "\n",
    "        return json_data\n",
    "    \n",
    "#Implement response parsing method\n",
    "    def _parse_response(self, response: Union[List[List[str]], Dict]) -> Union[List[List[str]], Dict]:\n",
    "        return response\n",
    "\n",
    "\n",
    "# Just pass in the correct URL and model information. The following is the remote bge-m3 model.\n",
    "embedding_model = CustomOnlineEmbeddingModule(\n",
    "    embed_url=\"\",\n",
    "    embed_model_name=\"BAAI/bge-m3\",\n",
    "    api_key=\"\",\n",
    "    model_series=\"bge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33615c0e",
   "metadata": {},
   "source": [
    "For scenarios where GPU resources are relatively sufficient, or where model efficiency or concurrency requirements are high, it is recommended to use lazyllm’s built-in TrainableModule to implement local deployment of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd8127",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from lazyllm import TrainableModule, pipeline\n",
    "\n",
    "llm = TrainableModule('internlm2-chat-20b', stream=True)\n",
    "embedding_model = TrainableModule(\"bge-large-zh-v1.5\")\n",
    "rerank_model = TrainableModule(\"bge-reranker-large\")\n",
    "\n",
    "pipeline(llm, embedding_model, rerank_model).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39eb7d3",
   "metadata": {},
   "source": [
    "## 1. Use vector database to implement persistent storage of knowledge base\n",
    "\n",
    "Vector database is a database system specially used to store, manage and retrieve high-dimensional vector data. It is widely used in search engines, recommendation systems, computer vision and natural language processing and other fields. Different from traditional relational databases, vector databases can efficiently store high-dimensional vectors and use similarity retrieval and optimized indexing technologies to greatly improve the efficiency of large-scale vector searches. They are an important data storage and retrieval tool for current AI applications.\n",
    "\n",
    "LazyLLM natively supports two open source vector databases, ChromaDB and Milvus, providing out-of-the-box vector storage and retrieval support. In actual use, users only need to configure the store_conf parameter in the Document definition stage and perform simple **storage and retrieval** configuration** to use these two databases to store the document-processed data locally, and load the data directly from the local when the next system starts, avoiding repeated entry of documents into the database and achieving persistent storage of the knowledge base. The implementation of using store_conf to configure Document storage is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = lazyllm.Document(dataset_path='/path/to/your/document',\n",
    "           embed=lazyllm.OnlineEmbeddingModule(),\n",
    "           ...,\n",
    "           store_conf=store_conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a0ae50c",
   "metadata": {},
   "source": [
    "The parameter **store_conf** receives the storage configuration in the form of a dictionary. The type and kwargs parameters need to be passed in during configuration. The specific fields are explained as follows:\n",
    "\n",
    "- type: Use storage type. As introduced before, the storage types currently supported by LazyLLM are map (based on memory), chroma (based on ChromaDB vector database) and milvus (based on Milvus vector database):\n",
    "  - map: Only use memory key/value storage to temporarily store all document processing data. The data will disappear after the system is restarted;\n",
    "  - chroma: Use ChromaDB to store data. ChromaDB is a lighter vector database. Compared with Milvus, the amount of data it can handle is limited and suitable for debugging. For more information about ChromoDB, please refer to [ChromaDB official documentation](https://www.trychroma.com/);\n",
    "  - milvus: Use Milvus to store data, support the retrieval and storage of massive data, support distributed deployment, and are more suitable for production-level applications. For more information about Milvus, please refer to [Milvus official documentation](https://www.milvus-io.com/overview);\n",
    "- kwargs: Additional configuration parameters required for specific storage, also entered in the form of a dictionary.\n",
    "  - ![image.png](12_images/img1.png)\n",
    "\n",
    "- When type is chroma, the required configuration parameters are:\n",
    "  - dir (required): directory where data is stored;\n",
    "- When type is milvus, the required configuration parameters are:\n",
    "  - uri (required): storage data address, which can be a **file path** or **remote connection url**;\n",
    "  - index_kwargs (optional): Index configuration of Milvus database, which can be a dict or list[dict]. The index configuration mainly contains two fields: index type index_type and metric type metric_type (i.e. Index and Similarity, very similar to the definition of LazyLLM). If the parameter is a dict, it means that all vector models use the same configuration; if you need to specify different vector models with different index configurations, you can pass it in in the form of a list, and embed_key can specify the configuration used. Currently, only floating point embedding and sparse embedding are supported. The parameters supported are as follows. Interested developers can go to the official website to view:\n",
    "    - [floating point embedding](https://milvus.io/docs/index-vector-fields.md?tab=floating)\n",
    "    - [sparse embedding](https://milvus.io/docs/index-vector-fields.md?tab=sparse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd1a5cac",
   "metadata": {},
   "source": [
    "### Practice 1 Use vector database to implement persistent storage\n",
    "\n",
    "#### System startup performance comparison\n",
    "\n",
    "In this part, while the other modules remain unchanged, memory, ChromaDB and Milvus are used as data storage respectively. The first and second startup times of the three were tested respectively to demonstrate the improvement of the efficiency of the secondary startup of the system by persistent storage. The file directory contains files in pdf, docx, and txt formats, with a total of 1614 nodes after being divided into blocks according to sentences.\n",
    "\n",
    "![image.png](12_images/img2.png)\n",
    "\n",
    "- Memory storage\n",
    "\n",
    "Since LazyLLM uses memory as the storage type by default, developers can define a Document that uses memory storage without any additional configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a23413",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "document = lazyllm.Document(\n",
    "    dataset_path='/path/to/your/document',\n",
    "    embed=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5b8de",
   "metadata": {},
   "source": [
    "- ChromaDB\n",
    "\n",
    "ChromaDB is characterized by ease of use, high performance and tight integration of AI applications. It is suitable for application scenarios that require rapid deployment and lightweight management, such as chat robots, personalized recommendations, search engines, etc. Since ChromaDB implements persistent storage based on sqlite3, it does not support distributed deployment and is suitable for small local applications. ChromaDB using LazyLLM needs to configure the following fields in store_conf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60373554",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "chroma_store_conf = {\n",
    "  'type': 'chroma', \n",
    "  'kwargs': {\n",
    "'dir': 'dbs/test_chroma', # The dir passed in by chromadb is a folder. If it does not exist, it will be automatically created.\n",
    "   }\n",
    "}\n",
    "\n",
    "document = lazyllm.Document(\n",
    "    dataset_path='/path/to/your/document',\n",
    "    embed=embedding_model,\n",
    "    store_conf=chroma_store_conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bf266",
   "metadata": {},
   "source": [
    "- Milvus\n",
    "\n",
    "As an open source vector database designed for high-performance vector retrieval, Milvus can efficiently handle trillions of vector indexes. Compared with ChromaDB, Milvus not only provides vector storage, but also provides a high-speed retrieval interface, which can achieve efficient retrieval of vectors based on multiple indexing methods. To use Milvus as the storage backend, you need to pass in the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cfa1f",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "milvus_store_conf = {\n",
    "'type': 'milvus', # Specify the storage backend type\n",
    "  'kwargs': {\n",
    "'uri': 'test.db', # Store the backend address. This example uses the local file test.db. If the file does not exist, create a new file.\n",
    "'index_kwargs': { # Storage backend index configuration\n",
    "'index_type': 'FLAT', # Index type\n",
    "'metric_type': 'COSINE', # Similarity calculation method\n",
    "    }\n",
    "  },\n",
    "}\n",
    "\n",
    "document = lazyllm.Document(\n",
    "    dataset_path='/path/to/your/document',\n",
    "    embed=embedding_model,\n",
    "    store_conf=milvus_store_conf \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e82fb6",
   "metadata": {},
   "source": [
    ">Please note that currently LazyLLM supports Milvus 2.4.x, please note that your pymilvus >= 2.4.11, milvus-lite == 2.4.10.\n",
    ">\n",
    ">When using Milvus, the retrieval will be based on the index configuration in the input configuration and the corresponding algorithm will be retrieved. There is no need to specify the similarity parameter in Retriever.\n",
    "\n",
    "For a quick comparison, write the following test script to implement the performance test of the three stores ([Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/use_diffierent_vector_store.py#L1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import lazyllm\n",
    "from lazyllm import LOG\n",
    "\n",
    "from online_models import embedding_model # Use online models\n",
    "\n",
    "DOC_PATH = os.path.abspath(\"docs\") # General directory of practical documents\n",
    "\n",
    "def test_store(store_conf: dict=None):\n",
    "    \"\"\"Receive storage configuration and test system startup performance under different configurations\"\"\"\n",
    "    st1 = time.time()\n",
    "    dataset_path = os.path.join(DOC_PATH, \"test\") # The path where the document is located\n",
    "\n",
    "    docs = lazyllm.Document(\n",
    "        dataset_path=dataset_path,\n",
    "    embed=embedding_model, # Set embedding model\n",
    "    store_conf=store_conf #Set storage configuration\n",
    "        )\n",
    "    docs.create_node_group(name='sentence', parent=\"MediumChunk\", transform=lambda x: x.split('.')) # Create a node group\n",
    "    \n",
    "    if store_conf and store_conf.get('type') == \"milvus\":\n",
    "# When the storage type is milvus, no similarity parameter is required\n",
    "        retriever1 = lazyllm.Retriever(docs, group_name=\"sentence\", topk=3)\n",
    "    else:\n",
    "# similariy=cosine to use vector retrieval\n",
    "        retriever1 = lazyllm.Retriever(docs, group_name=\"sentence\", similarity='cosine', topk=3)\n",
    "    \n",
    "    retriever1.start() # Start the retriever\n",
    "    et1 = time.time()\n",
    "\n",
    "# Test the time taken for a single retrieval\n",
    "    st2 = time.time()\n",
    "    res = retriever1(\"Chinatown\")\n",
    "    et2 = time.time()\n",
    "    nodes = \"\\n======\\n\".join([node.text for node in res]) # Output the search results\n",
    "    msg = f\"Init time: {et1 - st1}, retrieval time: {et2 - st2}s\\n\" # Output system time\n",
    "    LOG.info(msg)\n",
    "    LOG.info(nodes)\n",
    "    return msg\n",
    "\n",
    "def test_stable_store():\n",
    "    \"\"\"Test multiple storage configurations at once\"\"\"\n",
    "    chroma_store_conf = {\n",
    "        'type': 'chroma', \n",
    "        'kwargs': {\n",
    "            'dir': 'dbs/chroma1',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    milvus_store_conf = {\n",
    "        'type': 'milvus',\n",
    "        'kwargs': {\n",
    "            'uri': 'dbs/milvus1.db',\n",
    "            'index_kwargs': {\n",
    "            'index_type': 'HNSW',\n",
    "            'metric_type': 'COSINE',\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    test_conf = {\n",
    "        \"map\": None,\n",
    "        \"chroma\": chroma_store_conf,\n",
    "        \"milvus\": milvus_store_conf\n",
    "    }\n",
    "    start_times = \"\"\n",
    "    for store_type, store_conf in test_conf.items():\n",
    "        LOG.info(f\"Store type: {store_type}\")\n",
    "        res = test_store(store_conf=store_conf)\n",
    "        start_times += res\n",
    "    print(start_times)\n",
    "\n",
    "test_stable_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c65ca8",
   "metadata": {},
   "source": [
    "The running results are as follows:\n",
    "\n",
    "| **Storage type** | **First startup/retrieval time (s)** | **Second startup/retrieval time (s)** |\n",
    "| ------------ | ---------------------------- | ---------------------------- |\n",
    "| map          | 17.71/0.36                   | 17.30/0.36                   |\n",
    "| chroma       | 17.69/0.37                   | 13.89(↓21.5%)/0.36           |\n",
    "| milvus       | 15.30/0.02                   | 1.80(↓88.2%)/0.02            |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df5f2003",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](12_images/img4.png)\n",
    "\n",
    "![image-2.png](12_images/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bf06d",
   "metadata": {},
   "source": [
    "\n",
    "According to the time-consuming data, we can see that due to performance errors when the system runs the program multiple times, using two vector databases to implement data persistence storage will improve system startup performance to varying degrees. When the system is started for the first time, the system initialization time of the three storage types is not much different, all at 16 to 17 seconds. However, when the system is started for the second time, the initialization time widens the gap. Among them, using ChromaDB has a small improvement in system startup efficiency, saving about 21% of the time. Using Milvus has greatly improved the system's secondary startup performance, saving about 87.36% of the startup time. It can be seen that using the storage backend can save time during the second startup of the system, and after using persistent storage, the system does not need to re-execute document blocking and document embedding, thereby saving a certain amount of computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a761f3",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_1.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a56ae6",
   "metadata": {},
   "source": [
    "#### Storage data management\n",
    "\n",
    "LazyLLM provides a storage data management interface. By passing in manager=True or manager='ui' when creating a document, you can implement the addition, deletion, modification, and query functions of stored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "document = lazyllm.Document(dataset_path='/path/to/your/document',\n",
    "                            embed=lazyllm.OnlineEmbeddingModule(),\n",
    "                            store_conf=milvus_store_conf,\n",
    "                            manager=True) # manager='ui'\n",
    "\n",
    "document.start().wait()\n",
    "doc_manager_url = document.manager.url \n",
    "#doc_manager_url should be an address in the form http://127.0.0.1:12345/generate\n",
    "# The valid part is http://127.0.0.1:12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25f2f7",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_2_doc_manager.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_3_doc_manager.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8817965",
   "metadata": {},
   "source": [
    "\n",
    "- manager=True \n",
    "\n",
    "After starting the document management service, users can implement document management functions through a variety of interfaces, including querying documents, querying node groups, adding or updating documents, querying or updating document metadata, and other functions.\n",
    "\n",
    "- manager='ui' \n",
    "\n",
    "Provide a more intuitive interface to facilitate developers to view the group list, view the group file list, upload and delete files directly on the page. In both cases, users can add, delete, modify, and query documents through the interface. You can access the Swagger API interface document at http://127.0.0.1:12345/docs to obtain more detailed interface documentation. If you need to add or delete documents, you can perform specified operations through the http protocol. Specifically, document manager provides the following interfaces\n",
    "\n",
    "> API documentation GET /docs\n",
    ">\n",
    "> List files GET /list_files\n",
    ">\n",
    "> Upload files POST /upload_files\n",
    ">\n",
    "> Add files POST /add_files\n",
    ">\n",
    "> Delete files POST /delete_files\n",
    ">\n",
    "> Add metafile POST /add_metadata\n",
    ">\n",
    "> Delete meta information POST /delete_metadata_item\n",
    ">\n",
    "> Add or update meta information /update_or_create_metadata_keys\n",
    ">\n",
    "> Reset meta information /reset_metadata\n",
    "\n",
    "Here is a simple example of uploading a file. You only need to splice the relevant URL from the API document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5781134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import requests\n",
    "\n",
    "doc_manager_url = '127.0.0.1:12345' # Change this to your url\n",
    "\n",
    "# Get the number of documents in the document library\n",
    "res = requests.get(f\"http://{doc_manager_url}/list_files\")\n",
    "data = res.json().get('data')\n",
    "# data is in the form\n",
    "# [['36e494cd803e2eb0d04772a08277174ea13c1e9ba8a001c012def7437afc7d74', \n",
    "#   'test.txt', \n",
    "#   'file_path', \n",
    "#   '2025-03-07 12:03:52', \n",
    "#   '2025-03-07 12:03:52', \n",
    "#   '{\"docid\": \"36e494cd803e2eb0d04772a08277174ea13c1e9ba8a001c012def7437afc7d74\", \n",
    "#   \"lazyllm_doc_path\": \"file_path\"}', \n",
    "#   'success',\n",
    "#    1]]\n",
    "print(len(data))\n",
    "\n",
    "# Upload virtual document\n",
    "# Here the document will be uploaded to the dataset_path path when Document is initialized.\n",
    "# Note, just fill in the file name at the location of 'test1.txt'\n",
    "files = [('files', ('test1.txt', io.BytesIO(b\"file1 content\"), 'text/plain')),\n",
    "        ('files', ('test2.txt', io.BytesIO(b\"file2 content\"), 'text/plain'))]\n",
    "\n",
    "# Parameters, override is set to true to overwrite the original document with the same name, metadatas are document meta information, you can upload it as needed\n",
    "data = dict(override='true', metadatas=json.dumps([{\"version\": \"v1.2\"}，{\"version\": \"v1.3\"}]), user_path='/path')\n",
    "# Splice parameters into url\n",
    "url = f\"http://{doc_manager_url}/upload_files\" + ('?' + '&'.join([f'{k}={v}' for k, v in data.items()]))\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "# Get the number of documents in the document library\n",
    "res = requests.get(f\"http://{doc_manager_url}:39598/list_files?details=False\")\n",
    "data = res.json().get('data')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9963c5",
   "metadata": {},
   "source": [
    "Assuming that the document library has only one file, the output of the above code should satisfy:\n",
    "\n",
    "```bash\n",
    " 1\n",
    " 3\n",
    "```\n",
    "\n",
    "## 2. Build efficient index to improve retrieval performance\n",
    "\n",
    "Before this lesson, we learned to create multi-node groups and use multi-path recall to optimize the retrieval effect. The addition of this strategy has higher requirements for the performance of the recall phase. In this section, we will show you the power of indexing through several sets of practices. We will teach you step by step how to use LazyLLM's custom index component to create an efficient index, use an efficient Milvus database from scratch, and use the Milvus database in the simplest way in LazyLLM. Finally, we will introduce you to some engineering optimization techniques in the RAG system (such as caching and parallel mechanisms), and comprehensively introduce to you how to improve the efficiency of the retrieval stage.\n",
    "\n",
    "### Practice 2 Build an efficient index and improve retrieval speed\n",
    "\n",
    "According to the previous introduction, there are currently some indexes (Index) built into LazyLLM, such as DefaultIndex, _FileNodeIndex and SmartEmbeddingIndex. Among them, DefaultIndex is the default Index, which supports embedding and text types. It performs similarity calculation in order and returns topk nodes, that is, linear search. Linear search has high time complexity and is not suitable for retrieval of large data sets; _FileNodeIndex supports obtaining corresponding nodes through file; SmartEmbeddingIndex supports high-speed vector retrieval through the Milvus interface.\n",
    "\n",
    "#### Let’s try our hand at building a dictionary tree using IndexBase\n",
    "\n",
    "Since the built-in linear search retrieval efficiency of LazyLLM is low, this practice will introduce how to use LazyLLM to build a custom index and improve the speed of vector retrieval. In LazyLLM, all indexes are created using the IndexBase component, and then the index component can be registered in the Document and used in subsequent retrieval processes. Let's first take the dictionary lookup example in Lecture 11 and use IndexBase to create a dictionary tree to achieve efficient word query.\n",
    "\n",
    "##### Index implementation\n",
    "\n",
    "Creating a custom index using IndexBase requires implementing three methods: index update (update), node removal (remove), and retrieval (query). The dictionary tree is a multi-tree, and we create a dictionary tree index according to its definition (see [GitHub link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/retriever_with_custom_index_trie_tree.py#L16)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "import lazyllm\n",
    "from lazyllm.tools.rag import IndexBase, LazyLLMStoreBase, DocNode\n",
    "from lazyllm.common import override\n",
    "\n",
    "#Define dictionary tree nodes\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TrieNode] = {} # The child node set key is a letter\n",
    "        self.is_end_of_word: bool = False # Complete word mark\n",
    "        self.uids: set[str] = set() # Save the complete word uid of the current node\n",
    "\n",
    "class TrieTreeIndex(IndexBase):\n",
    "    def __init__(self, store: 'LazyLLMStoreBase'):\n",
    "        self.store = store # Bind storage\n",
    "        self.root = TrieNode() # Define the root node\n",
    "        self.uid_to_word: Dict[str, str] = {} # uid--word mapping relationship\n",
    "\n",
    "    @override\n",
    "    def update(self, nodes: List['DocNode']) -> None:\n",
    "        if not nodes or nodes[0]._group != 'trie_tree':\n",
    "            return\n",
    "        # Create an index for each word\n",
    "        for n in nodes:\n",
    "            uid = n._uid\n",
    "            word = n.text\n",
    "            self.uid_to_word[uid] = word\n",
    "            node = self.root\n",
    "            # Iterate through each letter from the first letter of the word\n",
    "            for char in word:\n",
    "                # Get the child branch of the corresponding letter of the node\n",
    "                node = node.children.setdefault(char, TrieNode())\n",
    "            node.is_end_of_word = True\n",
    "            node.uids.add(uid)\n",
    "\n",
    "    @override\n",
    "    def remove(self, uids: List[str], group_name: Optional[str] = None) -> None:\n",
    "        \"\"\"Remove word from index\"\"\"\n",
    "        if group_name != 'trie_tree':\n",
    "            return\n",
    "        for uid in uids:\n",
    "            word = self.uid_to_word.pop(uid, None)\n",
    "            if not word:\n",
    "                continue\n",
    "            self._remove(self.root, word, 0, uid)\n",
    "            \n",
    "    def _remove(self, node: TrieNode, word: str, index: int, uid: str) -> bool:\n",
    "        if index == len(word):\n",
    "            if uid not in node.uids:\n",
    "                return False\n",
    "            node.uids.remove(uid)\n",
    "            node.is_end_of_word = bool(node.uids)\n",
    "            return not node.children and not node.uids\n",
    "        char = word[index]\n",
    "        child = node.children.get(char)\n",
    "        if not child:\n",
    "            return False\n",
    "        should_delete = self._remove(child, word, index + 1, uid)\n",
    "        if should_delete:\n",
    "            del node.children[char]\n",
    "            return not node.children and not node.uids\n",
    "        return False\n",
    "\n",
    "    @override\n",
    "    def query(self, query: str, group_name: str, **kwargs) -> List[str]:\n",
    "        node = self.root\n",
    "        # Traverse each letter of the word in query and find whether the word exists from the dictionary tree\n",
    "        for char in query:\n",
    "            node = node.children.get(char)\n",
    "            if node is None:\n",
    "                return []\n",
    "        return self.store.get_nodes(group_name=group_name, uids=list(node.uids)) if node.is_end_of_word else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c616a",
   "metadata": {},
   "source": [
    "The above code defines a scalar index based on a dictionary tree. First, this class inherits from IndexBase. The update and remove methods are used to add and delete the index of the corresponding word. The query method is used to find whether the target word is included in the dictionary tree. If the word is included in the dictionary tree, the node corresponding to the word is returned.\n",
    "\n",
    ">Note: When using the default storage (memory storage), the update and deletion of nodes will automatically be associated with the update and deletion of Index.\n",
    "\n",
    "##### Index Registration\n",
    "\n",
    "After defining the Index component above, you need to register it into the LazyLLM framework. When registering, we register in the form of key-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10560b",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from lazyllm.tools.rag import Document\n",
    "\n",
    "# Register by instantiating the object\n",
    "document = Document(dataset_path=\"dataset_path\", manager=False)\n",
    "# Here document.get_store() will be passed transparently to TrieTreeIndex. If the Index you define does not receive any data during initialization, there is no need to pass it in.\n",
    "document.register_index(\"trie_tree\", TrieTreeIndex, document.get_store())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b286bb54",
   "metadata": {},
   "source": [
    "In the above code, a Document object is first instantiated, and then the Index component is registered through the Document instantiation object. When registering, you need to specify a string type key, that is, the name of the registered index, here is \"trie_tree\", then the defined class TrieTreeIndex, and finally specify the required initialization parameters, which are the parameters that the __init__ method of the custom Index class needs to receive. You can pass in any constant as an initialization parameter, or you can pass in document.get_store() or document.get_embed(), which are parameters provided by lazy initialization in Document, representing the storage instance and embedded model respectively. If there are no parameters, no values ​​need to be passed in.\n",
    "\n",
    "##### Index usage\n",
    "\n",
    "After defining and registering Index, it can be used in the retriever Retriever. We prepare a huge vocabulary list, which contains 370,000 English strings (mostly English words).\n",
    "\n",
    "![image.png](12_images/img5.png)\n",
    "\n",
    "We also define a linear search index LinearIndex in the same way as above , note that this is only to show the performance difference between linear search and dictionary tree. In actual production, due to performance requirements, it is not recommended to use linear index for search. For the complete code, see [link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/retriever_with_custom_index_trie_tree.py#L82):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSearchIndex(IndexBase):\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "\n",
    "    @override\n",
    "    def update(self, nodes: List['DocNode']) -> None:\n",
    "        if not nodes or nodes[0]._group != 'linear':\n",
    "            return\n",
    "        for n in nodes:\n",
    "            self.nodes.append(n)\n",
    "\n",
    "    @override\n",
    "    def remove(self, uids: List[str], group_name: Optional[str] = None) -> None:\n",
    "        if group_name != 'linear':\n",
    "            return\n",
    "        for uid in uids:\n",
    "            for i, n in enumerate(self.nodes):\n",
    "                if n._uid == uid:\n",
    "                    del self.nodes[i]\n",
    "                    break\n",
    "\n",
    "    @override\n",
    "    def query(self, query: str, **kwargs) -> List[str]:\n",
    "        # Assuming each word appears only once, only exact matching is performed\n",
    "        res = []\n",
    "        for n in self.nodes:\n",
    "            if n.text == query:\n",
    "                res.append(n)\n",
    "                break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d69d66",
   "metadata": {},
   "source": [
    "Define the index performance test program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd86c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trie_index(queries: list[str]):\n",
    "    dataset_path = os.path.join(DOC_PATH, \"index\")\n",
    "    docs1 = lazyllm.Document(dataset_path=dataset_path, embed=embedding_model)\n",
    "    #Create node group\n",
    "    docs1.create_node_group(name='linear', transform=(lambda d: d.split('\\r\\n')))\n",
    "    docs1.create_node_group(name='tree', transform=(lambda d: d.split('\\r\\n')))\n",
    "    # Register index\n",
    "    docs1.register_index(\"trie_tree\", TrieTreeIndex, docs1.get_store())\n",
    "    docs1.register_index(\"linear_search\", LinearSearchIndex)\n",
    "    #Create a retriever and specify the corresponding index type\n",
    "    retriever1 = lazyllm.Retriever(docs1, group_name=\"linear\", index=\"linear_search\", topk=1)\n",
    "    retriever2 = lazyllm.Retriever(docs1, group_name=\"tree\", index=\"trie_tree\", topk=1)\n",
    "    # Retrieval initialization\n",
    "    retriever1.start()\n",
    "    retriever2.start()\n",
    "    for query in queries:\n",
    "        st = time.time()\n",
    "        res = retriever1(query)\n",
    "        et = time.time()\n",
    "        LOG.info(f\"query: {query}, linear time: {et - st}, linear res: {res[0].text}\")\n",
    "    \n",
    "        st = time.time()\n",
    "        res = retriever2(query)\n",
    "        et = time.time()\n",
    "        LOG.info(f\"query: {query}, trie time: {et - st}, trie res: {res[0].text}\")\n",
    "\n",
    "test_trie_index([\"a\", \"lazyllm\", \"zwitterionic\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bb950",
   "metadata": {},
   "source": [
    "Code running results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2f64f",
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "source": [
    "```bash\n",
    "query: a, linear time: 9.894371032714844e-05, linear res: a\n",
    "query: a, trie time: 7.939338684082031e-05, trie res: a\n",
    "query: lazyllm, linear time: 0.04016876220703125, linear res: lazyllm\n",
    "query: lazyllm, trie time: 0.00011754035949707031, trie res: lazyllm\n",
    "query: zwitterionic, linear time: 0.08771300315856934, linear res: zwitterionic\n",
    "query: zwitterionic, trie time: 0.00011014938354492188, trie res: zwitterionic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25edb76",
   "metadata": {},
   "source": [
    "It can be seen that both dictionary tree and linear search successfully search the corresponding nodes, but there is a clear difference in the time spent:\n",
    "\n",
    "Linear search takes longer and longer as the search word goes further back in the vocabulary, while dictionary tree retrieval time is relatively stable. During the retrieval process of the last word in the 37w vocabulary list, the linear search took 0.0877s, and the dictionary tree only took 0.0001s, saving 99.89% of the time! Therefore, a properly designed index can greatly improve the retrieval efficiency of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cef052",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_4.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890174ff",
   "metadata": {},
   "source": [
    "\n",
    "#### Combine the scenario and write the HNSW index yourself\n",
    "\n",
    "In RAG scenarios, high-performance ANN vector indexes are often established to speed up vector retrieval. According to the introduction in the previous class, everyone has a certain understanding of HNSW. Next, we will also build an HNSW index according to the above idea of ​​building an index, and compare the speed of vector retrieval in the case of LazyLLM's built-in cosine similarity + default index.\n",
    "\n",
    "The implementation of HNSW index relies on the hnswlib library. One of the underlying ANN libraries of the popular vector database Milvus is hnswlib, which provides HNSW retrieval for milvus. The specific code is as follows ([GitHub link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/retriever_with_custom_index_hnsw.py#L19)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNSWIndex(IndexBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed: Dict[str, Callable],\n",
    "            store: LazyLLMStoreBase,\n",
    "            max_elements: int = 10000, # Define the maximum index capacity\n",
    "            ef_construction: int = 200, # Balance index construction speed and search accuracy. The larger the index, the higher the accuracy but the slower the construction speed.\n",
    "            M: int = 16, # represents the number of edges of each vector during table construction. The higher M, the larger the memory usage, the higher the accuracy, and the slower the construction speed.\n",
    "            dim: int = 1024, # vector dimension\n",
    "            **kwargs\n",
    "        ):\n",
    "        self.embed = embed\n",
    "        self.store = store\n",
    "        # Since the vector id in hnswlib cannot be str, a label is created to maintain the vector number.\n",
    "        # Create a dictionary to maintain the relationship between node id and vector id in hnsw\n",
    "        self.uid_to_label = {}\n",
    "        self.label_to_uid = {}\n",
    "        self.next_label = 0\n",
    "        #Initialize hnsw_index\n",
    "        self._index_init(max_elements, ef_construction, M, dim)\n",
    "\n",
    "    def _index_init(self, max_elements, ef_construction, M, dim):\n",
    "        # hnswlib supports multiple distance algorithms: l2, IP inner product and cosine\n",
    "        self.index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        self.index.init_index(\n",
    "            max_elements=max_elements,\n",
    "            ef_construction=ef_construction,\n",
    "            M=M,\n",
    "            allow_replace_deleted=True\n",
    "        )\n",
    "        self.index.set_ef(100) # Set the maximum number of neighbors when searching. Higher values ​​will lead to better accuracy, but the search speed will be slower.\n",
    "        self.index.set_num_threads(8) # Set the number of threads used during batch searches and index building processes\n",
    "\n",
    "    @override\n",
    "    def update(self, nodes: List['DocNode']):\n",
    "        if not nodes or nodes[0]._group != 'block':\n",
    "            return\n",
    "        # Node vectorization, here only the default embedding model is shown\n",
    "        parallel_do_embedding(self.embed, [], nodes=nodes, group_embed_keys={'block': [\"__default__\"]})\n",
    "        vecs = [] # vector list\n",
    "        labels = [] # vector id list\n",
    "        for node in nodes:\n",
    "            uid = str(node._uid)\n",
    "            # Record the relationship between uid and label. If it is a new uid, write next_label\n",
    "            if uid in self.uid_to_label:\n",
    "                label = self.uid_to_label[uid]\n",
    "            else:\n",
    "                label = self.next_label\n",
    "                self.uid_to_label[uid] = label\n",
    "                self.next_label += 1\n",
    "            # Get the default embedding result\n",
    "            vec = node.embedding['__default__'] \n",
    "            vecs.append(vec)\n",
    "            labels.append(label)\n",
    "            self.label_to_uid[label] = uid\n",
    "        \n",
    "        # Create hnsw index based on vector\n",
    "        data = np.vstack(vecs)\n",
    "        ids  = np.array(labels, dtype=int)\n",
    "        self.index.add_items(data, ids)\n",
    "\n",
    "    @override\n",
    "    def remove(self, uids, group_name=None):\n",
    "        \"\"\"\n",
    "        Mark and delete a batch of vectors corresponding to uid and clean up the mapping\n",
    "        \"\"\"\n",
    "        if group_name != 'block':\n",
    "            return\n",
    "        for uid in map(str, uids):\n",
    "            if uid not in self.uid_to_label:\n",
    "                continue\n",
    "            label = self.uid_to_label.pop(uid)\n",
    "            self.index.mark_deleted(label)\n",
    "            self.label_to_uid.pop(label, None)\n",
    "\n",
    "    @override\n",
    "    def query(\n",
    "        self,\n",
    "        query: str,\n",
    "        topk: int,\n",
    "        embed_keys: List[str],\n",
    "        **kwargs,\n",
    "    ) -> List['DocNode']:\n",
    "        # Generate query vector\n",
    "        parts = [self.embed[k](query) for k in embed_keys]\n",
    "        qvec = np.concatenate(parts)\n",
    "        #Call hnsw knn_query method for vector retrieval\n",
    "        labels, distances = self.index.knn_query(qvec, k=topk, num_threads=self.index.num_threads)\n",
    "        results = []\n",
    "        #Get retrieval topk\n",
    "        for lab, dist in zip(labels[0], distances[0]):\n",
    "            uid = self.label_to_uid.get(lab)\n",
    "            results.append(uid)\n",
    "            if len(results) >= topk:\n",
    "                break\n",
    "        # Get the node corresponding to uid from store\n",
    "        return self.store.get_nodes(group_name='block', uids=results) if len(results) > 0 else []\n",
    "\n",
    "\n",
    "def test_hnsw_index():\n",
    "    dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "    docs1 = lazyllm.Document(dataset_path=dataset_path, embed=embedding_model)\n",
    "    docs1.create_node_group(name='block', transform=(lambda d: d.split('\\n')))\n",
    "    docs1.register_index(\"hnsw\", HNSWIndex, docs1.get_embed(), docs1.get_store())\n",
    "    retriever1 = lazyllm.Retriever(docs1, group_name=\"block\", similarity=\"cosine\", topk=3)\n",
    "    retriever2 = lazyllm.Retriever(docs1, group_name=\"block\", index=\"hnsw\", topk=3)\n",
    "    retriever1.start()\n",
    "    retriever2.start()\n",
    "    q = \"Securities regulation?\"\n",
    "    st = time.time()\n",
    "    res = retriever1(q)\n",
    "    et = time.time()\n",
    "    context_str = \"\\n---------\\n\".join([r.text for r in res])\n",
    "    LOG.info(f\"query: {q}, default time: {et - st}, default res:\\n {context_str}\")\n",
    "\n",
    "    st = time.time()\n",
    "    res = retriever2(q)\n",
    "    et = time.time()\n",
    "    context_str = \"\\n---------\\n\".join([r.text for r in res])\n",
    "    LOG.info(f\"query: {q}, HNSW time: {et - st}, HNSW res: \\n{context_str}\")\n",
    "\n",
    "test_hnsw_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d49d4",
   "metadata": {},
   "source": [
    "The running results are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f338651",
   "metadata": {
    "attributes": {
     "classes": [
      "markdown"
     ],
     "id": ""
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "query: Securities regulation? , default time: 0.5182957649230957, default res:\n",
    "Protective measures from financial regulatory authorities\n",
    "---------\n",
    "In order to standardize the capital account management of securities companies, prevent business risks, and protect the legitimate rights and interests of customers, in accordance with the \"China\n",
    "---------\n",
    "In order to implement the \"Measures for the Administration of Securities Brokerage Business\" and guide securities companies to carry out securities brokerage business in a standardized manner, we will coordinate\n",
    "query: Securities regulation? , HNSW time: 0.021764516830444336, HNSW res:\n",
    "Protective measures from financial regulatory authorities\n",
    "---------\n",
    "In order to implement the \"Measures for the Administration of Securities Brokerage Business\" and guide securities companies to carry out securities brokerage business in a standardized manner, we will coordinate\n",
    "---------\n",
    "In order to standardize the capital account management of securities companies, prevent business risks, and protect the legitimate rights and interests of customers, in accordance with the \"China\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2bf8b",
   "metadata": {},
   "source": [
    "It can be seen that using the default cosine similarity and default index to perform vector retrieval takes 0.518s, while using the self-implemented HNSW index to perform vector retrieval takes 0.022s, saving about 95% of the time. This conclusion is consistent with the practice of scalar indexing, that is, by establishing an efficient index and \"exchanging space for time\", the amount of calculation can be greatly reduced and the retrieval efficiency can be improved.\n",
    "\n",
    "### Practice 3: Start from scratch and use the high-performance vector database Milvus\n",
    "\n",
    "Vector indexes are very powerful, but the cost of manually building a high-performance index from 0 is relatively high. If you are careful, you must have discovered that in the previous storage configuration, the configuration parameters of the Milvus vector database already include index-related configuration, that’s right! Vector databases have already indexed these high-performance vectors. In the actual research and development process, we only need to use these high-performance vector databases to achieve high-performance vector retrieval!\n",
    "\n",
    "#### Introduction to the use of native Milvus\n",
    "\n",
    "Let’s first take a look at the native basic usage of the high-performance vector database Milvus, the protagonist of this practice, including the installation of milvus and the use of basic functions (initialization, data injection, data retrieval).\n",
    "\n",
    "##### Install\n",
    "\n",
    "In this practice, we use Milvus Lite, a python library included in `pymilvus` that can be embedded into applications. Milvus also supports deployment on Docker and Kubernetes, making it suitable for production use cases. Run the following command to complete the installation of pymilvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956cf94",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U pymilvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea7f1b",
   "metadata": {},
   "source": [
    "##### use\n",
    "\n",
    "First we use the following code to create a milvus client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a54e0",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "# Create a milvus client and pass in the storage path of the local database. If the path does not exist, create it.\n",
    "client = MilvusClient(\"dbs/origin_milvus.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1d9e",
   "metadata": {},
   "source": [
    "In Milvus, we need a Collections to store vectors and their associated metadata. It is equivalent to a table in a traditional SQL database. When creating Collections, you can define Schema and index parameters to configure vector specifications, such as dimension (dimension), index type (index_params), distance metric (metric_type), etc. [Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter12/use_original_milvus.py#L8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef83f34",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# In the initialization phase, if a collection with the same name already exists, delete it first\n",
    "if client.has_collection(collection_name=\"demo_collection\"):\n",
    "    client.drop_collection(collection_name=\"demo_collection\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07bb73",
   "metadata": {},
   "source": [
    "> Note: create_collection supports more input parameters, such as primary key column name primary_field_name, vector column name vector_field_name, vector index parameter index_params, similarity parameter metric_type, etc. Only basic usage is introduced here, and the rest use default parameters.\n",
    "\n",
    "Then prepare the document slices, vectorize them, create a data set according to the format accepted by milvus, and inject it into the database. Here we add an additional field subject (subject) to serve as the metadata of the data. [Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter12/use_original_milvus.py#L16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "vecs =[embedding_model(doc) for doc in docs] \n",
    "data = [\n",
    "    {\"id\": i, \"vector\": vecs[i], \"text\": docs[i], \"subject\": \"history\"}\n",
    "    for i in range(len(vecs))\n",
    "]\n",
    "# Data injection\n",
    "res = client.insert(collection_name=\"demo_collection\", data=data)\n",
    "print(f\"Inserted data into client:\\n {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c688ac",
   "metadata": {},
   "source": [
    "When performing query retrieval, you also need to vectorize the query first, then call the search method in the client, specify the corresponding collection and retrieval quantity, and support specifying which fields to output (output_fields). [Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter12/use_original_milvus.py#L30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b390443",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Alan Turing?\"\n",
    "# query vectorization\n",
    "q_vec = embedding_model(query)\n",
    "# Retrieve\n",
    "res = client.search(\n",
    "collection_name=\"demo_collection\", # Specify collection\n",
    "    data=[q_vec],\n",
    "limit=2, # Specify the number of searches (top_k)\n",
    "output_fields=[\"text\", \"subject\"], #Specify the fields included in the search results\n",
    ")\n",
    "print(f\"Query: {query} \\nSearch result:\\n {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b1dbf",
   "metadata": {},
   "source": [
    "Milvus not only supports simple and efficient vector retrieval, but also supports the use of metadata as filter fields to achieve consistent retrieval of scalar index + vector index. In the code below, we also create three paragraphs and set the subject to different subjects during data injection. In the input parameters of the search method, we enter the filter parameter to set the data we want to filter. [Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter12/use_original_milvus.py#L43)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6320c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Machine learning has been used for drug design.\",\n",
    "    \"Computational synthesis with AI algorithms predicts molecular properties.\",\n",
    "    \"DDR1 is involved in cancers and fibrosis.\",\n",
    "]\n",
    "vecs =[embedding_model(doc) for doc in docs]\n",
    "data = [\n",
    "    {\"id\": 3 + i, \"vector\": vecs[i], \"text\": docs[i], \"subject\": \"biology\"}\n",
    "    for i in range(len(vecs))\n",
    "]\n",
    "client.insert(collection_name=\"demo_collection\", data=data)\n",
    "res = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    data=[embedding_model(\"tell me AI related information\")],\n",
    "filter=\"subject == 'biology'\", # Fields expected to be filtered\n",
    "    limit=2,\n",
    "    output_fields=[\"text\", \"subject\"],\n",
    ")\n",
    "print(f\"Filter Query: {query} \\nSearch result:\\n {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878ffef",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_5.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c84469",
   "metadata": {},
   "source": [
    "\n",
    "We combine the above usage methods and other components of LazyLLM to rebuild a simple RAG system ([Code GitHub link](https://github.com/LazyAGI/Tutorial/blob/282ffb74e3fe7c5c28df4ad498ed972973dfbc62/rag/codes/chapter12/rag_with_original_milvus.py#L1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lazyllm\n",
    "from lazyllm.tools.rag import SimpleDirectoryReader, SentenceSplitter\n",
    "from lazyllm.tools.rag.doc_node import MetadataMode\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "from online_models import embedding_model, llm, rerank_model\n",
    "\n",
    "DOC_PATH = os.path.abspath(\"docs\")\n",
    "\n",
    "###################### File storage ##################################\n",
    "# milvus client initialization\n",
    "client = MilvusClient(\"dbs/rag_milvus.db\")\n",
    "if client.has_collection(collection_name=\"demo_collection\"):\n",
    "    client.drop_collection(collection_name=\"demo_collection\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=1024,\n",
    ")\n",
    "\n",
    "#Load local files and parse -- block slicing -- vectorization -- storage\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "docs = SimpleDirectoryReader(input_dir=dataset_path)()\n",
    "block_transform = SentenceSplitter(chunk_size=256, chunk_overlap=25)\n",
    "\n",
    "nodes = []\n",
    "for doc in docs:\n",
    "    nodes.extend(block_transform(doc))\n",
    "\n",
    "# Slice vectorization\n",
    "vecs = [embedding_model(node.get_text(MetadataMode.EMBED)) for node in nodes]\n",
    "data = [\n",
    "    {\"id\": i, \"vector\": vecs[i], \"text\": nodes[i].text}\n",
    "    for i in range(len(vecs))\n",
    "]\n",
    "#Data injection\n",
    "res = client.insert(collection_name=\"demo_collection\", data=data)\n",
    "\n",
    "###################### Search Q&A ##################################\n",
    "query = \"What are the basic specifications for securities management?\"\n",
    "\n",
    "# Retrieval and generation\n",
    "prompt = 'You are a friendly AI Q&A assistant who needs to provide answers based on the given context and question. \\\n",
    "Answer the questions based on the following information:\\\n",
    "        {context_str} \\n '\n",
    "llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "q_vec = embedding_model(query)\n",
    "res = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    data=[q_vec],\n",
    "    limit=12,\n",
    "    output_fields=[\"text\"],\n",
    ")\n",
    "\n",
    "# Extract search results\n",
    "contexts = [res[0][i].get('entity').get(\"text\", \"\") for i in range(len(res[0]))]\n",
    "\n",
    "# rearrange\n",
    "rerank_res = rerank_model(text=query, documents=contexts, top_n=3)\n",
    "rerank_contexts = [contexts[res[0]] for res in rerank_res]\n",
    "\n",
    "context_str = \"\\n-------------------\\n\".join(rerank_contexts)\n",
    "\n",
    "res = llm({\"query\": query, \"context_str\": context_str})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f6e48",
   "metadata": {},
   "source": [
    "In the above code, we first define the milvus client using the native usage method, and then use LazyLLM's own document parser to extract the document content, convert it into slices, vectorize it and inject it into the vector database. Then perform query retrieval, extract the retrieval content and perform reordering, and then give the large model to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef85ba",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_6.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39732d",
   "metadata": {},
   "source": [
    "#### Using Milvus in LazyLLM\n",
    "\n",
    "From the above practical content, it is not difficult to find that milvus can achieve persistent storage and strong retrieval performance. However, the way to use native milvus is indeed a bit complicated, and users need to spend a lot of money to learn how to use a vector database from scratch. This is somewhat \"troublesome\" for developers. Simply LazyLLM has perfectly adapted Milvus. As mentioned previously, you can easily connect milvus to the RAG system with simple storage and indexing backend configuration. The specific configuration method is to add an additional field 'indices' to the above-mentioned storage backend configuration store_conf. indices is a python dictionary, key is the name of the index type, and value is the parameter required by the index type. The specific configuration is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add0919",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "store_conf = {\n",
    "    'type': 'map',\n",
    "    'indices': {\n",
    "        'smart_embedding_index': {\n",
    "'backend': 'milvus', # Set the index to use the Milvus backend\n",
    "              'kwargs': {\n",
    "'uri': 'dbs/test.db', # Milvus data storage address\n",
    "                  'index_kwargs': {        \n",
    "'index_type': 'HNSW', # Set the index type\n",
    "'metric_type': 'COSINE', # Set the metric type\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08a9c7",
   "metadata": {},
   "source": [
    "- key: 'smart_embedding_index' (use the built-in vector index of the milvus vector database to achieve efficient vector retrieval)\n",
    "- values: \n",
    "    - 'backend': Index backend, currently only supports passing in 'milvus', the Milvus vector database is used.\n",
    "    - 'kwargs': Additional configuration items when selecting the corresponding index backend. When selecting milvus, the configuration items are consistent with the storage backend using milvus, which are uri and index_kwargs.\n",
    "\n",
    "Milvus supports a variety of indexing methods. Generally speaking, floating-point indexes (that is, indexing dense vectors) have many usage scenarios and are suitable for semantic retrieval of large-scale data. Binary indexes and sparse indexes are suitable for smaller data sets and can ensure a higher recall rate. Each indexing method has a corresponding recommended measurement method. You can choose according to the following table according to your needs:\n",
    "\n",
    "| Index mode | Concrete type | Measurement type |\n",
    "|------------|--------------------------------------------------------------------------------------------------------------------------------------------|------------------------------|\n",
    "| Floating point index | - Plane<br>- IVF_FLAT<br>- IVF_SQ8<br>- IVF_PQ<br>- GPU_IVF_FLAT<br>- GPU_IVF_PQ<br>- HNSW<br>- DISKANN | - Euclidean distance (L2)<br>- Inner product (IP)<br>- Cosine similarity (COSINE) |\n",
    "| Binary Index | - BIN_FLAT<br>- BIN_IVF_FLAT | - Jaccard (JACCARD)<br> - HAMMING (HAMMING) |\n",
    "| Sparse Index | - SPARSE_INVERTED_INDEX<br>- SPARSE_WAND | - Inner Product (IP) |\n",
    "\n",
    "Based on the above milvus configuration, we use LazyLLM's built-in SmartEmbeddingIndex to implement Milvus index access, and also compare the retrieval speed with Retriever using the default index (for the complete code, see [Github link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter12/rag_with_lazyllm_milvus.py#L1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import lazyllm\n",
    "from lazyllm import LOG\n",
    "\n",
    "from online_models import embedding_model\n",
    "\n",
    "DOC_PATH = os.path.abspath(\"docs\")\n",
    "\n",
    "milvus_store_conf = {\n",
    "    'type': 'map',\n",
    "    'indices': {\n",
    "        'smart_embedding_index': {\n",
    "            'backend': 'milvus',\n",
    "            'kwargs': {\n",
    "                'uri': \"dbs/test_map_milvus.db\",\n",
    "                'index_kwargs': {\n",
    "                    'index_type': 'HNSW',\n",
    "                    'metric_type': 'COSINE',\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "\n",
    "docs = lazyllm.Document(dataset_path=dataset_path, embed=embedding_model, store_conf=milvus_store_conf)\n",
    "\n",
    "# Use default index\n",
    "retriever1 = lazyllm.Retriever(docs, group_name=\"MediumChunk\", topk=6, similarity=\"cosine\")\n",
    "# Use milvus built-in HNSW index\n",
    "retriever2 = lazyllm.Retriever(docs, group_name=\"MediumChunk\", topk=6, index='smart_embedding_index')\n",
    "retriever1.start()\n",
    "retriever2.start()\n",
    "\n",
    "q = \"Securities regulation?\"\n",
    "st = time.time()\n",
    "res = retriever1(q)\n",
    "et = time.time()\n",
    "LOG.info(f\"query: {q}, default time: {et - st}\")\n",
    "\n",
    "st = time.time()\n",
    "res = retriever2(q)\n",
    "et = time.time()\n",
    "LOG.info(f\"query: {q}, milvus time: {et - st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808450cf",
   "metadata": {},
   "source": [
    "In the configuration of store_conf, the storage backend and index backend are two independent parts. However, if the storage type is milvus, since the index configuration information is already included in kwargs, there is no need to configure additional indices to implement built-in vector retrieval.\n",
    "\n",
    "As you can see, when using Milvus, it is the same as using Milvus as a storage backend. Pass the index configuration indices into store_conf, and then specify index='smart_embedding_index' in Retriever. Compared with the native usage, using store_conf makes the use of vector database easier and the code more concise and readable. The results of running the program are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0a91e",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "source": [
    "```bash\n",
    "query: Securities regulation? , default time: 0.16447973251342773\n",
    "query: Securities regulation? , milvus time: 0.02178812026977539\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc36d9",
   "metadata": {},
   "source": [
    "Using LazyLLM's built-in Milvus to perform retrieval saves approximately 86.75% of retrieval time compared to the default vector index. Using Milvus as the index backend, the code to build a RAG system based on the multi-embedding recall strategy is as follows. The code below shows how to use dense and sparse retrieval at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b474b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import bind, deploy\n",
    "\n",
    "milvus_store_conf = {\n",
    "    'type': 'milvus',\n",
    "    'kwargs': {\n",
    "        'uri': \"milvus.db\",\n",
    "        'index_kwargs': [\n",
    "            {\n",
    "                'embed_key': 'bge_m3_dense',\n",
    "                'index_type': 'IVF_FLAT',\n",
    "                'metric_type': 'COSINE',\n",
    "            },\n",
    "            {\n",
    "                'embed_key': 'bge_m3_sparse',\n",
    "                'index_type': 'SPARSE_INVERTED_INDEX',\n",
    "                'metric_type': 'IP',\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "bge_m3_dense = lazyllm.TrainableModule('bge-m3')\n",
    "bge_m3_sparse = lazyllm.TrainableModule('bge-m3').deploy_method((deploy.AutoDeploy, {'embed_type': 'sparse'}))\n",
    "embeds = {'bge_m3_dense': bge_m3_dense, 'bge_m3_sparse': bge_m3_sparse}\n",
    "document = lazyllm.Document(dataset_path='/path/to/your/document',\n",
    "           embed=embeds,\n",
    "           store_conf=milvus_store_conf)\n",
    "\n",
    "document.create_node_group(name=\"block\", transform=lambda s: s.split(\"\\n\") if s else '')\n",
    "bge_rerank = lazyllm.TrainableModule(\"bge-reranker-large\")\n",
    "\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    with lazyllm.parallel().sum as ppl.prl:\n",
    "        ppl.prl.retriever1 = lazyllm.Retriever(doc=document,\n",
    "                         group_name=\"block\",\n",
    "                         embed_keys=['bge_m3_dense'],\n",
    "                         topk=3)\n",
    "        ppl.prl.retriever = lazyllm.Retriever(doc=document,\n",
    "                         group_name=\"block\",\n",
    "                         embed_keys=['bge_m3_sparse'],\n",
    "                         topk=3)\n",
    "    ppl.reranker = lazyllm.Reranker(name='ModuleReranker',model=bge_rerank, topk=3) | bind(query=ppl.input)\n",
    "    ppl.formatter = (\n",
    "      lambda nodes, query: dict(\n",
    "          context_str=[node.get_content() for node in nodes],\n",
    "          query=query)\n",
    "    ) | bind(query=ppl.input)\n",
    "    \n",
    "    ppl.llm = lazyllm.OnlineChatModule().prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "webpage = lazyllm.WebModule(ppl, port=23492).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e63b7",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_7.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a230b",
   "metadata": {},
   "source": [
    "Combined with the storage configuration introduced previously, we will fully optimize the storage and indexing of the RAG system in Lecture 7 to comprehensively improve the execution efficiency of the system. The code is as follows ([GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter12/rag_use_milvus_store.py)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lazyllm\n",
    "from lazyllm import Reranker\n",
    "\n",
    "from online_models import embedding_model, llm, rerank_model # Use online models\n",
    "\n",
    "DOC_PATH = os.path.abspath(\"docs\")\n",
    "\n",
    "#milvus storage and index configuration\n",
    "milvus_store_conf = {\n",
    "    'type': 'milvus',\n",
    "    'kwargs': {\n",
    "        'uri': \"dbs/milvus1.db\",\n",
    "        'index_kwargs': {\n",
    "        'index_type': 'HNSW',\n",
    "        'metric_type': 'COSINE',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "\n",
    "# Define Document and pass in configuration to use milvus storage and retrieval\n",
    "docs = lazyllm.Document(dataset_path=dataset_path, embed=embedding_model, store_conf=milvus_store_conf)\n",
    "#Create sentence node group\n",
    "docs.create_node_group(name='sentence', parent=\"MediumChunk\", transform=(lambda d: d.split('。')))\n",
    "#Create MediumChunk, sentence node group multi-channel recall,\n",
    "retriever1 = lazyllm.Retriever(docs, group_name=\"MediumChunk\", topk=6, index='smart_embedding_index')\n",
    "retriever2 = lazyllm.Retriever(docs, group_name=\"sentence\", target=\"MediumChunk\", topk=6, index='smart_embedding_index')\n",
    "retriever1.start()\n",
    "retriever2.start()\n",
    "#Create reranker\n",
    "reranker = Reranker('ModuleReranker', model=rerank_model, topk=3)\n",
    "\n",
    "prompt = 'You are a friendly AI Q&A assistant who needs to provide answers based on the given context and question. \\\n",
    "Answer the questions based on the following information:\\\n",
    "        {context_str} \\n '\n",
    "llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "query = \"What are the guidelines for securities management?\"\n",
    "\n",
    "nodes1 = retriever1(query=query)\n",
    "nodes2 = retriever2(query=query)\n",
    "rerank_nodes = reranker(nodes1 + nodes2, query)\n",
    "context_str = \"\\n======\\n\".join([node.get_content() for node in rerank_nodes])\n",
    "print(f\"context_str: \\n{context_str}\")\n",
    "res = llm({\"query\": query, \"context_str\": context_str})\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e84e5fae",
   "metadata": {},
   "source": [
    "Milvus not only supports local db file links, but also supports access to remote server endpoints.\n",
    "\n",
    "![image.png](12_images/img6.png)\n",
    "\n",
    "For example, we expect to deploy the Milvus Standalone service on a Linux system.\n",
    "\n",
    "- Install Docker and check the hardware and software requirements according to Milvus official documentation.\n",
    "- Install Milvus in Docker, installation script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204db093",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Download the installation script\n",
    "curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh\n",
    "\n",
    "# Start Docker container\n",
    "bash standalone_embed.sh start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727030f6",
   "metadata": {},
   "source": [
    "- Installation results:\n",
    "    - The docker container named Milvus is started on port 19530. To change the default configuration of Milvus, add the settings to the user.yaml file in the current folder and restart the service.\n",
    "    - Milvus data volumes are mapped to volumes/milvus in the current folder.\n",
    "\n",
    "### Practice 4 Engineering optimization skills, caching and parallel retrieval\n",
    "\n",
    "In the previous Chapter 11, we mentioned some engineering optimization techniques, such as caching mechanism, multi-task parallelism, etc. Next we will practice respectively.\n",
    "\n",
    "#### Use K-V cache\n",
    "\n",
    "This practice uses a simple k-v dict to simulate the caching mechanism. We build a process from RAG system startup to retrieval, and set up the kv dictionary to save the retrieved query and node set, and test the retrieval time of the system with or without the caching mechanism:\n",
    "\n",
    "> Tips: `index='smart_embedding_index'` has been abandoned in the latest lazyllm and needs to be changed to `similarity='cosine'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_store_conf = {\n",
    "    'type': 'map',\n",
    "    'indices': {\n",
    "        'smart_embedding_index': {\n",
    "        'backend': 'milvus',\n",
    "        'kwargs': {\n",
    "            'uri': \"dbs/test_cache.db\",\n",
    "            'index_kwargs': {\n",
    "                'index_type': 'HNSW',\n",
    "                'metric_type': 'COSINE',\n",
    "            }\n",
    "        },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "\n",
    "docs = lazyllm.Document(\n",
    "    dataset_path=dataset_path,\n",
    "    embed=embedding_model,\n",
    "    store_conf=milvus_store_conf\n",
    ")\n",
    "docs.create_node_group(name='sentence', parent=\"MediumChunk\", transform=(lambda d: d.split('。')))\n",
    "\n",
    "retriever1 = lazyllm.Retriever(docs, group_name=\"MediumChunk\", topk=6, similarity='cosine')\n",
    "retriever2 = lazyllm.Retriever(docs, group_name=\"sentence\", target=\"MediumChunk\", topk=6, similarity='cosine')\n",
    "retriever1.start()\n",
    "retriever2.start()\n",
    "\n",
    "reranker = Reranker('ModuleReranker', model=rerank_model, topk=3)\n",
    "\n",
    "# Set fixed query\n",
    "query = \"What are the basic specifications for securities management?\"\n",
    "\n",
    "# Run the retrieval process without caching mechanism 5 times and record the time\n",
    "time_no_cache = []\n",
    "for i in range(5):\n",
    "    st = time.time()\n",
    "    nodes1 = retriever1(query=query)\n",
    "    nodes2 = retriever2(query=query)\n",
    "    rerank_nodes = reranker(nodes1 + nodes2, query)\n",
    "    et = time.time()\n",
    "    t = et - st\n",
    "    time_no_cache.append(t)\n",
    "print(f\"No cache The {i+1}th query takes time: {t}s\")\n",
    "\n",
    "# Define dict[list] to store the retrieved query and node collection to implement a simple caching mechanism\n",
    "kv_cache = defaultdict(list)\n",
    "for i in range(5):\n",
    "    st = time.time()\n",
    "#If the query is not in the cache, execute the normal retrieval process. If the query hits the cache, directly obtain the node set in the cache.\n",
    "    if query not in kv_cache:\n",
    "        nodes1 = retriever1(query=query)\n",
    "        nodes2 = retriever2(query=query)\n",
    "        rerank_nodes = reranker(nodes1 + nodes2, query)\n",
    "# After the retrieval is completed, cache the query and retrieval nodes\n",
    "        kv_cache[query] = rerank_nodes\n",
    "    else:\n",
    "        rerank_nodes = kv_cache[query]\n",
    "    et = time.time()\n",
    "    t = et - st\n",
    "    time_no_cache.append(t)\n",
    "print(f\"KV cache {i+1} query time: {t}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81143309",
   "metadata": {},
   "source": [
    "The above code mainly implements the startup and retrieval links in the RAG system. For the retrieval link, the kv cache mechanism is implemented in the form of a dictionary. When the retrieval starts, it will first check whether the currently queried node is already in the cache. If it exists, it is a cache hit, and the query results in the cache can be directly fetched. Otherwise, the normal retrieval process will be carried out, and the retrieval results will be stored in the cache at the end. After running the program, you get the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b5c45",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "No cache The first query took: 1.3868563175201416s\n",
    "No cache The second query took: 1.277320146560669s\n",
    "No cache The third query took: 1.2744269371032715s\n",
    "No cache The fourth query took: 1.3921117782592773s\n",
    "No cache The fifth query took: 1.3207831382751465s\n",
    "The first query of KV cache took 1.4092140197753906s\n",
    "The second query of KV cache took: 2.384185791015625e-07s\n",
    "The third query of KV cache took 1.430511474609375e-06s\n",
    "The fourth query of KV cache took 2.384185791015625e-07s\n",
    "The fifth query of KV cache took 2.384185791015625e-07s\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3401cd75",
   "metadata": {},
   "source": [
    "It can be seen that when the system does not cache query results, the time of each query is 1. A few seconds, but when using cache, except for the first normal retrieval, all other retrievals are completed in an instant. Therefore, a reasonable design of the cache mechanism can further improve the system retrieval performance based on efficient vector indexing.\n",
    "\n",
    ">Note: The purpose of the practice is mainly to demonstrate the improvement of retrieval performance by the caching mechanism. In the actual production process, an in-memory data management system (such as Redis) is usually used to implement related functions. At the same time, many factors need to be considered to establish a complete caching mechanism. like:\n",
    ">\n",
    ">- Document updates are cache cleansing\n",
    ">- Specific definition of high fever query\n",
    ">- Similar query hits\n",
    ">- ...\n",
    "\n",
    "#### Parallel execution, multi-channel recall efficiency improvement\n",
    "\n",
    "You can find that in the previous multi-channel recall scenario, the retrievers performed retrievals sequentially, which is actually very inefficient. Considering that there is no coupling between retrievers, we can use parallel in the Flow component of LazyLLM to implement parallel multi-way recall of retrievers.\n",
    "\n",
    "![image.png](12_images/img7.png)\n",
    "\n",
    "The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169668f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_store_conf = {\n",
    "    'type': 'milvus',\n",
    "    'kwargs': {\n",
    "        'uri': \"dbs/test_parallel.db\",\n",
    "        'index_kwargs': {\n",
    "        'index_type': 'HNSW',\n",
    "        'metric_type': 'COSINE',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "\n",
    "docs1 = lazyllm.Document(\n",
    "    dataset_path=dataset_path,\n",
    "    embed=embedding_model,\n",
    "    store_conf=milvus_store_conf\n",
    ")\n",
    "docs1.create_node_group(name='sentence', parent=\"MediumChunk\", transform=(lambda d: d.split('。')))\n",
    "retriever1 = lazyllm.Retriever(docs1, group_name=\"MediumChunk\", topk=3)\n",
    "retriever2 = lazyllm.Retriever(docs1, group_name=\"sentence\", target=\"MediumChunk\", topk=3)\n",
    "\n",
    "retriever1.start()\n",
    "retriever2.start()\n",
    "\n",
    "with lazyllm.parallel().sum as prl:\n",
    "    prl.r1 = retriever1\n",
    "    prl.r2 = retriever2\n",
    "\n",
    "query = \"What are the basic specifications for securities management?\"\n",
    "\n",
    "st = time.time()\n",
    "retriever1(query=query)\n",
    "retriever2(query=query)\n",
    "et1 = time.time()\n",
    "prl(query)\n",
    "et2 = time.time()\n",
    "print(f\"Sequential retrieval time: {et1-st}s\")\n",
    "print(f\"Parallel retrieval time: {et2-et1}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49234f",
   "metadata": {},
   "source": [
    "Execute the above code to get the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa66bb",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "source": [
    "```bash\n",
    "Sequential retrieval time: 0.0436248779296875s\n",
    "Parallel retrieval time: 0.025980472564697266s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae9666",
   "metadata": {},
   "source": [
    "It can be seen that using parallel retrieval can effectively save retrieval time and improve retrieval efficiency. Finally, we combine all the above optimization strategies to achieve a high-performance RAG system with optimized efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_store_conf = {\n",
    "    'type': 'milvus',\n",
    "    'kwargs': {\n",
    "        'uri': \"dbs/test_rag.db\",\n",
    "        'index_kwargs': {\n",
    "        'index_type': 'HNSW',\n",
    "        'metric_type': 'COSINE',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "dataset_path = os.path.join(DOC_PATH, \"test\")\n",
    "#Define kv cache\n",
    "kv_cache = defaultdict(list)\n",
    "\n",
    "docs1 = lazyllm.Document(dataset_path=dataset_path, embed=embedding_model, store_conf=milvus_store_conf)\n",
    "docs1.create_node_group(name='sentence', parent=\"MediumChunk\", transform=(lambda d: d.split('。')))\n",
    "\n",
    "prompt = 'You are a friendly AI Q&A assistant who needs to provide answers based on the given context and question. \\\n",
    "Answer the questions based on the following information: \\\n",
    "    {context_str} \\n '\n",
    "\n",
    "with lazyllm.pipeline() as recall:\n",
    "# Parallel multi-way recall\n",
    "    with lazyllm.parallel().sum as recall.prl:\n",
    "        recall.prl.r1 = lazyllm.Retriever(docs1, group_name=\"MediumChunk\", topk=6)\n",
    "        recall.prl.r2 = lazyllm.Retriever(docs1, group_name=\"sentence\", target=\"MediumChunk\", topk=6)\n",
    "    recall.reranker = lazyllm.Reranker(name='ModuleReranker',model=rerank_model, topk=3) | lazyllm.bind(query=recall.input)\n",
    "    recall.cache_save = (lambda nodes, query: (kv_cache.update({query: nodes}) or nodes)) | lazyllm.bind(query=recall.input)\n",
    "    \n",
    "with lazyllm.pipeline() as ppl:\n",
    "# Cache check\n",
    "    ppl.cache_check = lazyllm.ifs(\n",
    "        cond=(lambda query: query in kv_cache),\n",
    "        tpath=(lambda query: kv_cache[query]),\n",
    "        fpath=recall\n",
    "    )\n",
    "    ppl.formatter = (\n",
    "        lambda nodes, query: dict(\n",
    "            context_str=\"\\n\".join(node.get_content() for node in nodes),\n",
    "            query=query)\n",
    "    ) | lazyllm.bind(query=ppl.input)\n",
    "    ppl.llm = llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "w = lazyllm.WebModule(ppl, port=23492, stream=True).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c23837",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_8.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20adc45",
   "metadata": {},
   "source": [
    "## 3. Use the vLLM framework to start the quantitative model service\n",
    "\n",
    "vLLM is an efficient inference framework optimized specifically for large language model (LLM) inference. It aims to significantly improve the inference speed of large models and reduce graphics memory usage. It adopts a series of optimization technologies, such as efficient continuous batch processing (PagedAttention) and dynamic KV cache management, making it more advantageous than traditional methods in GPU inference scenarios.\n",
    "\n",
    "### 1. Start the local service directly\n",
    "\n",
    "LazyLLM natively supports three large model inference acceleration frameworks: LightLLM, LMDeploy and vLLM, and the Infinity embedded model acceleration framework. Users only need to specify the corresponding framework through TrainableModule.deploy_method when using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import TrainableModule, deploy\n",
    "\n",
    "llm = TrainableModule('model_name').deploy_method(deploy.vllm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7a4c0",
   "metadata": {},
   "source": [
    "vLLM supports inference services for most large models. You can first confirm whether the model you want to use is in the [LazyLLM supported model list](https://docs.lazyllm.ai/zh-cn/stable/Home/model_list/). The ones followed by AWQ, 4bit, etc. in this list are quantitative models. You can select a quantitative model according to your needs and start the dialogue service, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import TrainableModule, deploy\n",
    "\n",
    "llm = TrainableModule('Qwen2-72B-Instruct-AWQ').deploy_method(deploy.vllm).start()\n",
    "print(llm(\"hello, who are you?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3478d51",
   "metadata": {},
   "source": [
    "Let's take Qwen2-72B-Instruct and its AWQ quantized version as an example to compare the size and running speed of the two. Running the Qwen2-72B-Instruct model of BF16 or FP16 requires at least 144GB of video memory (such as 2xA100-80G or 5xV100-32G); running the Int4 model requires at least 48GB of video memory (such as 1xA100-80G or 2xV100-32G), which is 33% of the original (data from Qwen Official [[ModelScope Community](https://modelscope.cn/models/qwen/Qwen-72B/)]).\n",
    "\n",
    "![image.png](12_images/img8.png)\n",
    "\n",
    "Let's briefly compare the running speed of the two (in a more rigorous case, you can compare the differences in multiple executions. For the complete code, see [GitHub link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter12/use_quantized_llm.py)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lazyllm import TrainableModule, deploy, launchers\n",
    "\n",
    "start_time = time.time()\n",
    "llm = TrainableModule('Qwen2-72B-Instruct').deploy_method(\n",
    "        deploy.Vllm).start()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Original model loading time:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "llm_awq = TrainableModule('Qwen2-72B-Instruct-AWQ').deploy_method(deploy.Vllm).start()\n",
    "end_time = time.time()\n",
    "print(\"AWQ quantitative model loading time:\", end_time-start_time)\n",
    "\n",
    "query = \"Generate a 1,000-word report on artificial intelligence development\"\n",
    "\n",
    "start_time = time.time()\n",
    "llm(query)\n",
    "end_time = time.time()\n",
    "print(\"Original model takes time:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "llm_awq(query)\n",
    "end_time = time.time()\n",
    "print(\"AWQ quantization model takes time:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678077b",
   "metadata": {},
   "source": [
    "Also on 4 A800 cards the loading and execution speeds are as follows:\n",
    "\n",
    "> Original model loading time: 129.6051540374756\n",
    ">\n",
    "> Original model takes: 13.104065895080566\n",
    ">\n",
    "> AWQ quantification model loading time: 86.4980857372284\n",
    ">\n",
    "> AWQ quantization model takes time: 8.81701111793518\n",
    "\n",
    "The token/s information in the log information output by LazyLLM are:\n",
    "\n",
    "> INFO 03-12 19:52:50 metrics.py:341] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 30.8 tokens/s\n",
    ">\n",
    "> INFO 03-12 20:00:03 metrics.py:341] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 41.2 tokens/s\n",
    "\n",
    "The loading and execution time of the AWQ model on a single card is: 137 seconds and 23 seconds. In other words, when there are 4 cards, the quantized model can handle 1.3-1.4 times more requests. When the user cannot meet the resources to execute the original model, the quantitative model can be used to obtain a generation effect that is not much different from the original model effect ([Official data](https://qwen.readthedocs.io/en/latest/benchmark/quantization_benchmark.html) claims that the evaluation average difference of the quantitative model in MMLU, CEval, and IEval is only 0.9 points).\n",
    "\n",
    "### 2. Access vLLM API through OnlineChatModule\n",
    "\n",
    "The API interface published by vLLM is consistent with OpenAI, so we can implement interface access through lazyllm.OnlineChatModule. The advantage of this method is that the large model service is decoupled from the RAG system. There is no need to restart the model service when restarting the system, saving time on model loading.\n",
    "\n",
    "For the Qwen2-72B-Instruct model, the direct startup method of using vLLM is to enter on the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0a587",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "source": [
    "```bash\n",
    "vllm serve /path/to/your/model \\\n",
    "        --quantization awg_marlin \\\n",
    "        --served-model-name qwen2 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 25120 \\ \n",
    "        --trust-remote-code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245e108",
   "metadata": {},
   "source": [
    "If your vLLM version is earlier than 0.5.3, pass the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3ab33",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "source": [
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server\n",
    "        --model /path/to/your/model\n",
    "        --served-model-name qwen2 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 25120\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b143b81",
   "metadata": {},
   "source": [
    "Then we access in LazyLLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d3320",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from lazyllm import OnlineChatModule\n",
    "\n",
    "# Need to throw the environment variable LAZYLLM_OPENAI_API_KEY to specify the service interface as openai mode\n",
    "# export LAZYLLM_OPENAI_API_KEY=sk...\n",
    "llm = OnlineChatModule(model=\"qwen2\", base_url=\"http://127.0.0.1:25120/v1/\")\n",
    "print(llm('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b325847",
   "metadata": {},
   "source": [
    "output\n",
    "\n",
    "> Hello! How can I assist you today?\n",
    "\n",
    "This method can effectively reduce the time consumption caused by frequent system restarts during debugging. It can also reduce the system startup time together with persistent storage when the official system is restarted. If you want to use your own token for verification, you can pass in `--api-key <your-api-key>` when starting the vLLM service, and then complete the verification by setting the environment variable `LAZYLLM_OPENAI_API_KEY` when accessing.\n",
    "\n",
    "In addition, if you have a customized service interface, you can also implement your own online chat interface by inheriting OnlineChatModuleBase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm.module import OnlineChatModuleBase\n",
    "from lazyllm.module.onlineChatModule.fileHandler import FileHandlerBase\n",
    "class CustomChatModule(OnlineChatModuleBase):\n",
    "    def __init__(self,\n",
    "        base_url: str = \"<new platform base url>\",\n",
    "        model: str = \"<new platform model name>\",\n",
    "        system_prompt: str = \"<new platform system prompt>\",\n",
    "        stream: bool = True,\n",
    "        return_trace: bool = False):\n",
    "        super().__init__(model_type=\"new_class_name\",\n",
    "            api_key=lazyllm.config['new_platform_api_key'],\n",
    "            base_url=base_url,\n",
    "            system_prompt=system_prompt,\n",
    "            stream=stream,\n",
    "            return_trace=return_trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d54569c2",
   "metadata": {},
   "source": [
    "### RAG system based on vector database and vLLM service\n",
    "\n",
    "Applying all the strategies in this tutorial (persistent storage and high-speed vector search based on vector databases, vLLM to start the quantitative reasoning model) to the multi-channel recall RAG introduced in Advanced 1, the following code is obtained. Under the same conditions, the startup time is significantly reduced compared to the previous version, and the response time is shortened to a certain extent.\n",
    "\n",
    "![image.png](12_images/img9.png)\n",
    "\n",
    "We mentioned the use of QA pairs in Advanced 1 and recommended debugging after learning the vector database. This is because QA pair extraction takes a long time and a lot of tokens. If it is executed every time the system is restarted, it will waste a lot of time and tokens. However, if a persistent storage strategy is used, the waiting time each time the system is restarted can be adjusted. Combined with the document management interface, you can also adjust and delete the QA pairs extracted from large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import bind\n",
    "\n",
    "# Use Milvus storage backend\n",
    "chroma_store_conf = {\n",
    "  'type': 'chroma', \n",
    "  'kwargs': {\n",
    "    'dir': 'qa_pair_chromadb',\n",
    "   },\n",
    "  'indices': {\n",
    "    'smart_embedding_index': {\n",
    "      'backend': 'milvus',\n",
    "      'kwargs': {\n",
    "        'uri': \"qa_pair/test.db\",\n",
    "        'index_kwargs': {\n",
    "          'index_type': 'HNSW',\n",
    "          'metric_type': 'COSINE',\n",
    "        }\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "rewriter_prompt = \"You are a query rewriting assistant, responsible for template switching for user queries.\\\n",
    "Note that you don't need to answer, just rewrite the question to make it easier to search\\\n",
    "Here is a simple example:\\\n",
    "Input: What is RAG? \\\n",
    "Output: What is the definition of RAG? \"\n",
    "rag_prompt = 'You will play the role of an AI Q&A assistant and complete a dialogue task.'\\\n",
    "    ' In this task, you need to provide your answer based on the given context and question.'\n",
    "\n",
    "# Define embedding model and reordering model\n",
    "# online_embedding = lazyllm.OnlineEmbeddingModule()\n",
    "embedding_model = lazyllm.TrainableModule(\"bge-large-zh-v1.5\").start()\n",
    "\n",
    "# If you want to use the online rearrangement model\n",
    "# Currently LazyLLM only supports qwen and glm online rearrangement models, please specify the corresponding API key.\n",
    "# online_rerank = lazyllm.OnlineEmbeddingModule(type=\"rerank\")\n",
    "# Local reordering model\n",
    "offline_rerank = lazyllm.TrainableModule('bge-reranker-large').start()\n",
    "\n",
    "llm = lazyllm.OnlineChatModule(base_url=\"http://127.0.0.1:36858/v1\")\n",
    "\n",
    "qa_parser = lazyllm.LLMParser(llm, language=\"zh\", task_type=\"qa\")\n",
    "\n",
    "docs = lazyllm.Document(\"/path/to/your/document\", embed=embedding_model, store_conf=chroma_store_conf)\n",
    "docs.create_node_group(name='block', transform=(lambda d: d.split('\\n')))\n",
    "docs.create_node_group(name='qapair', transform=qa_parser)\n",
    "\n",
    "def retrieve_and_rerank():\n",
    "    with lazyllm.pipeline() as ppl:\n",
    "        with lazyllm.parallel().sum as ppl.prl:\n",
    "# CoarseChunk is the default chunk name provided by LazyLLM with a size of 1024\n",
    "            ppl.prl.retriever1 = lazyllm.Retriever(doc=docs, group_name=\"CoarseChunk\", index=\"smart_embedding_index\", topk=3)\n",
    "            ppl.prl.retriever2 = lazyllm.Retriever(doc=docs, group_name=\"block\", similarity=\"bm25_chinese\", topk=3)\n",
    "        ppl.reranker = lazyllm.Reranker(\"ModuleReranker\",\n",
    "                                         model=offline_rerank,\n",
    "                                         topk=3) | bind(query=ppl.input)\n",
    "    return ppl\n",
    "\n",
    "with lazyllm.pipeline() as ppl:\n",
    "# llm.share means reusing a large model. If this is set to promptrag_prompt, rewrite_prompt will be overwritten.\n",
    "    ppl.query_rewriter = llm.share(lazyllm.ChatPrompter(instruction=rewriter_prompt))\n",
    "    with lazyllm.parallel().sum as ppl.prl:\n",
    "        ppl.prl.retrieve_rerank = retrieve_and_rerank()\n",
    "        ppl.prl.qa_retrieve = lazyllm.Retriever(doc=docs, group_name=\"qapair\", index=\"smart_embedding_index\", topk=3)      \n",
    "    ppl.formatter = (\n",
    "          lambda nodes, query: dict(\n",
    "              context_str='\\n'.join([node.get_content() for node in nodes]),\n",
    "              query=query)\n",
    "        ) | bind(query=ppl.input)\n",
    "    ppl.llm = llm.share(lazyllm.ChatPrompter(instruction=rag_prompt, extra_keys=['context_str']))\n",
    "\n",
    "lazyllm.WebModule(ppl, port=23491, stream=True).start().wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f645c",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./12_videos/12_9.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ad95f",
   "metadata": {},
   "source": [
    "\n",
    "Based on the content of this tutorial on how LazyLLM uses vector databases to achieve persistent storage and high-speed vector retrieval, it can effectively reduce the calculation time of the RAG system in the restart and execution phases, and can also save a lot of token fees from the perspective of certain node groups. Secondly, through the flexible use of the vLLM framework, faster inference speed can be obtained. By using quantified models, hardware requirements can be reduced and hardware costs can be saved while retaining similar generation effects. It can also reduce model loading time when starting the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

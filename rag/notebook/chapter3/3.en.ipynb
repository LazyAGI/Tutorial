{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00628591",
   "metadata": {},
   "source": [
    "# Chapter 3: How Large Models Work â€” Understanding Call Logic and Prompt Engineering with LazyLLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a42b185",
   "metadata": {},
   "source": [
    ">In the previous chapter we introduced the fundamentals of RAG and built a minimal pipeline with LazyLLM. This chapter explores LazyLLM's unique features so we can rebuild a data-flow-centric RAG application with a cleaner structure.\n",
    ">\n",
    ">You'll learn how to compose data flows, how to call both hosted and local LLMs, how to design prompts for them, and how to reuse one local checkpoint to create different personas. We'll finish by refactoring the previous RAG demo with LazyLLM's data flows.\n",
    ">\n",
    ">Let's get started!\n",
    "\n",
    "Welcome to LazyLLM!\n",
    "\n",
    "LazyLLM is a development framework for multi-agent LLM applications. It helps you build complex AI systems with very little effort and iterate on them continuously. The recommended workflow is:\n",
    "\n",
    "**Prototype -> Data Analysis -> Iterative Optimization**\n",
    "\n",
    "Start by validating the prototype, analyze bad cases with representative data, and then refine algorithms or fine-tune the models to improve quality. LazyLLM is designed to free researchers and engineers from repetitive engineering so they can focus on algorithms and data.\n",
    "\n",
    "The framework offers a consistent experience for different technology stacks within the same moduleâ€”unified invocation, service, and deployment.\n",
    "\n",
    "![](3_images/img1.png)\n",
    "\n",
    "For newcomers, LazyLLM dramatically simplifies LLM application development. You don't need to learn how to host different APIs, pick fine-tuning frameworks, split models, or build a web UI. With prebuilt components and lightweight composition you can ship production-grade tools quickly.\n",
    "\n",
    "For experts, LazyLLM is extremely flexible. Its modular design lets you integrate proprietary algorithms, industry tooling, and the latest research ideas to build powerful applications tailored to any scenario.\n",
    "\n",
    "This tutorial focuses on the core usage patterns. After going through it you will understand the main design concepts and be able to build a role-playing chatbot from scratch.\n",
    "\n",
    "For more tutorials and API references, see the [LazyLLM documentation](https://docs.lazyllm.ai/zh-cn/stable/) or check out our Bilibili series:\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/1intro_lazy.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "## Environment Setup âœˆ\n",
    "\n",
    "If Python is already installed on your machine, run the following commands to install the base `lazyllm` package and its dependencies. For additional installation options, refer to Chapter 2.\n",
    "\n",
    "**Install with pip**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a6eb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip install lazyllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ba333",
   "metadata": {},
   "source": [
    "**Install from source**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1aedac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "git clone https://github.com/LazyAGI/LazyLLM.git\n",
    "cd LazyLLM\n",
    "pip3 install -r requirements.txt\n",
    "export PYTHONPATH=$PWD:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73f09c9b",
   "metadata": {},
   "source": [
    "## Call LLMs ðŸ¤–\n",
    "\n",
    "LazyLLM exposes hosted and local models through a unified interface, so each LLM can act like a black box. Focus on the inputs, outputs, and parameter choices instead of the subtle differences between providers.\n",
    "\n",
    "![image.png](3_images/img2.png)\n",
    "### 1. Call hosted LLMs ðŸŒ\n",
    "\n",
    "`OnlineChatModule` is the entry point for hosted APIs such as OpenAI, SenseNova, and any other provider. Pass the provider/model configuration you need and LazyLLM will normalize the rest.\n",
    "\n",
    "â—â—â— Before you start debugging, export your API key as an environment variable. LazyLLM raises an error when it cannot find the required variables. See Chapter 2 for a full walkthrough. To access SenseNova, configure the variables below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ddc03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "export LAZYLLM_SENSENOVA_API_KEY=\"...\"\n",
    "export LAZYLLM_SENSENOVA_SECRET_KEY=\"...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f2e67",
   "metadata": {},
   "source": [
    "If you only set one platform key, you can instantiate `OnlineChatModule` without specifying `source`. When multiple keys are available, LazyLLM tries `openai > sensenova > glm > kimi > qwen` in that order. To explicitly select a provider or model, pass `source` and `model` to `OnlineChatModule`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = lazyllm.OnlineChatModule(source=\"sensenova\")\n",
    "\n",
    "# Specify an explicit model\n",
    "sensechat = lazyllm.OnlineChatModule(\"sensenova\", model=\"SenseChat-5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b302410",
   "metadata": {},
   "source": [
    "The snippet below calls an online model (the SenseNova keys are already configured in the environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "online_model = lazyllm.OnlineChatModule()\n",
    "print(online_model(\"Hello there, who are you?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b32569",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a1chat_online1.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "If the default model is unavailable, set the `model` argument explicitly, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db76ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "online_model = lazyllm.OnlineChatModule(source=\"sensenova\", model=\"DeepSeek-V3\")\n",
    "print(online_model(\"Hello, are you DeepSeek?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e16177",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a1chat_online2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 2. Call local LLMs ðŸ’»\n",
    "\n",
    "`TrainableModule` exposes every local resource (LLMs, embedding models, multimodal checkpoints, etc.) and can train, fine-tune, or serve them. Local inference follows two steps:\n",
    "\n",
    "1. Start the model service with an inference runtime.\n",
    "2. Call the service from Python.\n",
    "\n",
    "LazyLLM offers a truly lazy experience: pass the absolute model path to `TrainableModule` and call `start()`, or define `LAZYLLM_MODEL_PATH` to point to the directory that holds your checkpoints and only pass the model name. If the checkpoint is missing, LazyLLM downloads it automatically to `~/.lazyllm/model` (override via `LAZYLLM_MODEL_CACHE_DIR`).\n",
    "\n",
    "LazyLLM supports multiple inference frameworks such as LightLLM and vLLM. If you do not specify one, LazyLLM picks the best option based on model size and the provided data. To lock the backend, configure it as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920e3e",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/chat_local.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import deploy\n",
    "\n",
    "llm = lazyllm.TrainableModule('internlm2-chat-7b').\\\n",
    "        deploy_method((deploy.Vllm, {\n",
    "            'port': 8081,\n",
    "            'host': '0.0.0.0',\n",
    "        })).start()\n",
    "res = llm('hi')\n",
    "print(\"LLM output:\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513afb21",
   "metadata": {},
   "source": [
    "This example configures the inference backend through `deploy_method`:\n",
    "\n",
    "* `deploy.Vllm` pins vLLM as the runtime.\n",
    "* `host` and `port` set the address where the service is exposed.\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a1chat_local.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 3. Using Prompts ðŸ’­\n",
    "\n",
    "A prompt is the text or instruction provided to an NLP or AI system. It is the primary way users interact with a model. A prompt is not only the userâ€™s inputâ€”it often defines the task itself. With well-designed prompts, we can guide a model to produce responses in a specific style or direction.\n",
    "\n",
    "Prompts supply essential contextual information. In a dialogue system, for example, the model generates responses based on predefined system instructions together with user input. Different prompts lead to different outputs, which means prompt design directly affects the quality, accuracy, and relevance of the generated content. When using large models in a question-answering system, we can use prompts to specify the modelâ€™s role, tone, and style of response.\n",
    "\n",
    "#### Basic prompts\n",
    "\n",
    "LazyLLM provides prompt templates that you can configure when initializing a model. After defining the prompt, you simply pass user input at inference time. The example below defines two online models: llm1, which uses the default settings, and llm2, which uses a custom prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f0e53",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/prompt_with_llm_base.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3278ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "llm1 = lazyllm.OnlineChatModule()\n",
    "llm2 = lazyllm.OnlineChatModule().prompt(\"You are a kitten. After every answer, add 'Meow meow meow'.\")\n",
    "\n",
    "print('Default output:   ', llm1('Hello'))\n",
    "print('Custom prompt:    ', llm2('Hello'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74538da3",
   "metadata": {},
   "source": [
    "Console output:\n",
    "```bash\n",
    "Default prompt:    Hello! How can I assist you today?\n",
    "Custom prompt:     Hello, how can I help you? Meow meow meow\n",
    "```\n",
    "Notice how the customized prompt forces the model to append \"Meow meow meow\" to every reply.\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a2prompt1.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "#### Dynamic prompts\n",
    "\n",
    "In many cases, a prompt needs to include additional information at runtime. To support this, we can add variables as placeholders inside the prompt and replace them with the desired content during inference. This allows us to create dynamic prompts that adapt to different inputs. The following example illustrates how this works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96c9ff",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/prompt_with_llm_placeholder.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "llm2 = lazyllm.OnlineChatModule().prompt(\"Answer the question using the passage: {content}\")\n",
    "\n",
    "passage = ('Sun Wukong is the first disciple of Tang Sanzang in Journey to the West and is also known as '\n",
    "           'Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage '\n",
    "           'Equal to Heaven. Because he once managed the heavenly stables he was given the title Bimawen, '\n",
    "           'and after completing the pilgrimage he was granted the title Victorious Fighting Buddha by the Tathagata.')\n",
    "\n",
    "# Print prompt_content for illustration only\n",
    "prompt_content = llm2._prompt.generate_prompt({'input': 'What other names does Sun Wukong have?', 'content': passage}, return_dict=True)\n",
    "print(prompt_content)\n",
    "\n",
    "# Model inference\n",
    "print(llm2({'input': 'What other names does Sun Wukong have?', 'content': passage}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c3c66",
   "metadata": {},
   "source": [
    "In the following example we add a `{content}` placeholder to the prompt so the passage can be injected at inference time. Call `generate_prompt` to assemble the full payload. `return_dict=True` formats the prompt as the JSON message structure required by hosted models (QWen in this case); without it the output is a single string optimized for local runtimes. We print the prompt only for demonstration purposes.\n",
    "\n",
    "```bash\n",
    "{'messages': [{'role': 'system', 'content': 'You are a large-scale language model from Alibaba Cloud, your name is Tongyi Qianwen, and you are a helpful assistant.\n",
    "Answer the question based on the passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\\n\\n'}, {'role': 'user', 'content': 'What other names does Sun Wukong have?'}]}\n",
    "```\n",
    "\n",
    "Without `return_dict` the prompt looks like this:\n",
    "\n",
    "```bash\n",
    "'You are a large-scale language model from Alibaba Cloud, your name is Tongyi Qianwen, and you are a helpful assistant.Answer the question based on the passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\\n\\n\\n\\n\n",
    "What other names does Sun Wukong have?\\n\\n'\n",
    "```\n",
    "\n",
    "Once we send the prompt to the model we get the following answer:\n",
    "\n",
    "```bash\n",
    "Sun Wukong is also known as:\n",
    "\n",
    "1. Sun Xingzhe\n",
    "2. The Monkey King\n",
    "3. The Handsome Monkey King (self-proclaimed)\n",
    "4. The Great Sage Equal to Heaven (self-proclaimed)\n",
    "5. Bimawen (for managing the heavenly stables)\n",
    "\n",
    "After completing the pilgrimage he was granted the title Victorious Fighting Buddha.\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a2prompt2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "#### Standalone prompts\n",
    "\n",
    "So far we attached prompts directly to an LLM. Sometimes it is more convenient to define a prompt template first and then assign it to different models. LazyLLM ships two prompt helpers: `AlpacaPrompter` and `ChatPrompter` (the previous sections used `ChatPrompter`). They mainly differ in format:\n",
    "\n",
    "`AlpacaPrompter`:\n",
    "\n",
    "```bash\n",
    "{system}\\n{instruction}\\n{tools}\\n{user}### Response:\\n\n",
    "```\n",
    "\n",
    "`ChatPrompter`:\n",
    "\n",
    "```bash\n",
    "{sos}{system}{instruction}{tools}{eos}\\n\\n{history}\\n{soh}\\n{user}{input}\\n{eoh}{soa}\\n\n",
    "```\n",
    "\n",
    "The fields are defined as follows:\n",
    "\n",
    "* `instruction`: The task instruction. This is the main part of the prompt that we configure earlier.\n",
    "* `history`: Conversation history derived from previous user interactions. The format can be `[[a, b], [c, d]]` or `[{\"role\": \"user\", \"content\": \"\"}, {\"role\": \"assistant\", \"content\": \"\"}]`.\n",
    "* `tools`: A list of tools available to the model. Tools can be provided when creating the `prompter` or passed in by the user.If tools are defined during prompter construction, they cannot be overridden at runtime. The expected format is:`[{\"type\": \"function\", \"function\": {\"name\": \"\", \"description\": \"\", \"parameters\": {}, \"required\": []}}]`\n",
    "* `user`: Optional user-level instructions. This is specified through the `instruction` input. If `instruction` is a string, it is treated as a system instruction. If it is a dictionary, its keys must be either `system` or `user`. `system` defines system-level instructions, and `user` defines user-level instructions.\n",
    "\n",
    "The following fields are filled automatically based on the model configuration (users and developers do not need to provide them; LazyLLM handles this internally):\n",
    "\n",
    "* `system`: The system prompt. It is automatically set based on model metadata. If not specified, the default is: `You are an AI-Agent developed by LazyLLM.`\n",
    "* sos: `start of system`, marks the beginning of the system prompt.\n",
    "* eos: `end of system`, marks the end of the system prompt.\n",
    "* soh: `start of human`, marks the beginning of the user input.\n",
    "* eoh: `end of human`, marks the end of the user input.\n",
    "* soa: `start of assistant`, marks the beginning of the model output.\n",
    "* eoa: `end of assistant`, marks the end of the model output.\n",
    "\n",
    "Letâ€™s first look at how these two independent prompts are combined.\n",
    "\n",
    "Suppose the text passage and user question are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "passage = ('Sun Wukong is the first disciple of Tang Sanzang in Journey to the West and is also known as '\n",
    "           'Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage '\n",
    "           'Equal to Heaven. Because he once managed the heavenly stables he was given the title Bimawen, '\n",
    "           'and after completing the pilgrimage he was granted the title Victorious Fighting Buddha by the Tathagata.')\n",
    "query = 'What other names does Sun Wukong have?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52748e",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```bash\n",
    "Independent Prompt (Alpaca):\n",
    "'You are an AI-Agent developed by LazyLLM.\\nBelow is an instruction that describes a task, paired with additional messages that provide context when available. Write a response that appropriately completes the request.\\n\n",
    "### Instruction:\\nSystem instruction\\n\\nUser instruction.\n",
    "### Passage:\\nSun Wukong is one of Tang Sanzang\\''s four disciples in the novel *Journey to the West*, ranked first among them. He is also known as Sun Xingzhe and Monkey King. He proclaimed himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once served in Heaven as the Keeper of the Heavenly Horses, he was also called Bimawen. After completing the pilgrimage, he was granted the title \"Fighting-Victorious Buddha\" by the Tathagata.\n",
    "### Question:\n",
    "What are the names of Sun Wukong?\n",
    "### Response:'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee8279",
   "metadata": {},
   "source": [
    "AlpacaPrompter (Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b362fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter1 = lazyllm.AlpacaPrompter({\n",
    "    'system': 'System instruction',\n",
    "    'user': 'User instruction.\\n### Passage: {content}\\n### Question: {input}\\n'\n",
    "    })\n",
    "content = prompter1.generate_prompt({'input': query, 'content': passage})\n",
    "print(\"\\nStandalone prompt (Alpaca):\\n\", repr(content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af589545",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2e1a8",
   "metadata": {},
   "source": [
    "```bash\n",
    "Standalone prompt (Alpaca):\n",
    " 'You are an AI-Agent developed by LazyLLM.\n",
    "Below is an instruction that describes a task, paired with extra messages such as input that provides further context if possible. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "System instruction\n",
    "\n",
    "User instruction.\n",
    "### Passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\n",
    "### Question: What other names does Sun Wukong have?\n",
    "### Response:\\n'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046fd18",
   "metadata": {},
   "source": [
    "ChatPrompter (standalone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter2 = lazyllm.ChatPrompter({\n",
    "    'system': 'System instruction',\n",
    "    'user': 'User instruction.\\n### Passage: {content}\\n### Question: {input}\\n'\n",
    "    })\n",
    "content = prompter2.generate_prompt({'input': query, 'content': passage})\n",
    "print(\"\\nStandalone prompt (Chat):\\n\", repr(content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b9f69",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2193d",
   "metadata": {},
   "source": [
    "```bash\n",
    "Standalone prompt (Chat):\n",
    " 'You are an AI-Agent developed by LazyLLM.System instruction\\n\\n\\n\\nUser instruction.\n",
    "### Passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\n",
    "### Question: What other names does Sun Wukong have?\\n\\n\\n'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f48855",
   "metadata": {},
   "source": [
    "Now attach the prompts to an actual LLM (InternLM2-Chat-7B in this case).\n",
    "\n",
    "AlpacaPrompter + LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = lazyllm.TrainableModule(\"internlm2-chat-7b\").prompt(prompter1)\n",
    "res = m1._prompt.generate_prompt({'input': query, 'content': passage})\n",
    "print(\"\\nPrompt + LLM (Alpaca):\\n\", repr(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f7122",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f85e5",
   "metadata": {},
   "source": [
    "```bash\n",
    "Prompt + LLM (Alpaca):\n",
    " 'You are an AI assistant whose name is InternLM.\n",
    "- InternLM is a conversational language model that is developed by Shanghai AI Laboratory. It is designed to be helpful, honest, and harmless.\n",
    "- InternLM can understand and communicate fluently in the language chosen by the user such as English and Chinese.\n",
    "Below is an instruction that describes a task, paired with extra messages such as input that provides further context if possible. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "System instruction\n",
    "\n",
    "User instruction.\n",
    "### Passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\n",
    "### Question: What other names does Sun Wukong have?\n",
    "### Response:\\n'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c94a8",
   "metadata": {},
   "source": [
    "ChatPrompter + LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe619",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = lazyllm.TrainableModule(\"internlm2-chat-7b\").prompt(prompter2)\n",
    "res = m2._prompt.generate_prompt({'input': query, 'content': passage})\n",
    "print(\"\\nPrompt + LLM (Chat):\\n\", repr(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9fa07",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93a6fa",
   "metadata": {},
   "source": [
    "```bash\n",
    "Prompt + LLM (Chat):\n",
    " '<|im_start|>system\n",
    "You are an AI assistant whose name is InternLM.\n",
    "- InternLM is a conversational language model that is developed by Shanghai AI Laboratory. It is designed to be helpful, honest, and harmless.\n",
    "- InternLM can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.System instruction<|im_end|>\\n\\n\\n\\n<|im_start|>user\\n\\nUser instruction.\n",
    "### Passage: Sun Wukong is the first disciple of Tang Sanzang in Journey to the West. He is also known as Sun Xingzhe and the Monkey King. He crowned himself the Handsome Monkey King and the Great Sage Equal to Heaven. Because he once managed the heavenly stables he was called Bimawen, and after completing the pilgrimage he earned the title Victorious Fighting Buddha.\n",
    "### Question: What other names does Sun Wukong have?\\n\\n<|im_end|>\\n<|im_start|>assistant\\n\\n'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6f0b3",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a2prompt3.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "Format comparison:\n",
    "\n",
    "|                     | **Alpaca format**   | **Chat format**               |\n",
    "| ---------------------- | ------------------------ | ---------------------------------- |\n",
    "| **Best for**   | Single-turn Q&A / instruction tuning     | Multi-turn conversations and complex tasks               |\n",
    "| **Context** | No conversation memory | Conversation history is preserved         |\n",
    "| **Structure** | Simple                   | Flexible                         |\n",
    "| **Roles**   | Single role                 | Multiple roles (`system`, `user`, `assistant`) |\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th><strong>Hosted format</strong></th>\n",
    "      <th><strong>Local format</strong></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Best for</th>\n",
    "      <td>Calling hosted models</td>\n",
    "      <td>Self-hosted inference</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Structure</th>\n",
    "      <td>JSON</td>\n",
    "      <td>String</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Notes</th>\n",
    "      <td>Includes explicit roles</td>\n",
    "      <td>Contains special markers such as <code>&lt;|im_start|&gt;</code></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "### 4. Reusing a single model multiple times ðŸ§¤\n",
    "\n",
    "In the previous section, we showed how to configure separate prompts for different models. But can we attach different prompts while sharing the same underlying model? In LazyLLM, the answer is yes. This is especially useful for local models, since you do not need to deploy multiple copies of the same model just to support different roles. This can significantly reduce GPU memory usage.\n",
    "\n",
    "**Usage 1**: In the same process, use **share** to let multiple prompts share a single model instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82019677",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/share_llm_with_prompt.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65531b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "prompt1 = \"Role-play as a kitten and append 'Meow meow meow' to every answer.\"\n",
    "prompt2 = \"Role-play as a chick and append 'Cluck cluck' to every answer.\"\n",
    "\n",
    "llm = lazyllm.TrainableModule(\"internlm2-chat-7b\")\n",
    "llm1 = llm.share(prompt=prompt1)\n",
    "llm2 = llm.share(prompt=prompt2)\n",
    "\n",
    "# Deploy the LLM\n",
    "llm.start()\n",
    "\n",
    "# Show:\n",
    "inputs = 'Hello'\n",
    "print('Base LLM:        ', llm(inputs))\n",
    "print('Prompt #1 LLM:   ', llm1(inputs))\n",
    "print('Prompt #2 LLM:   ', llm2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea57d1",
   "metadata": {},
   "source": [
    "In this example we deploy a single InternLM2-Chat-7B checkpoint and call `share` with two different prompts, effectively creating two personas on top of the same model. Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025db18",
   "metadata": {},
   "source": [
    "```bash\n",
    "Base LLM output:      Hello! I'm InternLM and I'm happy to help. What can I do for you?\n",
    "Prompt #1 persona:    Meow meow meow, hello! How can I help?\n",
    "Prompt #2 persona:    Cluck cluck, hello! I'm InternLM, nice to meet you.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e11cc",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a3chat1.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "**Usage 2**: run the inference service in a separate process and share the LLM by configuring the inference backend and the service URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "m = lazyllm.TrainableModule('internlm2-chat-7b').deploy_method(\n",
    "    lazyllm.deploy.lightllm, url='http://10.119.17.169:36846/generate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd97f3",
   "metadata": {},
   "source": [
    "**Tip**: In addition to running inference directly in code as shown in Usage 1, LazyLLM also provides a command-line tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf15aec",
   "metadata": {},
   "source": [
    "```bash\n",
    "lazyllm deploy internlm2-chat-7b\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "787ce5b2",
   "metadata": {},
   "source": [
    "![](3_images/img3.png)\n",
    "\n",
    "### 5. Three-line chatbot ðŸ¤–\n",
    "\n",
    "You only need three lines of LazyLLM code to create a chatbot. `lazyllm.WebModule` wraps any data flow in a web service so you can debug it through a UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "llm = lazyllm.TrainableModule(\"internlm2-chat-7b\").prompt(\"Role-play as a kitten and append 'Meow meow meow' to every answer.\")\n",
    "webpage = lazyllm.WebModule(llm, port=23466, history=[llm], stream=True).start().wait()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b76c7e20",
   "metadata": {},
   "source": [
    "`WebModule` details:\n",
    "\n",
    "* Use `llm` as the chat backend.\n",
    "* `port` sets the port for the chat UI.\n",
    "* `history=[llm]` feeds the model output back as context, giving the bot conversation memory.\n",
    "* `stream=True` enables streaming responses.\n",
    "* `start()` launches the chatbot.\n",
    "* `wait()` keeps the service alive; without it the deployment would stop immediately.\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/a3chat2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "## Data Flow Overview ðŸ”€\n",
    "\n",
    "LazyLLM is built around data flows, so it ships many flow components that can be composed like building blocks. Instead of wiring every connection manually, each data-flow stage receives the output of the previous stage and dispatches it to the next consumer automatically. Available components include Pipeline, Parallel, Switch, If, Loop, Diverter, Warp, Graph, and more.\n",
    "\n",
    "![image.png](3_images/img4.png)\n",
    "\n",
    "This section introduces every data-flow primitive so you can understand them before we refactor the RAG example.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">All data flows support Python's `with` statement, which keeps the definitions clean and mirrors the flow structure via indentation. Each example below shows both the functional and the `with`-style definitions.\n",
    "\n",
    "### 1. Pipeline\n",
    "\n",
    "A pipeline runs sequentially: each stage consumes the previous output and emits the next input. Pipelines accept functions, lambda expressions, or callable objects. The structure looks like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d341a10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "input -> module1 -> ... -> moduleN -> out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0f6d8",
   "metadata": {},
   "source": [
    "The example below runs sequential logic with a pipeline. Functions, lambdas, and callable classes (objects that implement `__call__`) can all participate.\n",
    "\n",
    "Functional style:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f4fb8",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/pipeline.py#L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "f1 = lambda x: x * 2\n",
    "\n",
    "def f2(input):\n",
    "  return input - 1\n",
    "\n",
    "class AddOneFunctor(object):\n",
    "  def __call__(self, x): return x + 1\n",
    "\n",
    "f3 = AddOneFunctor()\n",
    "\n",
    "# Manual execution\n",
    "inp = 2\n",
    "x1 = f1(inp)\n",
    "x2 = f2(x1)\n",
    "x3 = f3(x2)\n",
    "out_normal = AddOneFunctor()(x3)\n",
    "\n",
    "# Use a pipeline\n",
    "ppl = lazyllm.pipeline(f1, f2, f3, AddOneFunctor)\n",
    "out_ppl1 = ppl(inp)\n",
    "\n",
    "print(f\"Input {inp}, manual output:\", out_normal)\n",
    "print(f\"Input {inp}, pipeline output:\", out_ppl1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0708fb",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```bash\n",
    "Input 2 -> manual output:   5\n",
    "Input 2 -> pipeline output: 5\n",
    "```\n",
    "\n",
    "With-style:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df3f47",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/pipeline_with.py#L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "f1 = lambda x: x * 2\n",
    "\n",
    "def f2(input):\n",
    "  return input - 1\n",
    "\n",
    "class AddOneFunctor(object):\n",
    "  def __call__(self, x): return x + 1\n",
    "\n",
    "f3 = AddOneFunctor()\n",
    "\n",
    "# Build the pipeline with a context manager\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    ppl.func1 = f1\n",
    "    ppl.func2 = f2\n",
    "    ppl.func3 = f3\n",
    "    ppl.func4 = AddOneFunctor\n",
    "\n",
    "inp = 2\n",
    "out_ppl1 = ppl(inp)\n",
    "\n",
    "print(f\"Input {inp}, pipeline output:\", out_ppl1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30a181",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```bash\n",
    "Input 2 -> pipeline output: 5\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/1pipeline.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/1pipeline_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 2. Parallel\n",
    "\n",
    "`Parallel` runs multiple pipelines side by side. The structure looks like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cd194",
   "metadata": {},
   "source": [
    "```bash\n",
    "      /> module11 -> ... -> module1N -> out1 \\\n",
    "input ->  module21 -> ... -> module2N -> out2 -> (out1, out2, out3)\n",
    "      \\> module31 -> ... -> module3N -> out3 /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb364dc",
   "metadata": {},
   "source": [
    "`Parallel` can format its output so downstream components can consume it more easily. It currently supports dict, tuple, list, and string outputs. Examples:\n",
    "\n",
    "Functional style:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5907adf",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/parallel.py#L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "prl1 = lazyllm.parallel(test1, test2, test3)\n",
    "prl2 = lazyllm.parallel(path1=test1, path2=test2, path3=test3).asdict\n",
    "prl3 = lazyllm.parallel(test1, test2, test3).astuple\n",
    "prl4 = lazyllm.parallel(test1, test2, test3).aslist\n",
    "prl5 = lazyllm.parallel(test1, test2, test3).join(', ')\n",
    "\n",
    "print(\"Default output: prl1(1) -> \", prl1(1), type(prl1(1)))\n",
    "print(\"Dict output:    prl2(1) -> \", prl2(1), type(prl2(1)))\n",
    "print(\"Tuple output:   prl3(1) -> \", prl3(1), type(prl3(1)))\n",
    "print(\"List output:    prl4(1) -> \", prl4(1), type(prl4(1)))\n",
    "print(\"String output:  prl5(1) -> \", prl5(1), type(prl5(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e0a32",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2c362",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(1) ->  (2, 4, 0.5) <class 'lazyllm.common.common.package'>\n",
    "Dict output:    prl2(1) ->  {'path1': 2, 'path2': 4, 'path3': 0.5} <class 'dict'>\n",
    "Tuple output:   prl3(1) ->  (2, 4, 0.5) <class 'tuple'>\n",
    "List output:    prl4(1) ->  [2, 4, 0.5] <class 'list'>\n",
    "String output:  prl5(1) ->  2, 4, 0.5 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f00ac1",
   "metadata": {},
   "source": [
    "With-style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/parallel_with.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe84931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "with lazyllm.parallel() as prl1:\n",
    "    prl1.func1 = test1\n",
    "    prl1.func2 = test2\n",
    "    prl1.func3 = test3\n",
    "\n",
    "with lazyllm.parallel().asdict as prl2:\n",
    "    prl2.path1 = test1\n",
    "    prl2.path2 = test2\n",
    "    prl2.path3 = test3\n",
    "\n",
    "with lazyllm.parallel().astuple as prl3:\n",
    "    prl3.func1 = test1\n",
    "    prl3.func2 = test2\n",
    "    prl3.func3 = test3\n",
    "\n",
    "with lazyllm.parallel().aslist as prl4:\n",
    "    prl4.func1 = test1\n",
    "    prl4.func2 = test2\n",
    "    prl4.func3 = test3\n",
    "\n",
    "with lazyllm.parallel().join(', ') as prl5:\n",
    "    prl5.func1 = test1\n",
    "    prl5.func2 = test2\n",
    "    prl5.func3 = test3\n",
    "\n",
    "print(\"Default output: prl1(1) -> \", prl1(1), type(prl1(1)))\n",
    "print(\"Dict output:    prl2(1) -> \", prl2(1), type(prl2(1)))\n",
    "print(\"Tuple output:   prl3(1) -> \", prl3(1), type(prl3(1)))\n",
    "print(\"List output:    prl4(1) -> \", prl4(1), type(prl4(1)))\n",
    "print(\"String output:  prl5(1) -> \", prl5(1), type(prl5(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd22a1",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a457ad",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(1) ->  (2, 4, 0.5) <class 'lazyllm.common.common.package'>\n",
    "Dict output:    prl2(1) ->  {'path1': 2, 'path2': 4, 'path3': 0.5} <class 'dict'>\n",
    "Tuple output:   prl3(1) ->  (2, 4, 0.5) <class 'tuple'>\n",
    "List output:    prl4(1) ->  [2, 4, 0.5] <class 'list'>\n",
    "String output:  prl5(1) ->  2, 4, 0.5 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4826c5",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/2parallel.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/2parallel_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 3. Diverter\n",
    "\n",
    "`Diverter` is a specialized parallel tool where each input follows its own branch and the outputs are aggregated at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8587b2",
   "metadata": {},
   "source": [
    "```bash\n",
    "#                 /> in1 -> module11 -> ... -> module1N -> out1 \\\n",
    "# (in1, in2, in3) -> in2 -> module21 -> ... -> module2N -> out2 -> (out1, out2, out3)\n",
    "#                 \\> in3 -> module31 -> ... -> module3N -> out3 /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc529ec8",
   "metadata": {},
   "source": [
    "Use `Diverter` when you need to manage multiple independent processing pipelines inside a single flow. Output formatting is similar to `Parallel`â€”dict, tuple, list, and string outputs are available.\n",
    "\n",
    "Functional style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/diverter.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "prl1 = lazyllm.diverter(test1, test2, test3)\n",
    "prl2 = lazyllm.diverter(path1=test1, path2=test2, path3=test3).asdict\n",
    "prl3 = lazyllm.diverter(test1, test2, test3).astuple\n",
    "prl4 = lazyllm.diverter(test1, test2, test3).aslist\n",
    "prl5 = lazyllm.diverter(test1, test2, test3).join(', ')\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "print(\"Default output: prl1(inputs) -> \", prl1(inputs), type(prl1(inputs)))\n",
    "print(\"Dict output:    prl2(inputs) -> \", prl2(inputs), type(prl2(inputs)))\n",
    "print(\"Tuple output:   prl3(inputs) -> \", prl3(inputs), type(prl3(inputs)))\n",
    "print(\"List output:    prl4(inputs) -> \", prl4(inputs), type(prl4(inputs)))\n",
    "print(\"String output:  prl5(inputs) -> \", prl5(inputs), type(prl5(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc84b1",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfec130",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(inputs) ->  (2, 8, 1.5) <class 'lazyllm.common.common.package'>\n",
    "Dict output:    prl2(inputs) ->  {'path1': 2, 'path2': 8, 'path3': 1.5} <class 'dict'>\n",
    "Tuple output:   prl3(inputs) ->  (2, 8, 1.5) <class 'tuple'>\n",
    "List output:    prl4(inputs) ->  [2, 8, 1.5] <class 'list'>\n",
    "String output:  prl5(inputs) ->  2, 8, 1.5 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634bd84",
   "metadata": {},
   "source": [
    "With-style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/diverter_with.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "with lazyllm.diverter() as prl1:\n",
    "    prl1.func1 = test1\n",
    "    prl1.func2 = test2\n",
    "    prl1.func3 = test3\n",
    "\n",
    "with lazyllm.diverter().asdict as prl2:\n",
    "    prl2.func1 = test1\n",
    "    prl2.func2 = test2\n",
    "    prl2.func3 = test3\n",
    "\n",
    "with lazyllm.diverter().astuple as prl3:\n",
    "    prl3.func1 = test1\n",
    "    prl3.func2 = test2\n",
    "    prl3.func3 = test3\n",
    "\n",
    "with lazyllm.diverter().aslist as prl4:\n",
    "    prl4.func1 = test1\n",
    "    prl4.func2 = test2\n",
    "    prl4.func3 = test3\n",
    "\n",
    "with lazyllm.diverter().join(', ') as prl5:\n",
    "    prl5.func1 = test1\n",
    "    prl5.func2 = test2\n",
    "    prl5.func3 = test3\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "print(\"Default output: prl1(inputs) -> \", prl1(inputs), type(prl1(inputs)))\n",
    "print(\"Dict output:    prl2(inputs) -> \", prl2(inputs), type(prl2(inputs)))\n",
    "print(\"Tuple output:   prl3(inputs) -> \", prl3(inputs), type(prl3(inputs)))\n",
    "print(\"List output:    prl4(inputs) -> \", prl4(inputs), type(prl4(inputs)))\n",
    "print(\"String output:  prl5(inputs) -> \", prl5(inputs), type(prl5(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850e7be",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846ae15",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(inputs) ->  (2, 8, 1.5) <class 'lazyllm.common.common.package'>\n",
    "Dict output:    prl2(inputs) ->  {'func1': 2, 'func2': 8, 'func3': 1.5} <class 'dict'>\n",
    "Tuple output:   prl3(inputs) ->  (2, 8, 1.5) <class 'tuple'>\n",
    "List output:    prl4(inputs) ->  [2, 8, 1.5] <class 'list'>\n",
    "String output:  prl5(inputs) ->  2, 8, 1.5 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335fe34",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/3diverter.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/3diverter_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 4. Warp\n",
    "\n",
    "`Warp` applies the same processing module to multiple inputs in parallel. It \"warps\" a single module over many inputs so each element is processed independently, boosting throughput.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac959e",
   "metadata": {},
   "source": [
    "```bash\n",
    "#                 /> in1 \\                            /> out1 \\\n",
    "# (in1, in2, in3) -> in2 -> module1 -> ... -> moduleN -> out2 -> (out1, out2, out3)\n",
    "#                 \\> in3 /                            \\> out3 /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce767f3",
   "metadata": {},
   "source": [
    ">**Notes**\n",
    ">\n",
    ">* Do not use Warp for asynchronous tasks such as training or deployment.\n",
    ">* Warp does not support dict outputs.\n",
    "\n",
    "Like `Parallel`, Warp can format its output as tuples, lists, or strings (no dict support yet).\n",
    "\n",
    "Functional style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/warp.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ab802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "prl1 = lazyllm.warp(test1, test2, test3)\n",
    "# prl2 = lazyllm.warp(path1=test1, path2=test2, path3=test3).asdict # Not Implemented\n",
    "prl3 = lazyllm.warp(test1, test2, test3).astuple\n",
    "prl4 = lazyllm.warp(test1, test2, test3).aslist\n",
    "prl5 = lazyllm.warp(test1, test2, test3).join(', ')\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "print(\"Default output: prl1(inputs) -> \", prl1(inputs), type(prl1(inputs)))\n",
    "print(\"Tuple output:   prl3(inputs) -> \", prl3(inputs), type(prl3(inputs)))\n",
    "print(\"List output:    prl4(inputs) -> \", prl4(inputs), type(prl4(inputs)))\n",
    "print(\"String output:  prl5(inputs) -> \", prl5(inputs), type(prl5(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28223aec",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792bd79",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(inputs) ->  (4.0, 6.0, 8.0) <class 'lazyllm.common.common.package'>\n",
    "Tuple output:   prl3(inputs) ->  (4.0, 6.0, 8.0) <class 'tuple'>\n",
    "List output:    prl4(inputs) ->  [4.0, 6.0, 8.0] <class 'list'>\n",
    "String output:  prl5(inputs) ->  4.0, 6.0, 8.0 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9a65c",
   "metadata": {},
   "source": [
    "With-style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/warp_with.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "test1 = lambda a: a + 1\n",
    "test2 = lambda a: a * 4\n",
    "test3 = lambda a: a / 2\n",
    "\n",
    "with lazyllm.warp() as prl1:\n",
    "    prl1.func1 = test1\n",
    "    prl1.func2 = test2\n",
    "    prl1.func3 = test3\n",
    "\n",
    "with lazyllm.warp().astuple as prl3:\n",
    "    prl3.func1 = test1\n",
    "    prl3.func2 = test2\n",
    "    prl3.func3 = test3\n",
    "\n",
    "with lazyllm.warp().aslist as prl4:\n",
    "    prl4.func1 = test1\n",
    "    prl4.func2 = test2\n",
    "    prl4.func3 = test3\n",
    "\n",
    "with lazyllm.warp().join(', ') as prl5:\n",
    "    prl5.func1 = test1\n",
    "    prl5.func2 = test2\n",
    "    prl5.func3 = test3\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "print(\"Default output: prl1(inputs) -> \", prl1(inputs), type(prl1(inputs)))\n",
    "print(\"Tuple output:   prl3(inputs) -> \", prl3(inputs), type(prl3(inputs)))\n",
    "print(\"List output:    prl4(inputs) -> \", prl4(inputs), type(prl4(inputs)))\n",
    "print(\"String output:  prl5(inputs) -> \", prl5(inputs), type(prl5(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c3096",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe3677",
   "metadata": {},
   "source": [
    "```bash\n",
    "Default output: prl1(inputs) ->  (4.0, 6.0, 8.0) <class 'lazyllm.common.common.package'>\n",
    "Tuple output:   prl3(inputs) ->  (4.0, 6.0, 8.0) <class 'tuple'>\n",
    "List output:    prl4(inputs) ->  [4.0, 6.0, 8.0] <class 'list'>\n",
    "String output:  prl5(inputs) ->  4.0, 6.0, 8.0 <class 'str'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0c1e2",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/4warp.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/4warp_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 5. IFS\n",
    "\n",
    "`IFS` implements the classic if-else pattern. It evaluates a condition and routes the input to the \"true\" or \"false\" branch accordingly. The `with` syntax does not add much value here, so we only show the functional style:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce7ad5",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/ifs.py#L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "cond = lambda x: x > 0\n",
    "true_path = lambda x: x * 2\n",
    "false_path = lambda x: -x\n",
    "\n",
    "ifs_flow = lazyllm.ifs(cond, true_path, false_path)\n",
    "\n",
    "res1 = ifs_flow(10)\n",
    "print('Input: 10, output:', res1)\n",
    "res2 = ifs_flow(-5)\n",
    "print('Input: -5, output:', res2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88efa57",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6585e64",
   "metadata": {},
   "source": [
    "```bash\n",
    "Input: 10, Output: 20\n",
    "Input: -5, Output: 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d9d84d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/5ifs.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 6. Switch\n",
    "\n",
    "`Switch` provides a way to route data through different flows based on the value of an expression or the truth value of a condition. Its behavior is similar to a switchâ€“case statement in traditional programming languages.\n",
    "\n",
    "When using this control-flow tool, you need to define a condition function cond and the corresponding branch functions moduleX (these can also be other control-flow components such as Pipeline). One special case is the string `default`, which can be used as a fallback branch when no other condition matches.\n",
    "\n",
    "Illustration of the workflow:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46321d39",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "# switch(exp):\n",
    "#     case cond1: input -> module11 -> ... -> module1N -> out; break\n",
    "#     case cond2: input -> module21 -> ... -> module2N -> out; break\n",
    "#     case cond3: input -> module31 -> ... -> module3N -> out; break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f235d",
   "metadata": {},
   "source": [
    "`Switch` exposes a `judge_on_full_input` flag. When set to `True` (default), the same input is fed to both the condition and the branch. When set to `False`, the first argument goes to the condition and the remaining arguments go to the branch, so make sure you supply at least two inputs. Example:\n",
    "\n",
    "Functional style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/switch.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "# Condition functions\n",
    "is_positive = lambda x: x > 0\n",
    "is_negative = lambda x: x < 0\n",
    "\n",
    "# Each condition corresponds to a branch function\n",
    "positive_path = lambda x: 2 * x\n",
    "negative_path = lambda x: -x\n",
    "default_path = lambda x: '000'\n",
    "\n",
    "# Switch #1 (the same value is passed to the condition and branch function)\n",
    "switch1 = lazyllm.switch(\n",
    "    is_positive, positive_path,\n",
    "    is_negative, negative_path,\n",
    "    'default', default_path)\n",
    "\n",
    "print('\\nInput x is shared by the condition and branch:')\n",
    "print(\"1Path Positive: \", switch1(2))\n",
    "print(\"1Path Default:  \", switch1(0))\n",
    "print(\"1Path Negative: \", switch1(-5))\n",
    "\n",
    "# Switch #2 (separate inputs for the condition and branch)\n",
    "switch2 = lazyllm.switch(\n",
    "    is_positive, positive_path,\n",
    "    is_negative, negative_path,\n",
    "    'default', default_path,\n",
    "    judge_on_full_input=False)\n",
    "\n",
    "print('\\nInputs x and y go to the condition and branch respectively:')\n",
    "print(\"2Path Positive: \", switch2(-1, 2))\n",
    "print(\"2Path Default:  \", switch2(1, 2))\n",
    "print(\"2Path Negative: \", switch2(0, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd081d24",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67d39e",
   "metadata": {},
   "source": [
    "```bash\n",
    "Input x shared by condition and branch:\n",
    "1Path Positive:  4\n",
    "1Path Default:   000\n",
    "1Path Negative:  5\n",
    "\n",
    "Inputs x,y routed separately:\n",
    "2Path Positive:  -2\n",
    "2Path Default:   4\n",
    "2Path Negative:  000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e3479",
   "metadata": {},
   "source": [
    "With-style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/switch_with.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "# Condition functions\n",
    "is_positive = lambda x: x > 0\n",
    "is_negative = lambda x: x < 0\n",
    "\n",
    "# Each condition corresponds to a branch function\n",
    "positive_path = lambda x: 2 * x\n",
    "negative_path = lambda x: -x\n",
    "default_path = lambda x: '000'\n",
    "\n",
    "# Switch #1 (the same value is passed to the condition and branch)\n",
    "with lazyllm.switch() as sw1:\n",
    "    sw1.case(is_positive, positive_path)\n",
    "    sw1.case(is_negative, negative_path)\n",
    "    sw1.case('default', default_path)\n",
    "\n",
    "print('\\nInput x is shared by the condition and branch:')\n",
    "print(\"1Path Positive: \", sw1(2))\n",
    "print(\"1Path Default:  \", sw1(0))\n",
    "print(\"1Path Negative: \", sw1(-5))\n",
    "\n",
    "# Switch #2 (separate inputs for the condition and branch)\n",
    "with lazyllm.switch(judge_on_full_input=False) as sw2:\n",
    "    sw2.case(is_positive, positive_path)\n",
    "    sw2.case(is_negative, negative_path)\n",
    "    sw2.case('default', default_path)\n",
    "\n",
    "print('\\nInputs x and y go to the condition and branch respectively:')\n",
    "print(\"2Path Positive: \", sw2(-1, 2))\n",
    "print(\"2Path Default:  \", sw2(1, 2))\n",
    "print(\"2Path Negative: \", sw2(0, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fcb67",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebd4f9",
   "metadata": {},
   "source": [
    "```bash\n",
    "Input x shared by condition and branch:\n",
    "1Path Positive:  4\n",
    "1Path Default:   000\n",
    "1Path Negative:  5\n",
    "\n",
    "Inputs x,y routed separately:\n",
    "2Path Positive:  -2\n",
    "2Path Default:   4\n",
    "2Path Negative:  000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcbc8f",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/6switch.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/6swich_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 7. Loop\n",
    "\n",
    "`Loop` repeatedly applies a set of steps until a stop condition is met or a maximum number of iterations is reached. The optional `judge_on_full_input` flag determines how outputs are fed back:\n",
    "\n",
    "* `True` (default): the entire output is passed to both the condition and the next iteration.\n",
    "* `False`: the first value goes to the condition and the remaining values become the next iteration's input, so the branch must return at least two values.\n",
    "\n",
    "Functional style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/loop.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "# Stop condition\n",
    "stop_func = lambda x: x > 10\n",
    "\n",
    "# Branch function\n",
    "module_func = lambda x: x * 2\n",
    "\n",
    "# Loop #1\n",
    "loop1 = lazyllm.loop(\n",
    "    module_func,\n",
    "    stop_condition=stop_func)\n",
    "\n",
    "print('Loop #1 output:', loop1(1))\n",
    "\n",
    "#==========================\n",
    "# Branch function #2\n",
    "def module_func2(x):\n",
    "    print(\"\tloop: \", x)\n",
    "    return lazyllm.package(x + 1, x * 2)\n",
    "\n",
    "# Loop #2\n",
    "loop2 = lazyllm.loop(\n",
    "    module_func2,\n",
    "    stop_condition=stop_func,\n",
    "    judge_on_full_input=False)\n",
    "\n",
    "print('Loop #2 output:', loop2(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c1c93",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac13a74",
   "metadata": {},
   "source": [
    "```bash\n",
    "Loop #1 output: 16\n",
    "        loop:  1\n",
    "        loop:  2\n",
    "        loop:  4\n",
    "        loop:  8\n",
    "        loop:  16\n",
    "Loop #2 output: (32,)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60909e",
   "metadata": {},
   "source": [
    "With-style ([GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/loop_with.py#L1)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "\n",
    "# Stop condition\n",
    "stop_func = lambda x: x > 10\n",
    "\n",
    "# Branch functions\n",
    "module_func = lambda x: x\n",
    "modele_func2 = lambda x: x * 2\n",
    "\n",
    "# Loop #1\n",
    "with lazyllm.loop(stop_condition=stop_func) as loop1:\n",
    "    loop1.func1 = module_func\n",
    "    loop1.func2 = modele_func2\n",
    "\n",
    "print('Loop #1 output:', loop1(1))\n",
    "\n",
    "#==========================\n",
    "# Branch function #2\n",
    "def module_funcn2(x):\n",
    "    print(\"\tloop: \", x)\n",
    "    return lazyllm.package(x + 1, x * 2)\n",
    "\n",
    "# Loop #2\n",
    "with lazyllm.loop(stop_condition=stop_func, judge_on_full_input=False) as loop2:\n",
    "    loop2.func1 = module_func\n",
    "    loop2.func2 = module_funcn2\n",
    "\n",
    "print('Loop #2 output:', loop2(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b693a",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80322fb8",
   "metadata": {},
   "source": [
    "```bash\n",
    "Loop #1 output: 16\n",
    "        loop:  1\n",
    "        loop:  2\n",
    "        loop:  4\n",
    "        loop:  8\n",
    "        loop:  16\n",
    "Loop #2 output: (32,)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d05fa73",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/7loop.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/7loop_with.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "### 8. Bind\n",
    "\n",
    "All the flows above pass data along predefined paths, so injecting upstream values into deeper nodes can be tricky. `bind` solves this by letting data \"jump\" across a flow.\n",
    "\n",
    "![image.png](3_images/img5.png)\n",
    "\n",
    "We'll first define the functions `f1`, `f21`, `f22`, and `f3` shown above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cebd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(input): return input ** 2\n",
    "def f21(input1, input2=0): return input1 + input2 + 1\n",
    "def f22(input1, input2=0): return input1 + input2 - 1\n",
    "def f3(in1='placeholder1', in2='placeholder2', in3='placeholder3'): \n",
    "    return f'get [input:{in1}], [f21:{in2}], [f23: {in3}]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e5357",
   "metadata": {},
   "source": [
    "LazyLLM exposes `lazyllm.bind` for parameter binding:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062245ef",
   "metadata": {},
   "source": [
    "```text\n",
    "lazyllm.bind(func, param1, param2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6a02a",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* **`func`** is the target function.\n",
    "\n",
    "* **`param1`** is the first argument passed to `func`. In the diagram it equals the pipeline input (`ppl.input`).\n",
    "\n",
    "* **`param2`, `param3`, ...** refer to upstream outputs. LazyLLM provides placeholders `_0`, `_1`, `_2`, etc. `_0` is the output of the previous node; `_1` is the second output, and so on. In the example we bind `_0` and `_1` because the upstream stage emits two values.\n",
    "\n",
    "With binding we can recreate the shortcut shown above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e87c98",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/bind.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import pipeline, parallel, bind, _0, _1\n",
    "\n",
    "with pipeline() as ppl1:\n",
    "  ppl1.f1 = f1\n",
    "  with parallel() as ppl1.subprl2:\n",
    "    ppl1.subprl2.path1 = f21\n",
    "    ppl1.subprl2.path2 = f22\n",
    "  ppl1.f3 = bind(f3, ppl1.input, _0, _1)\n",
    "  \n",
    "print(\"ppl1 out: \", ppl1(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58272e",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd2218",
   "metadata": {},
   "source": [
    "```bash\n",
    "ppl1 out:  get [input:2], [f21:5], [f23: 3]]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8eab138",
   "metadata": {},
   "source": [
    ">**Note**: bindings only work inside the current data flow. You cannot bind external variables or cross-flow data.\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/8jump1.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "`bind` also overloads the `|` operator, so you can separate the function from its bound parameters. Sub-flows accept bindings as well. In the diagram below we bind the flow input to the first parameter of `subprl2` (red arrow) and feed `f1`'s output into the second parameter.\n",
    "\n",
    "![image.png](3_images/img6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065cedc",
   "metadata": {},
   "source": [
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter3/bind_more.py#L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyllm import pipeline, parallel, bind, _0, _1\n",
    "\n",
    "with pipeline() as ppl1:\n",
    "  ppl1.f1 = f1\n",
    "  with parallel().bind(ppl1.input, _0) as ppl1.subprl2:\n",
    "    ppl1.subprl2.path1 = f21\n",
    "    ppl1.subprl2.path2 = f22\n",
    "  ppl1.f3 = f3 | bind(ppl1.input, _0, _1)\n",
    "  \n",
    "print(\"ppl1 out: \", ppl1(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07cd44",
   "metadata": {},
   "source": [
    "Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd2b78",
   "metadata": {},
   "source": [
    "```bash\n",
    "ppl1 out:  get [input:2], [f21:7], [f23: 5]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3c3fc",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/8jump2.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n",
    "\n",
    "## Build RAG with LazyLLM\n",
    "\n",
    "Back in Chapter 2 we implemented a basic RAG pipeline with three steps: retrieval, augmentation, and generation. Using the flows above we can rebuild the same system with cleaner code. Full source:\n",
    "\n",
    "[GitHub code link](https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter3/rag_with_flow.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazyllm\n",
    "from lazyllm import bind\n",
    "\n",
    "# Document loader\n",
    "documents = lazyllm.Document(dataset_path=\"/mnt/lustre/share_data/dist/cmrc2018/data_kb\")\n",
    "prompt = 'You are an AI question-answering assistant. Provide answers based on the given context and question.'\n",
    "\n",
    "with lazyllm.pipeline() as ppl:\n",
    "    # Retriever\n",
    "    ppl.retriever = lazyllm.Retriever(doc=documents, group_name=\"CoarseChunk\", similarity=\"bm25_chinese\", topk=3)\n",
    "    ppl.formatter = (lambda nodes, query: {\"query\": query, \"context_str\": \"\".join([node.get_content() for node in nodes])}) | bind(query=ppl.input)\n",
    "    # Generator\n",
    "    ppl.llm = lazyllm.OnlineChatModule().prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))\n",
    "\n",
    "lazyllm.WebModule(ppl, port=23466).start().wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17617a3a",
   "metadata": {},
   "source": [
    "Let's refactor last chapter's RAG step by step and wire it up with data flows:\n",
    "\n",
    "<div style=\"text-align:center; margin:20px 0;\">\n",
    "  <video controls style=\"width:900px; max-width:100%; height:auto; border:1px solid #ccc; border-radius:8px; box-shadow:0 4px 8px rgba(0,0,0,0.1);\">\n",
    "    <source src=\"./3_videos/rag_pipeline.mp4\" type=\"video/mp4\" />\n",
    "    Your browser does not support the video tag.\n",
    "  </video>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
